% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage{float}
\usepackage{url}

\usepackage{listings}
\usepackage{color}
\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=Java,
basicstyle=\ttfamily,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
%numbers=left,
numberstyle=\tiny\color{black},
stepnumber=2,
numbersep=10pt,
tabsize=4,
showspaces=false,
showstringspaces=false}



\restylefloat{table}

\hyphenation{op-tical net-works semi-conduc-tor}



\begin{document}



\title{Dirt Spot Sweeping Random Strategy}


\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Mian Asbat Ahmad\\
       \affaddr{Department of Computer Science}\\
       \affaddr{The University of York}\\
       \affaddr{York, United Kingdom}\\
       \email{mian.ahmad@york.ac.uk}
% 2nd. author
\alignauthor
Manuel Oriol \\
       \affaddr{Department of Computer Science}\\
       \affaddr{The University of York}\\
       \affaddr{York, United Kingdom}\\
       \email{manuel.oriol@york.ac.uk}
}



\maketitle
\begin{abstract}
While random testing has recently gained momentum, very little has been known on failure domains and their shape. 
This paper presents an enhanced and improved form of automated random testing, called the Dirt Spot Sweeping Random (DSSR) strategy. DSSR is a new strategy that takes the assumption that a number of failure domains are contiguous.
DSSR starts as a regular random+ testing session --- a random testing session with some preference for boundary values. 
When a failure is found, it increases the chances of using neighbouring values in subsequent tests, thus slowly sweeping around the failure found in hope of finding failures from a different kind in its vicinity.

DSSR was implemented within the YETI random testing tool. We evaluate DSSR against random+ and pure random strategies by testing 80 classes with $10^5$ calls for each session 30 times for each strategy.
We found that for 68\% of the classes all three strategies find the same unique failures, for 9\% of the classes random+ performs better, for 14\% pure random performs better, and for 9\% DSSR performs better.
Overall, DSSR also found 2.3\% more unique failures than random and .3\% more unique failures than random+.
\end{abstract}

%A category including the fourth, optional field follows...
\category{D.2.5}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Comparison, Verification, }

\keywords{software testing, automated random testing, comparative analysis} % NOT required for Proceedings


%%%%%%%%%%%%%%%%%    INTRODUCTION   %%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
Success of a software testing technique is mainly based on the number of faults it discovers in the Software Under Test (SUT). An efficient testing process discovers the maximum number of faults in a minimum possible amount of time. Exhaustive testing, where software is tested against all possible inputs, is in most cases not feasible because of the size of the input domain, limited resources and strict time constraints. Therefore, strategies in automated software testing tools are developed with the aim to select more fault-finding test input from the input domain for a given SUT. Producing such targeted test input is difficult because each system has its own requirements and functionality.

Chan et al.~\cite{Chan1996} discovered that there are patterns of failure-causing inputs across the input domain. They divided the patterns into point, block and strip patterns on the basis of their occurrence across the input domain. Chen et al.~\cite{Chen2008} also found that the performance of random testing can be increased by slightly altering the technique of test case selection. In adaptive random testing, they found that the performance of random testing increases by up to 50\% when test input is selected evenly across the whole input domain. This was mainly attributed to the better distribution of input which increased the chance of selecting inputs from failure patterns. Similarly Restricted Random Testing \cite{Chan2002}, Feedback directed Random Test Generation \cite{Pacheco2007a}, Mirror Adaptive Random Testing \cite{Chen2003} and Quasi Random Testing \cite{Chen2005} also stress the need for test case selection covering the whole input domain to improve results.

In this paper we take the assumption that for a significant number of classes failure domains are contiguous or are very close by. From this assumption, we devised the Dirt Spot Sweeping\footnote{The name refers to the cleaning robots strategy which insists on places where dirt has been found in large amount.} Random strategy (DSSR) which starts as a random+ strategy --- a random strategy focusing more on boundary values. When it finds a new failure, it then increases the chances of finding more faults using neighbouring values. Note that, similarly to previous studies~\cite{Oriol2012} we approximate faults with unique failures. Of course, since this strategy is also a random testing strategy, it has the potential to find all unique failures in the program, but we expect it to be faster at finding unique failures for classes in which failure domains are contiguous than the pure random (R) and random+ (R+) strategies only.

We implemented DSSR as a strategy for the random testing tool YETI\footnote{\url{http://www.yetitest.org}}. To evaluate our approach, we tested thirty times each one of 80 classes from the Qualitas Corpus\footnote{\url{http://www.qualitascorpus.com}} with each of the three strategies DSSR, R, and R+. We found that for 68\% of the classes all three strategies find the same unique failures, for 9\% of the classes random+ performs better, for 14\% pure random performs better, and for 9\% DSSR performs better.
Overall, DSSR also found 2.3\% more unique failures than random and .3\% more unique failures than random+.


The rest of this paper is organized as follows:
Section~\ref{sec:dssr} describes the DSSR strategy. Section~\ref{sec:imp} presents our implementation of the strategy. Section~\ref{sec:eval} explains our experimental setup. Section~\ref{sec:res} shows the results of our experiments. Section~\ref{sec:discussion} discusses the results. Section~\ref{sec:rw} presents related work and we conclude in Section~\ref{sec:conc}.




%%%%%%%%%%%%%%%%%    DIRT SPOT SWEEPING STRATEGY  %%%%%%%%%%%%%%%

\section{Dirt Spot Sweeping Random Strategy}\label{sec:dssr}
The Dirt Spot Sweeping Random strategy (DSSR) is a new strategy which combines the random+ strategy with a dirt spot sweeping functionality. The strategy is based on two intuitions. First, boundaries have interesting values and using these values in isolation can provide high impact on test results. Second, faults and unique failures reside in contiguous blocks and stripes. If this is the case, DSSR increases the performance of the test strategy. Each strategy is briefly explained as follows.

\subsection{Random (R)}
The pure random strategy is a black-box testing technique in which the SUT is executed using randomly selected test data. Test results obtained are compared to the defined oracle, using SUT specifications in the form of contracts or assertions. In the absence of contracts and assertions the exceptions defined by the programming language are used as test oracles. 

Because of its black-box testing nature, this strategy is particularly effective in testing softwares where the developers want to keep the source code secret~\cite{Chen2010}. The generation of random test data is comparatively cheap and does not require too much intellectual and computation efforts~\cite{Ciupa2009, Ciupa2008}. It is mainly for this reason that various researchers have recommended this strategy for automatic testing tools \cite{Ciupa2008a}. YETI \cite{Oriol2010a, Oriol2010}, AutoTest \cite{Leitner2007, Ciupa2007}, QuickCheck \cite{Claessen2000}, Randoop \cite{Pacheco2007}, Jartage \cite{Oriat2004} are some of the most common automated testing tools based on random strategy.\\
\indent Efficiency of random testing was made suspicious with the intuitive statement of Myers \cite{Myers2004} who termed random testing as one of the poorest methods for software testing. However, experiments performed by various researchers, \cite{Ciupa2007, Duran1981, Duran1984, Hamlet1994, Ntafos2001} have experimentally proven that random testing is simple to implement, cost effective, highly efficient and free from human bias as compared to its rival techniques.

Because programs tested at random typically fail a large number of times (there are a large number of calls), it is necessary to cluster failures that likely represent the same fault. The traditional way of doing it is to compare the full stack traces and error types and use this as an equivalence class~\cite{Ciupa2007,Oriol2012} called a unique failure. This way of grouping failures is also used for random+ and DSSR.

\subsection{Random Plus Strategy (R+)}
The random+ strategy~\cite{Leitner2007} is an extension of the pure random strategy. It uses some special pre-defined values which can be simple boundary values or values that have high tendency of finding faults in the SUT. Boundary values~\cite{Beizer1990} are the values on the start and end of a particular type. For instance, such values for \verb+int+ could be \verb+MAX_INT+, \verb+MAX_INT-1+, \verb+MIN_INT+, \verb-MIN_INT+1-, \verb+-1+, \verb+0+, \verb+1+.



Similarly, the tester might also add some other special values that he considers effective in finding faults for the current SUT. For example, if a program under test has a loop from -50 to 50 then the tester can add -55 to -45, -5 to 5, 45 to 55 etc., to the pre-defined list of special values in order to be selected for a test. This static list of interesting values is manually updated before the start of the test and has slightly high priority than selection of random values because of more relevance and high chances of finding faults for the given SUT. These special values have high impact on the results particularly detecting problems in specifications~\cite{Ciupa2008}.


\subsection{Dirt Spot Sweeping}
Chan et al.~\cite{Chan1996} found that there are patterns of failure-causing inputs across the input domain. Figure \ref{fig:patterns} shows these patterns for two dimensional input domain. They divided these patterns into three types called points, block and strip patterns. The black area (Points, block and strip) inside the box show the input which causes the system to fail while white area inside the box represent the genuine input. Boundary of the box (black solid line) surrounds the complete input domain and also represents the boundary values. They also argue that a strategy has more chances of hitting these fault patterns if test cases far away from each other are selected. Other researchers~\cite{Chan2002, Chen2003, Chen2005}, also tried to generate test cases further away from one another targeting these patterns and achieved higher performance.

\begin{figure}[ht]                                    
\centering
\includegraphics[width= 8cm,height=2.5cm]{ART_Patterns.png}
\caption{Failure patterns across input domain~\cite{Chen2008}}
\label{fig:patterns}
\end{figure}

Dirt spot sweeping is the part of DSSR strategy that comes into action when a failure is found in the system. On finding a failure, it immediately adds the value causing the failure and its neighbouring values to the already existing list of interesting values. For example in a program if the \verb+int+ type value 50 causes a failure in the system then spot sweeping will add values from 47 to 53 to the list of interesting values. If the failure lies in the block or strip pattern, then adding its neighbours will explore other failures present in that block or strip. As against random plus where the list of interesting values remain static, the list of interesting values is dynamic and changes during the test execution of each program in the DSSR strategy.

\begin{figure}[ht]
\centering
\includegraphics[width=8cm,height=2.2cm]{block2.png}
\caption{DSSR covering block and strip pattern}
\label{fig:block2}
\end{figure}

Figure \ref{fig:block2} shows how dirst spot sweeping explores the failures residing in the block and strip patterns of a program. The failure coverage from the pattern is shown in spiral form because first failure will lead to second, second to third and so on till the end. In case the failure is positioned on the point pattern then the added values will not be very effective because point pattern is only an arbitrary failure point in the whole input domain.

\subsection{Structure of the Dirt Spot Sweeping Random Strategy}

The DSSR strategy is explained with the help of a flow-chart in Figure~\ref{fig:Working_DSSS}. In this process, the strategy continuously tracks the number of failures during the execution of the test session. To keep the system fast this tracking is done in a very effective way with zero or minimum overhead~\cite{Leitner2009}. The execution of test is performed normally until a failure is found in the SUT. Then the program does not only copy the values that lead to the failure, but also copies its surrounding values to the variable list of interesting values. As presented in the flowchart, if the failure finding value is of primitive type then the DSSR finds the type of the value and add values only of that particular type to the interesting values. Addition of these values increases the size of the list of interesting values that provide relevant test data for the remaining test session and the new generated test cases are more targeted towards finding new failures in the given SUT around pre-existing failures.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{flowchart1.pdf}
\caption{Working mechanism of DSSR Strategy}
\label{fig:Working_DSSS}
\end{figure}

Boundary values and other special values that have a high tendency of finding faults in the SUT are added to the list by random plus strategy prior to the start of test session where as to sweep the failure pattern, the fault-finding value and its surrounding values are added at runtime after a failure is found. Table \ref{table:addvalues} presents the values that are added to the list of interesting values when a failure is found. In the table the test value is represented by X where X can be int, double, float, long, byte, short, char and String. All values are converted to their respective types before adding to the list of interesting values and vice versa.

\begin{table}[ht]
%\scriptsize
\caption{Neighbouring values for primitive types and String} % title of Table
\centering % used for centering table
\begin{tabular}{| l | l |} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Type & Values to be added\\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
\multirow{1}{*}{X is int, double, float, } & ~ X,  X+1, X+2, X-1, X-2 \\ % inserting body of the
\multirow{1}{*}{long, byte, short \& char} &  \\ 

\hline
\multirow{8}{*}{X is String} & ~ X\\ % inserting body of the table

& ~ X + ``  "\\ % inserting body of the table
& ~ ``  " + X \\ % inserting body of the table
& ~ X.toUpperCase() \\
& ~ X.toLowerCase() \\
& ~ X.trim() \\
& ~ X.substring(2) \\
& ~ X.substring(1, X.length()-1) \\[1ex]
\hline
\hline %inserts single line
\end{tabular}
\label{table:addvalues} % is used to refer this table in the text
\end{table}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPLANATION OF DSSR STRATEGY %%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Explanation of DSSR on a concrete example}
The DSSR strategy is explained through a simple program seeded with at least three faults. The first fault is a division by zero exception denoted 1 while the second and third are failing assertion statements denoted 2 and 3 in the following program.  Below we describe how the DSSR strategy perform execution when the following class is expose to testing.

\begin{lstlisting}
/** 
* Calculate square of given number 
* and verify results. 
* The code contain 3 faults.
* @author (Mian and Manuel)
*/
public class Math1 {
 public void calc (int num1) {
  // Square num1 and store result. 
  int result1 = num1 * num1;
  int result2 = result1 / num1; // 1
  assert Math.sqrt(result1) == num1; // 2
  assert result1 >= num1; // 3
 } 
}
\end{lstlisting}



In the above code, one primitive variable of type \verb+int+ is used, therefore, the input domain for DSSR strategy is from \verb+-2,147,483,648 to 2,147,483,647+. The strategy further selects some values (\verb+0, Integer.MIN\_VALUE+ and \verb+Integer.MAX\_VALUE+) as interesting values which are prioritised for selection as inputs. 
As the test starts, three faults are quickly discovered by DSSR strategy in the following order.

\indent \textbf{Fault 1:} The DSSR strategy might select value \verb+0+ for variable \verb+num1+  in the first test case because \verb+0+ is available in the list of interesting values and therefore its priority is higher than other values. This will cause Java to generate division by zero exception.

\indent \textbf{Fault 2:} After catching the first fault, the strategy adds it and its surrounding values to the list of interesting values which includes \verb+0, 1, 2, 3 and -1, -2, -3+ in this case. In the second test case DSSR strategy may pick \verb+-3+ as a test value and lead to the second fault where assertion (2) fails because the square root of \verb+9+ will be \verb+3+ instead of the input value -3.

\indent \textbf{Fault 3:} After few tests DSSR strategy may select \verb+Integer.MAX\_VALUE+ for variable \verb+num1+  from the list of interesting values which will lead to the 3rd fault because \verb+result1+ will not be able to store the square of \verb+Integer.MAX\_VALUE+. Instead of the actual square value Java assigns a negative value (Java language rule) to variable result1 that will lead to the violation of the next assertion (3).

The process above explains that the pre-defined values including border values, fault-finding values and the surrounding values lead to the available faults quickly and in small number of tests as compared to random and random+ strategy. Random and random+ takes longer to discover the second and third fault because they start again searching for new unique failures randomly although the remaining faults are very close to the first one. 


%%%%%%%%%%%%%%%%%    IMPLEMENTATION OF DSSR STRATEGY   %%%%%%%%%%%%


\section{Implementation of the DSSR strategy}\label{sec:imp}
As mentioned previously, the implementation of the DSSR strategy is made in the YETI open-source automated random testing tool. YETI is developed in Java and capable of testing systems developed in procedural, functional and object-oriented languages. Its language-agnostic meta model enables it to test programs written in multiple languages including Java, C\#, JML and .Net. The core features of YETI include easy extensibility for future growth, speed of up to one million calls per minute on java code, real time logging, real time GUI support, ability to test programs using multiple strategies, and auto generation of test report at the end of the testing sessions. For large-scale testing there is a cloud-enabled version of YETI that is capable of executing parallel test sessions in Cloud~\cite{Oriol2010}. A number of hitherto faults have successfully been found by YETI in various production softwares~\cite{Oriol2012, Oriol2011}.

YETI can be divided into three decoupled main parts: the core infrastructure, language-specific bindings and strategies. The core infrastructure contains representation for routines, a group of types and a pool of specific type objects. The language specific bindings contain the code to make the call and process the results. The strategies section defines the procedure of how to select the modules (classes) from the project, how to select routines (methods) from these modules and how to generate values for the instances involved inside these routines. The most common strategies are random and random+. 

By default, YETI uses the random+ strategy if no particular strategy is defined during test initialization. It also enables the user to control the probability of using null values and the percentage of newly created objects for each test session. YETI provides an interactive Graphical User Interface (GUI) in which users can see the progress of the current test in real time. In addition to the GUI, YETI also provides extensive logs of the test session for more in-depth analysis.

The DSSR strategy has then been added as an extension of YetiRandomStrategy, which in itself is an extension of an abstract class YetiStrategy. The class hierarchy is shown in Figure \ref{fig:hierarchyofDSSR}.

\begin{figure}[h]
\centering
\includegraphics[width=4cm,height=4.5cm]{hierarchy.pdf}
\caption{Class Hierarchy of DSSR in YETI}
\label{fig:hierarchyofDSSR}
\end{figure}





%%%%%%%%%%%%%%%%%    EVALUATION   %%%%%%%%%%%%%%%%%%%%


\section{Evaluation}\label{sec:eval}

To evaluate the DSSR strategy, we compare its performances to the performances of both pure random testing (R) and the random+ (R+)~\cite{Leitner2007} strategy. General factors such as system software and hardware as well as the YETI specific factors like percentage of null values, percentage of newly created objects and interesting value injection probability have the same values for the experiments.

\subsection{Research questions}
To evaluate the DSSR strategy and its usefulness, we set out to answer the following research questions:
\begin{enumerate}
\item Is any of the three strategies R, R+ and DSSR provide better results than the other two?
\item Is there a subset of the classes for which R, R+, or DSSR provide better results than the other two?
\item If such categories exist, what are their sizes and how do they compare and can we pick a default strategy according to this citerion?
\end{enumerate}



\subsection{Experiments}

To evaluate the performances of DSSR we performed extensive testing of programs from the Qualitas Corpus~\cite{Tempero2010a}. The Qualitas Corpus is a curated collection of open source java projects built with the aim of helping empirical research on  software engineering. These projects are collected in an organized form containing both the source and binary forms. The present evaluation uses version 20101126 which contains 106 open source java projects. We picked 32 projects at random and picked 80 classes at random that produced at least 1 failure and did not timeout with a testing session of maximum 10 minutes. We tested each of the 80 classes thirty times with each strategy. Names and versions of the projects to which these classes belong are given in table~\ref{table:projects}.

Each class is evaluated through $10^5$ calls in each testing session.\footnote{The total number of tests is thus $80\times 30\times 3 \times 10^5 = 720\times 10^6~tests$.} 
Because of the absence of the contracts and assertions in the code under test, similarly to previous approaches~\cite{Oriol2012}, we use undeclared exceptions to compute unique failures found.


\begin{table}[h]
\caption{Name and versions of 32 Projects randomly selected from the Qualitas Corpus for the experiments}
\centering
\begin{tabular}{l}
ant-1.8.1\\
antlr-3.2\\
aoi-2.8.1\\
argouml-0.30.2\\
artofillusion281\\
aspectj-1.6.9\\
axion-1.0-M2,\\
azureus\\
castor-1.3.1\\
cayenne-3.0.1\\
cobertura-1.9.4.1\\
colt-1.2.0\\
emma-2.0.5312\\
freecs-1.3.20100406\\
hibernate-3.6.0\\
hsqldb-2.0.0\\
itext-5.0.3\\
jasml-0.10\\
jmoney-0.4.4\\
jruby-1.5.2\\
jsXe-04\_beta\\
quartz1.8.3\\
sandmark3.4\\
squirrel-sql-3.1.2\\
tapestry-5.1.0.5\\
tomcat-7.0.2\\
trove-2.1.0\\
velocity-1.6.4\\
weka-3.7.2\\
xalan-2.7.1\\
xerces-2.10.0\\
xmojo-5.0.0\\
\end{tabular}
\label{table:projects}
\end{table}



All tests are performed using a 64-bit Mac OS X Lion Version 10.7.4 running on 2 x 2.66 GHz 6-Core Intel Xeon with 6.00 GB (1333 MHz DDR3) of RAM. YETI runs on top of the Java\texttrademark  SE Runtime Environment [version 1.6.0\_35]. The machine took approximately 100 hours to process the experimental data.


\subsection{Performance measurement criteria}
Various measures including the E-measure, P-measure and F-measure have been used by researchers to find the effectiveness of the random test strategy. The E-measure (expected number of failures detected) and P-measure (probability of detecting at least one failure) were heavily criticized~\cite{Chen2008} and are not considered effective techniques for measuring efficiency of test strategy. The F-measure (number of test cases used to find the first fault) has been often used by researchers~\cite{Chen1996,Chen2004}. In our initial experiments the F-measure was used to evaluate the efficiency. Soon after a few experiments, it was realised that this was not the right choice because in some experiments the first strategy found the first fault quickly than the second strategy but on the completion of test session the first strategy found lower number of total faults than the second strategy. The preference to a strategy only because it found the first fault better without giving due consideration to the total number of faults was not fair~\cite{Liu2012}.


  
The literature review revealed that the F-measure is used where testing stops after identification of the first fault and the system is given back to the developers to remove the fault found. In such cases it make sense but now a days automated random testing tools test the whole system and print all of the faults found in one go therefore F-measure is not the favorable choice. Therefore in our experiments, performance of the strategy was measured by the maximum number of faults in a particular number of test calls \cite{Pacheco2007a}, \cite{Ciupa2007}, \cite{Ciupa2008b}. This measurement was found effective because it clearly measured the performance of the strategy when all the other factors were kept constant.

%%%%%%%%%%%%%%%%%    RESULTS   %%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
\centering
\includegraphics[width=18cm]{StackedBar100PercentMean.png}
\caption{Normalized stacked bar diagram of all tested classes.}
\label{fig:stackedbar}
\end{figure*}

\begin{table*} [htp!]
  \scriptsize
 \caption{Complete results for R, R+ and DSSR. Results present mean, max, min and relative standard deviation.}
	\begin{minipage}[h]{\textwidth}\centering
 	\begin{tabular}{cl c c c c c c c c c c c c}
      %\hline
      \multirow{2}{*}{}		& \multirow{2}{*}{Class Name}		& \multicolumn{4}{c}{R}							&	\multicolumn{4}{c}{R+}							&	\multicolumn{4}{c}{DSSR}	\\
      %\cline{3-14} 
      						&							& mean & max & min & rel std dev				& mean 	& max 	& min 	& rel std dev 		& mean 		& 	max 		&	 min 		& rel std dev \\
           %  \hline
     
 1						& Routine						& 7	&	7	&	7	& 		0					& 7		&  7		& 7		& 		0			& 7			& 7			& 7			&	0	\\
 2						& Response					& 6	&	6	&	6	& 		0					& 6		&  6		& 6		& 		0			& 6			& 6			& 6			&	0	\\
 3						& Repository					& 31	&	31	&	31	& 		0					& 40		&  40		& 40		& 		0			& 40			& 40			& 40			&	0	\\      
 4						& Rectangle					& 3	&	3	&	3	& 		0					& 3		&  3 		& 3		& 		0			& 3			& 3			& 3			&	0	\\      
 5						& Project						& 64.7&	71	&	60	& 		0.03					& 66.36	&  78		& 62		& 		0.04			& 68.53		& 78			& 64			&	0.04	\\      
 6						& ProjectFactory				& 1	&	1	&	1	& 		0					& 1		&  1		& 1		& 		0			& 1			& 1			& 1			&	0	\\      
 7						& PersistentSet					& 36	&	36	&	36	& 		0					& 36		&  36		& 36		& 		0			& 36			& 36			& 36 			&	0	\\      
 8						& PersistentMap				& 47	&	47	&	47	& 		0					& 47		&  47		& 47		& 		0			& 47			& 47			& 47			&	0	\\      
 9						& PersistentList					& 65	&	65	&	65	& 		0					& 65		&  65		& 65		& 		0			& 65			& 65			& 65			&	0	\\      
 10						& PersistentBag				& 68	&	68	&	68	& 		0					& 68		&  68		& 68		& 		0			& 68			& 68			& 68			&	0	\\      
 11						& Coverage					& 5	&	5	&	5	& 		0					& 5		& 5		& 5		& 		0			& 5			& 5			& 5			&	0	\\      
 12						& NodeSet					& 28.06&	29	&	26	& 		0.02					& 27.86	& 29 		& 26		& 		0.03			& 27.65		& 29			& 26			&	0.03	\\      
 13						& NodeSequence				& 38	&	46	&	30	& 		0.09					& 36.65	& 45 		& 30		& 		0.09			& 36.62		& 44			& 30			&	0.11	\\      
 14						& NameEntry					& 4	&	4	&	4	& 		0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
 15						& Response					& 5	&	5	&	5	& 		0					& 5		& 5 		& 5		& 		0			& 5			& 5			& 5			&	0\\      
 16						& Mat4						& 1	&	1	&	1	& 		0					& 1		&  1		& 1		& 		0			& 1			& 1			& 1			&	0\\      
 17						& List						& 5.27&	6	&	4	& 		0.16					& 5.65	& 6 		& 4		& 		0.09			& 5.34		& 6			& 2			&	0.09\\      
 18						& JmxUtilities					& 7.68&	8	&	6	& 		0.06					& 7.89	& 8 		& 7		& 		0.03			& 7.86		& 8			& 7			&	0.04\\      
 19						& JavaWrapper					& 2	&	2	&	2	& 		0					& 3.93	& 4 		& 3		& 		0.25			& 3.96		& 4			& 3			&	0.18\\      
 20						& ItemSet						& 4	&	4	&	4	& 		0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
 21						& IntStack						& 4	&	4	&	4	& 		0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
 22						& IntHolder					& 1	&	1	&	1	& 		0					& 1		& 1 		& 1		& 		0			& 1			& 1			& 1			&	0\\      
 23						& InstrumentTask				& 1.93&	2	&	1	& 		0.13					& 1.96	& 2 		& 1		& 		0.09			& 2			& 2			& 2			&	0.09\\      
 24						& Image						& 13.89&	18	&	7	& 		0.15					& 12.37	& 14 		& 4		& 		0.20			& 12.89		& 15			& 5			&	0.13\\      
 25						& HttpAuth					& 2	&	2	&	2	& 		0					& 2		& 2 		& 2		& 		0			& 2			& 2			& 2			&	0\\      
 26						& Group						& 11	&	11	&	11	& 		0					& 10.03	& 4 		& 11		& 		0.24			& 11			& 11			& 11			&	0\\      
 27						& Generator					& 17	&	17	&	17	& 		0					& 17		& 17 		& 17		& 		0			& 17			& 17			& 17			&	0\\      
 28						& FPGrowth					& 5	&	5	&	5	& 		0					& 5		&  5		& 5		& 		0			& 5			& 5			& 5			&	0\\      
 29						& Font						& 11.86&	12	&	11	& 		0.02					& 11.86	& 12 		& 11		& 		0.02			& 11.96		& 12			& 11			&	0.01\\      
 30						& FileUtil						& 1	&	1	&	1	& 		0					& 1		& 1 		& 1		& 		0			& 1			& 1			& 1			&	0\\      
 31						& Files						& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 31						& FileHandler					& 2	&	2	&	2	& 		0					& 2		& 2 		& 2		& 		0			& 2			& 2			& 2			&	0\\      
 33						& Facade						& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 34						& Entry						& 6	&	6	&	6	& 		0					& 6		& 6 		& 6		& 		0			& 6			& 6			& 6			&	0\\      
 35						& EntryComparator				& 13	&	13	&	13	& 		0					& 13		& 13 		& 13		& 		0			& 13			& 13			& 13			&	0\\      
 36						& EntryDecoder				& 7.93&	9	&	7	& 		0.08					& 8.10	& 9 		& 7		& 		0.09			& 8.13		& 9			& 7			&	0.08\\ 
 37						& Entities						& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 38						& DOMParser					& 6.75&	7	&	0	& 		0					& 7		& 7 		& 7		& 		0			& 7			& 7			& 7			&	0.18\\      
 39						& DiskIO						& 4	&	4	&	4	& 		0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
 40						& DirectoryScanner				& 32.68&	39	&	0	& 		0.27					& 35.13	& 38 		& 31		& 		0.04			& 35.41		& 39			& 32			&	0.04\\      
 41						& Debug						& 4.62&	6	&	4	& 		0.13					& 4.58	& 6 		& 4		& 		0.12			& 4.86		& 8			& 4			&	0.18\\      
 42						& ColumbaClient				& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 43						& ClassLoaderLogMan		& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 44						& CheckAssociator				& 7.06&	8	&	2	& 		0.16					& 6.44	& 9 		& 2		& 		0.33			& 6.96		& 9			& 2			&	0.18\\      
 45						& CatalogManager				& 7	&	7	&	7	& 		0					& 7		& 7 		& 7		& 		0			& 7			& 7			& 7			&	0\\      
 46						& Capabilities					& 1.27&	2	&	1	& 		0.35					& 1.51	& 2 		& 1		& 		0.33			& 1.37		& 2			& 1			&	0.36\\      
 47						& BitSet						& 9	&	9	&	9	& 		0					& 9		& 9 		& 9		& 		0			& 9			& 9			& 9			&	0\\      
 48						& BaseColor					& 14	&	14	&	14	& 		0					& 14		& 14 		& 14		& 		0			& 14			& 14			& 14			&	0\\      
 49						& ArchiveUtil					& 2	&	2	&	2	& 		0					& 2		& 2 		& 2		& 		0			& 2			& 2			& 2			&	0\\      
 50						& Apriori						& 3.10&	4	&	3	& 		0.09					& 3.24	& 4 		& 3		& 		0.13			& 3.17		& 4			& 3			&	0.11\\      
 51						& AntTypeDefinition				& 2.89&	4	&	2	& 		0.27					& 2.75	& 4 		& 2		& 		0.29			& 2.79		& 4			& 2			&	0.23\\      
 52						& AjTypeImpl					& 79.89&	83	&	79	& 		0.01					& 80.06	& 83 		& 79		& 		0.01			& 79.62		& 83			& 79			&	0.01\\      
 53						& AdminCore					& 5	&	5	&	5	& 		0					& 5		& 5 		& 5		& 		0			& 5			& 5			& 5			&	0\\      
 54						& ActionTranslator				& 95.86&	96	&	96	& 		0					& 96		& 96 		& 96		& 		0			& 96			& 96			& 96			&	0\\      
 55						& RubyBigDecimal				& 4 	&	4	&	4	& 		0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
 56						& Scanner					& 3.27&	5	&	2	& 		0.19					& 2.79	& 5 		& 2		& 		0.27			& 3.06		& 5			& 2			&	0.28\\      
 57						& Scene						& 26.10&	27	&	1	& 		0.18					& 25.93	& 27 		& 1		& 		0.18			& 26			& 27			& 1			&	0.18\\      
 58						& SelectionManager				& 3	&	3	&	3	& 		0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 59						& Server						& 15.51&	21	&	11	& 		0.20					& 16.93	& 12 		& 21		& 		0.16			& 16.93		& 12			& 21			&	0.17\\      
 60						& Sorter						& 1.96&	2	&	1	& 		0.09					& 	3	& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 61						& Sorting						& 3	&	3	&	3	& 		0					& 	3	& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 62						& SSL						&13	&	13	&	13	& 		0					& 13		& 13 		& 13		& 		0			& 13			& 13			& 13			&	0\\      
 63						& Statistics					& 14.75&	17	&	12	&	 	0.04					& 23.37	& 25 		& 22		& 		0.03			& 23.44		& 25			& 22			&	0.04\\      
 64						& Status						& 53	&	53	&	53	& 		0					& 53		& 53 		& 53		& 		0			& 53			& 53			& 53			&	0\\      
 65						& Storpwords					& 7.03&	8	&	7	& 		0.02					& 7.68	&  8		& 7		& 		0.06			& 7.65		& 8			& 7			&	0.06\\      
 66						& StringHelper					& 43.41& 45	&	41	& 		0.01					& 44		&  46		& 42		& 		0.02			& 43.55		& 45			& 42			&	0.02\\      
 67						& StringUtils					&19 	&	19	&	19	& 		0					& 19		& 19 		& 19		& 		0			& 19			& 19			& 19			&	0\\      
 68						& TextImpl					& 2 	&	2	&	2	& 		0					& 2		&  2		& 2		& 		0			& 2			& 2			& 2			&	0\\      
 69						& TouchCollector				& 3	&	3	&	3	& 		0					& 3		&  3		& 3		& 		0			& 3			& 3			& 3			&	0\\      
 70						& Trie						& 21.17&	22	&	21	& 		0.01					& 21.10	&  22		& 21		& 		0.01			& 21.03		& 22			& 21			&	0\\      
 71						& URI						& 5 	&	5	&	5	& 		0					& 5		&  5		& 5		& 		0			& 5			& 5			& 5			&	0\\      
 72						& Itextpdf						& 8	&	8	&	8	& 		0					& 8		&  8		& 8		& 		0			& 8			& 8			& 8			&	0\\      
 73						& WebMacro					& 5	&	5	&	5	& 		0					& 5.06	&  6		& 5		& 		0.05			& 5.06		& 7			& 5			&	0.07\\      
 74						& XMLAttributesImpl				& 8	&	8	&	8	& 		0					& 8		&  8		& 8		& 		0			& 8			& 8			& 8			&	0\\      
 75						& XMLChar					& 13	&	13	&	13	& 		0					& 13		&  13		& 13		& 		0			& 13			& 13			& 13			&	0\\      
 76						& XMLEntityManger				& 17.03&	18	&	17	& 		0.01					& 16.95	&  17		& 16		& 		0.01			& 16.96		& 17			& 16			&	0.01\\      
 77						& XMLEntityScanner				& 12	&	12	&	12	& 		0					& 12		&  12		& 12		& 		0			& 12			& 12			& 12			&	0\\      
 78						& XMLErrorReporter				& 5	&	5	&	5	& 		0					& 5		&  5		& 5		& 		0			& 5			& 5			& 5			&	0\\      
 79						& XObject						& 19	&	19	&	19	& 		0					& 19		&  19		& 19		& 		0			& 19			& 19			& 19			&	0\\      
 80						& XString						& 23.86&	24	&	23	& 		0.01					& 23.55	&  24		& 23		& 		0.02			& 23.75		& 24			& 23			&	0.01\\      
    %\hline 
    						\multicolumn{2}{l}{\textbf{Total}}						&1165.53	& 1181	&    1055	&		0.1069					& 1188.73		&	1224	&	1127	&	0.1153			& 1192.55		& 1234		& 1126		& 	0.1085\\
   %\hline
     \end{tabular}
 	\end{minipage}
    
    \label{table:Results}
\end{table*}

\section{Results}\label{sec:res}




\subsection{Is there an absolute best for DSSR, R+ and R?}
Figure~\ref{fig:stackedbar} presents the results of 80 randomly selected classes evaluated by the R, R+ and DSSR strategies in an intuitive normalized stacknar representation where projects are ranked according to the relative number of unique failures found by DSSR. As a first visual interpretation, it seems that, except in rare cases, all strategies find significantly the same number of uniques failures.

Table~\ref{table:Results} contains more detailed information: name of the classes, mean value, maximum number of unique failures, minimum number of unique failures and relative standard deviation for each of the 80 classes tested using R, R+ and DSSR strategy. The total value (table \ref{table:Results} last row) shows DSSR detects slightly more unique failuress (1192.55), on average, than R (1165.53) and R+ (1188.73). This represents 2.3\% on average than R and .3\% more than R+. It also shows that DSSR found a higher number of maximum unique failures (1234) and minimum unique failures (1126) than R (1181), (1055) and R+ (1224), (1127) respectively. This represents: 4.5\% improvements over R and .8\% over R+ for the maximum and 6.7\% improvement over R and .1\% decrease over R+ for the minimum. Eventually, the standard deviations are all of the order of magnitude of .1\% for all strategies.

The answer to this research question is thus that whereas DSSR produces a slightly higher number of unique failures, this is not significantly higher than R+. We can thus say that R+ and DSSR are better choices than R as an absolute strategy, but that neither significantly outperforms the other. 


\begin{figure*}[ht]
\centering
\includegraphics[width=10cm]{pie2.png}
\caption{Result Categories.}
\label{fig:pie}
\end{figure*}

\subsection{Are there classes for which either one of the strategies provides better results?}

Results can be split into six different categories as shown in figure~\ref{fig:pie}. The first category is the largest where each strategy performed equally well and found the same number of unique failures after $10^5$ tests. It contain 50 classes (62\% of the experiments).

The second category contains 11 (14\%) classes where R performed better than DSSR and R+. 

The third category contains 7 classes where (9\%) R+ performed better than DSSR and R. 

 The fourth category contain 7 classes (9\%) where DSSR performed better than R and R+. 

The fifth category contain only one class (1\%) where both DSSR and R found an equal number of unique failures and performed better than R+. 

The sixth category contain 4 classes (5\%) where DSSR and R+ found an equal number of unique failures and performed better than R. 

No class is found for which R performs equal to R+.

The answer to this research question is that most classes (62\%) did not exhibit significantly different behaviors independent of the strategy. In 38\% of the cases, though, one or two strategies work better than the other(s). In particular, 14\% of the classes performed better with R, 9\% of the classes performed better with R+ and 9\% with DSSR. This shows that the assumptions made when developing R+ --- some border values are more bug-prone --- and DSSR --- failure domains are connected --- only verify in a minority of cases and are code-dependent.



\subsection{Can we pick the best default strategy between R, R+ and DSSR?}
With the data presented in this section, it is not possible to pick a best strategy for all classes. In most cases, results are not different from one strategy to another, but in the other cases, the best strategy is very much dependent on the tested code and none of R, R+ and DSSR are signifficantly better than the other two on larger classes. 

In the next section we also discuss other factors that influence the outcome of such a question such as time and 




%%%%%%%%%%%%%%%%%    DISCUSSION   %%%%%%%%%%%%%%%%%%%%

\section{Discussion}\label{sec:discussion}
\textbf{Time taken by DSSR strategy, Random strategy and Random plus strategy to execute tests:}
To execute an equal number of test cases, DSSR takes slightly more time (between 5 and 10\% overhead) than both pure random and random plus. This is due to maitaining sets of interesting values. The overhead is dependent on our implementation and could also be reduced if needed. 

\textbf{Effect of test duration and number of tests on the results:}
All three techniques have the same potential for finding bugs. If testing infinitely, all techniques should find the same number of unique failures.
So the results will converge the longer (resp. the more tests) testing sessions contain. We suspect however that some of the unique failures found would be extremely long to find using random or random+ only. Further experiments should confirm this point.


\textbf{Effect of number of faults on results:} 
We found that the DSSR strategy performs better when the number of faults is higher in the code. The reason seems to be that when there are more faults, their domains are more connected and DSSR then works better. Further studies might use historical data to pick the best strategy.

\textbf{DSSR strategy dependence on finding the first unique failures early enough:}
During the experiments we found that if the unique failures is not found  quickly enough there is no value added to the list of interesting values and then the test is equivalent random testing. This means that better ways of populating failure-inducing values are needed to leverage DSSR better. As an example, the following piece of code would be unlikely to fail under the current setting:

\begin{lstlisting}
public void test(float value){
 if(value == 34.4445) { 
  10/0; 
 }
}
\end{lstlisting}

In this case, we could add constant literals from the SUT to the list of interesting values in a dynamic fashion. These literals can be obtained from the constant pool in the class files of the SUT.

In the example above the value 34.4445 and its surrounding values would  be added to the list of interesting values before the test starts and the DSSR strategy would find the unique failure right away.

\textbf{DSSR strategy and coverage:} Random strategies typically achieve high level of coverage~\cite{Oriol2010}. It might also be interesting to compare R, R+ and DSSR with respect to the achieved coverage or even to use a DSSR variant that adds a new interesting value and its neighbors when a new branch is reached.


\textbf{Threats to validity:} As usual with such empirical studies, the present work might suffer from a non-representative selection of classes.
The present selection was however made through random selection and objective criteria and it seems unlikely that they would not be representative.

The parameters of the study might also have prompted incorrect results. This is however unlikely due to previous results on random testing~\cite{Oriol2012}.



%%%%%%%%%%%%%%%%%    RW   %%%%%%%%%%%%%%%%%%%%

\section{Related Work}\label{sec:rw}

Random testing is a popular technique with simple algorithm but proven to find subtle faults in complex programs and Java libraries~\cite{Pacheco2005, Csallner2004, Claessen2000a}. Its simplicity, ease of implementation and efficiency in generating test cases make it a best choice for test automation~\cite{Hamlet1994}. Few of the well known automated tools based on random strategy includes Jartege~\cite{Oriat2004}, Eclat~\cite{Pacheco2005}, JCrasher~\cite{Csallner2004}, AutoTest \cite{Ciupa2007},~\cite{Ciupa2008a} and YETI~\cite{Oriol2010, Oriol2012}  which was used to conduct this research study.

In pursuit of better results and lower overhead, many variations of random strategy have been proposed~\cite{Chen2010, Chen2005, Chan2002, Chen2004a, Chen2003}. Adaptive random testing (ART), Quasi-random testing (QRT) and Restricted Random testing (RRT) achieved better results by selecting test inputs random but evenly spread across the input domain. Similarly Mirror ART and ART through dynamic partitioning increased the performances by reducing the overhead of ART. One of the main reason behind the better performance of these strategies is that even spread of test input increases the chance of exploring the fault patterns present in the input domain.
 
The random+ (R+) strategy~\cite{Leitner2007} is a variation of the random strategy in which interesting values, beside pure random values, are added to the list of test inputs. These interesting values includes border values~\cite{Beizer1990} which have high tendency of finding faults in the given SUT. Results conducted with R+ strategy show significant improvement of pure random strategy. DSSR strategy also rely on R+ strategy in the start until any fault is found where it switches to spot sweeping strategy.

It is interesting that numerous efforts have been made to discover the fault patterns~\cite{Chen2010, Chen2005, Chan2002, Chen2004a, Chen2003}, etc. but in our knowledge, none has been published on covering/sweeping all the faults lying in a specific pattern once it has been discovered.

A common practice to evaluate performance of newly created or existing strategies is to compare the results obtained (theoretically and empirically) after applying them to similar programs~\cite{Gutjahr1999, Duran1984, Hamlet1990}. Arcuri et al., stress on the use of random testing as a comparison baseline to assess other test strategies \cite{Arcuri2012}. We followed similar procedure and evaluated DSSR against R and R+ under constant conditions.

Qualitas Corpus~\cite{Tempero2010} is a collection of open source java programs maintained for independent empirical research~\cite{Oriol2012, Tempero2010a, Tempero2008}. These projects are carefully selected that spans across the whole set of java applications.


%%%%%%%%%%%%%%%%%    CONCLUSIONS   %%%%%%%%%%%%%%%%%%%%


\section{Conclusions}\label{sec:conc}
The main goal of the present study was to develop a new random strategy which could find more faults in lower number of test cases that would leverage on the assumption that in a significant number of classes, failure domains are contiguous or are very close by. The result is the dirt spot sweeping strategy, a strategy which adds neighbouring values of failure values to a set of preferred values. 

We implemented DSSR as a strategy for the random testing tool YETI and tested thirty times each one of 80 classes from the Qualitas Corpus with each of the three strategies DSSR, R, and R+.We found that for 68\% of the classes all three strategies find the same unique failures, for 9\% of the classes random+ performs better, for 14\% pure random performs better, and for 9\% DSSR performs better.
Overall, DSSR also found 2.3\% more unique failures than random and .3\% more unique failures than random+.

Overall DSSR is a strategy that uncovers more unique failures than both random and random+ strategies. It however achieves this with a 5-10\% overhead which makes it an unlikely candidate as a default strategy for a random testing tool. It however yields encouraging results and advocates to develop the technique further for settings in which it is significantly better than both R+ and R.

%%%%%%%%%%%%%%%%%    ACKNOWDLEGEMENT   %%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}
The authors thank the Department of Computer Science, University of York for its financial support through the Departmental Overseas Research Scholarship (DORS) award. Authors also thanks Richard Page, for his valuable help and generous support.




%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
