\documentclass[conference]{IEEEtran}

\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage{float}
\usepackage{url}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}
\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=Java,
basicstyle=\ttfamily,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
%numbers=left,
numberstyle=\tiny\color{black},
stepnumber=2,
numbersep=10pt,
tabsize=4,
showspaces=false,
showstringspaces=false}



\restylefloat{table}

\ifCLASSINFOpdf
\else
\fi
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
\title{Dirt Spot Sweeping Random Strategy: an upgraded version of Random+ Testing}
\author{\IEEEauthorblockN{Mian Asbat Ahmad}
\IEEEauthorblockA{Department of Computer Science\\
University of York\\
York, United Kingdom\\
Email: mian.ahmad@york.ac.uk}

\and
\IEEEauthorblockN{Manuel Oriol}
\IEEEauthorblockA{\begin{tabular}{c c}
ABB Corporate Research & Department of Computer Science\\
Industrial Software Systems & University of York\\
Baden-Dattwil, Switzerland & York, United Kingdom\\
Email: manuel.oriol@ch.abb.com & manuel.oriol@york.ac.uk
\end{tabular}}}
\maketitle


%%%%%%%%%%%%%%%%%    ABSTRACT   %%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The paper presents a new and improved automated random testing technique named as Dirt Spot Sweeping Random (DSSR) strategy based on the rationale that, ``when failures lies in the contiguous locations across the input domain, the effectiveness of random testing can be further improved through diversity of test cases". The DSSR strategy selects neighbouring values for the subsequent tests on identification of failure. Resultantly, selected values sweep around the failure leading to the discovery of new failures in the vicinity. To evaluate the effectiveness of DSSR strategy a total of 60 classes (35,785 lines of code), each class with 30 x $\boldsymbol{10^5}$ calls, were tested by Random (R), Random+ (R+) and DSSR strategies. T-Test analysis showed significantly better performance of DSSR compared to R strategy in 17 classes and R+ strategy in 9 classes. In the remaining classes all the three strategies performed equally well. Numerically, the DSSR strategy found 43 and 12 more unique failures than R and R+ strategies respectively. This study comprehends that DSSR strategy will have a profound positive impact on the failure-finding ability of R and R+ testing.



%T-Test analysis of the experimental data showed significantly better performance of DSSR compared to Random (R) and Random+ (R+) 
%It is evaluated against random (R) and random+ (R+) strategies by testing 60 classes (35,785 lines of code) with one hundred thousands ($10^5$) calls for each session, 30 times for each strategy. The results indicate that for 31 classes, all three strategies find the same unique failures. We analysed the 29 remaining classes using t-tests and found that for 7 classes DSSR is significantly better than both R+ and R, for 8 classes it performs similarly to R+ and is significantly better than R, and for 2 classes it performs similarly to R and is better than R+. In all other cases, DSSR, R+ and R do not perform significantly differently. Numerically, the DSSR strategy finds 43 more unique failures than R and 12 more unique failures than R+.

\end{abstract}
\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%    INTRODUCTION   %%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
The success of a software testing technique is mainly dependent on the number of failures it discovers in the SUT. An effective and efficient testing process discovers the maximum number of failures in minimum possible test cases and time. Exhaustive testing
%, where software is tested against all possible inputs, 
is not always feasible, 
%because of the large size of the input domain, limited resources and strict time constraints.
therefore, strategies in automated testing tools are developed with the aim to select more failure-finding test cases from the whole input domain. 

Chan et al.~\cite{Chan1996} discovered that there are patterns of failure-causing inputs across the input domain. They divided these into point, block and strip patterns on the basis of their occurrence. Chen~\cite{Chen2008} found that the performance of random testing can be increased by altering the technique of test case selection. Moreover, he also found that the performance increased up to 50\% when test input was selected evenly across the whole input domain. This was mainly attributed to the better distribution of input, which increased the chances of selecting inputs from failure patterns. 
%Restricted Random Testing \cite{Chan2002}, Feedback-Directed Random Testing \cite{Pacheco2007a}, Mirror Adaptive Random Testing \cite{Chen2003} and Quasi Random Testing \cite{Chen2005} have adopted similar approach for increasing the performance by evenly distributing the test cases in the input domain.

Based on the assumption that in a significant number of classes, failure domains are contiguous, the Dirt Spot Sweeping\footnote{The name refers to the cleaning robots strategy, which insists on places where dirt has been found in large amount.} is devised to give higher priority to the failure domains for identification of new failures efficiently. 
%It starts as a random+ strategy --- a random strategy focusing more on boundary values \cite{Ciupa2008}. When a new failure is found, it increases the chance of finding more failures using neighbouring values. When a failure is identified, the DSSR strategy selects neighbouring values for the subsequent tests. Resultantly, selected values sweep around the failure leading to the discovery of new failures in the vicinity. 
%DSSR strategy is an extension of R+ strategy, therefore, it has the potential to find all failures to be discovered by R+ and R. In addition, it is expected to be faster in finding failures in classes where failure domains are contagious. 
The DSSR strategy is implemented in the York Extensible Testing Infrastructure (YETI)\footnote{\url{http://www.yetitest.org}}, a random testing tool. To evaluate the effectiveness of DSSR strategy a total of 60 classes (35,785 lines of code) of 32 different projects from the Qualitas Corpus\footnote{\url{http://www.qualitascorpus.com}}, each class with 30 x ${10^5}$ calls, were tested by R, R+ and DSSR strategies.
%To evaluate our approach, we tested 30 times each one of the 60 classes of 32 different projects from the Qualitas Corpus\footnote{\url{http://www.qualitascorpus.com}} with each of the three strategies R, R+ and DSSR. 

%We implemented the DSSR strategy in the random testing tool YETI\footnote{\url{http://www.yetitest.org}}. To evaluate our approach, we tested 30 times each one of the 60 classes of 32 different projects from the Qualitas Corpus\footnote{\url{http://www.qualitascorpus.com}} with each of the three strategies R, R+ and DSSR. We observed that for 53\% of the classes all three strategies find the same unique failures, for remaining 47\% DSSR strategy perform up to 33\% better than random strategy and up to 17\% better than random+ strategy.
%We also validated the approach by comparing the significance of these results using t-tests and found out that for 7 classes DSSR was significantly better than both R+ and R, for 8 classes DSSR performed similarly to R+ and significantly better than R, while in 2 cases DSSR performed similarly to R and significantly better than R+. In all other cases, DSSR, R+ and R do not seem to perform significantly differently.
%Numerically, the DSSR strategy found 43 more unique failures than R and 12 more unique failures than R+ strategy. 

This paper is organized as follows: Section~\ref{sec:dssr} describes the DSSR strategy. Section~\ref{sec:imp} presents implementation of the DSSR strategy. Section~\ref{sec:eval} explains the experimental setup. Section~\ref{sec:res} reveals results of the experiments. Section~\ref{sec:discussion} discusses the results. Section~\ref{sec:rw} presents related work and Section~\ref{sec:conc} concludes the study.




%%%%%%%%%%%%%%%%%    DIRT SPOT SWEEPING STRATEGY  %%%%%%%%%%%%%%%

\section{Dirt Spot Sweeping Random Strategy}\label{sec:dssr}
Dirt Spot Sweeping Random strategy combines the R+ strategy with a Dirt Spot Sweeping (DSS) functionality. It is based on two intuitions. First, boundaries have interesting values and using these values in isolation can provide high impact on test results. Second, failures reside in contiguous patterns. If this is true, DSS increases the performance of the test strategy. Before presenting the details of the DSSR strategy, it is pertinent to review briefly the R and the R+ strategy.

\subsection{Random Strategy (R)}
The random strategy is a black-box testing technique in which the SUT is executed using randomly selected test data. Test results obtained are compared to the defined oracle, using SUT specifications in the form of contracts or assertions. In the absence of contracts and assertions, the exceptions defined by the programming language are used as test oracles. Because of its black-box testing nature, this strategy is particularly effective in testing softwares where the developers want to keep the source code secret~\cite{Chen2010}. The generation of random test data is comparatively cheap and does not require too much intellectual and computational effort~\cite{Ciupa2009, Ciupa2008}. It is mainly for this reason that various researchers have recommended R strategy in automated testing tools \cite{Ciupa2008}. YETI \cite{Oriol2010a}, AutoTest \cite{Leitner2007, Ciupa2007}, QuickCheck \cite{Claessen2000}, Randoop \cite{Pacheco2007} and JArtege \cite{Oriat2004} are some of the most common automated testing tools based on R strategy.

\indent Efficiency of random testing was made suspicious with the intuitive statement of Myer and Sandler~\cite{Myers2004} who termed random testing as one of the poorest methods for software testing. However, experiments performed by various researchers \cite{Ciupa2007, Duran1981, Duran1984, Hamlet1994, Ntafos2001} have proved experimentally that random testing is simple to implement, cost effective, efficient and free from human bias as compared to its rival techniques.


\subsection{Random Plus Strategy (R+)}
The random+ strategy~\cite{Leitner2007} is an extension of the R strategy. It uses some special pre-defined values which can be simple boundary values or values that have high tendency of finding failures in the SUT. Boundary values are the values in the start and end of a particular type \cite{Beizer1990}. For instance, such values for \verb+int+ could be \verb+MAX_INT+, \verb+MAX_INT-1+, \verb+MAX_INT-2+, \verb+MAX_INT-3+ \verb+0+, \verb+MIN_INT+, \verb-MIN_INT+1-, \verb-MIN_INT+2-, and \verb-MIN_INT+3-. The tester might also add some other special values that are considered effective in finding failures in the SUT. For example, if a program under test has a loop from \verb+-50 to 50+ then the tester can add \verb+-55 to -45, -5 to 5 and 45 to 55+ to the pre-defined list of special values. This static list of interesting values is manually updated before the start of the test and has a high priority (10\%) than selection of random values because of more relevance and better chances of finding failures.


\subsection{Dirt Spot Sweeping (DSS)}
Chan et al.~\cite{Chan1996} found that there are patterns of failure-causing inputs across the input domain. They divided these patterns into three types called point, block and strip patterns (figure \ref{fig:patterns}) and argued that a strategy has more chances of hitting the failure patterns if test cases are selected farther from each other. Other researchers~\cite{Chan2002, Chen2003, Chen2005}, also tried to generate test cases further away from one another targeting these patterns and achieved better performance. Such increase in performance indicates that failures more often occur contiguous across the input domain. 
\begin{figure}[ht]                                    
\centering
\includegraphics[width= 8cm,height=2.5cm]{ART_Patterns.png}
\caption{Failure patterns across input domain~\cite{Chen2008}}
\label{fig:patterns}
\end{figure}
In DSS, if a value reveals failure from the block or strip pattern then for the selection of the next test value, DSS may not look farthest from the known value but picks the closest value to find another failure from the same region. DSSR strategy relies on DSS that comes into action when a failure is found in the system. On finding a failure, it immediately adds the value causing the failure and its neighbouring values to the existing list of interesting values. For example, in a program when the \verb+int+ type value of \verb+50+ causes a failure in the system then DSS will add values from \verb+47 to 53+ to the list of interesting values. The addition of neighbouring values will explore other failures present in the block or strip domain of the SUT. The list of interesting values in DSSR strategy is dynamic and changes during the test execution of each program as against R+ where the list remains static.

\begin{figure}[ht]
\centering
\includegraphics[width=6.6cm,height=2cm]{block2.png}
\caption{DSSR strategy covering block and strip pattern}
\label{fig:block2}
\end{figure}

Figure \ref{fig:block2}, shows how DSS explores the failures residing in the block and strip patterns of a program. The coverage of block and strip pattern is shown in spiral form because first failure leads to second, second to third and so on till the end. In case the failure is positioned on the point pattern then the added values may not be effective because point pattern is only an arbitrary failure point in the whole input domain.

\subsection{Structure of the Dirt Spot Sweeping Random Strategy}

The DSSR strategy continuously tracks the number of failures during the execution of the test. This tracking is done in a very effective way with zero or minimum overhead~\cite{Leitner2009}. The test execution is started by R+ strategy and continues till a failure is found in the SUT after which the program copies the values leading to the failure as well as the surrounding values to the variable list of interesting values. 

\begin{figure}[ht]
\centering
%\includegraphics[width=\columnwidth]{flowchart1.pdf}
\includegraphics[width= 9cm,height=8cm]{flowchart4.pdf}
\caption{Working mechanism of DSSR strategy}
\label{fig:Working_DSSS}
\end{figure}

Both the variables \verb+currentFaults+ and \verb+oldFaults+ are initialised to 0 at the start of the test, when a fault is found the \verb+currentFaults+ value is incremented which make the condition true. The flowchart presented in Figure~\ref{fig:Working_DSSS} depicts the case when fault is caused by a primitive type value. The DSSR strategy identifies its type and adds values only of that particular type to the list of interesting values. The resultant list of interesting values provides relevant test data for the remaining test session and the generated test cases are more targeted towards finding new faults around the existing fault in the given SUT.

%Boundary and other special values that have a high tendency of finding failures in the SUT are added to the list of interesting values by random+ strategy prior to the start of test session where as in DSSR strategy the failure-finding and its surrounding values are added at runtime when a failure is found. 

%Table \ref{table:addvalues} presents the values are added to the list of interesting values when a failure is found. In the table the test value is represented by X where X can be int, double, float, long, byte, short, char and String. All values are converted to their respective types before adding them to the list of interesting values.

\begin{table}[ht]
%\scriptsize
\caption{Data types and Corresponding values to be added} % title of Table
\centering % used for centering table
\begin{tabular}{| l | l |} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Data Type & Values to be added\\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
\multirow{1}{*}{X is int, double, float, } & X,  X+1, X+2, X+3,  \\ % inserting body of the
\multirow{1}{*}{long, byte, short \& char} & X-1, X-2, X-3 \\ 

\hline
\multirow{8}{*}{X is String} & X\\ % inserting body of the table

& X + ``  "\\ % inserting body of the table
& ``  " + X \\ % inserting body of the table
& X.toUpperCase() \\
& X.toLowerCase() \\
& X.trim() \\
& X.substring(2) \\
& X.substring(1, X.length()-1) \\
\hline
\multirow{1}{*}{X is object of user} & Call its constructor recursively \\ % inserting body of the
\multirow{1}{*}{defined class} & until empty or primitive values \\[1ex]
\hline
\hline %inserts single line
\end{tabular}
\bigskip
\label{table:addvalues} % is used to refer this table in the text
\end{table}





Table \ref{table:addvalues} presents the data types with the corresponding neighbouring values to be added to the list of interesting values. In the table the test value is represented by X where X can be int, double, float, long, byte, short, char and String. All values are converted to their respective types before adding them to the list of interesting values.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPLANATION OF DSSR STRATEGY %%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Explanation of DSSR strategy on a concrete example}
The DSSR strategy is explained through a simple program seeded with three faults. The first fault is a division by zero exception denoted by 1 while the second and third faults are failing assertion denoted by 2 and 3 respectively in the given program. It is followed by the description of how the strategy performs execution.

\begin{lstlisting}
/** 
* Calculate square of given number 
* and verify result. 
* @author (Mian and Manuel)
*/
public class Math1 {
 public void calc (int num1) {
  // Square num1 and store result. 
  int result1 = num1 * num1;
  int result2 = result1 / num1; // 1
  assert result1 >= num1; // 2
  assert Math.sqrt(result1) == num1; // 3
 } 
}
\end{lstlisting}
In the above code, one primitive variable of type \verb+int+ is used; therefore, the input domain for DSSR strategy is from \verb+-2,147,483,648 to 2,147,483,647+. The strategy further select values (\verb+0, Integer.MIN_VALUE+ \& \verb+Integer.MAX_VALUE+) as interesting values that are prioritized for selection as test inputs. 
As the test starts, three faults are quickly discovered by DSSR strategy in the following order:

\indent \textbf{fault 1:} The strategy selects value \verb+0+ for variable \verb+num1+  in the first test case because \verb+0+ is available in the list of interesting values and its priority is higher than other values. This will cause Java to generate division by zero exception (1). After discovering the fault, the strategy adds its surrounding values to the list of interesting values i.e.\verb+ 1, 2, 3, -1, -2, -3+. 

\indent \textbf{fault 2:} After a few tests the DSSR strategy may select \\ \verb+Integer.MAX_VALUE+ for variable \verb+num1+  from the list of interesting values leading to the discovery of the 2nd fault because int variable \verb+result1+ will not be able to store the square of \verb+Integer.MAX_VALUE+. Instead of the actual square value Java assigns \verb+1+ (Java language rule) to variable \verb+result1+ that will lead to the violation of the next assertion (2).

\indent \textbf{fault 3:} In the third test case the strategy may pick \verb+-3+ as a test value, which is added to the list of interesting values after the discovery of first fault. This may lead to the third fault where assertion (3) fails because the square root of \verb+9+ is \verb+3+ against the input value of -3.



The above process explains that including the border, fault-finding and surrounding values to the list of interesting values in DSSR strategy leads to the discovery of faults quickly and in fewer tests as compared to R and R+ strategies. The R and R+ strategies takes more time and number of tests to discover the second and third faults because the search for new unique faults starts again randomly in spite of the fact that the remaining faults are very close to the first fault.


%%%%%%%%%%%%%%%%%    IMPLEMENTATION OF DSSR STRATEGY   %%%%%%%%%%%%


\section{Implementation of the DSSR \\ strategy}\label{sec:imp}

Implementation of the DSSR strategy is made in the YETI, an open-source automated random testing tool. YETI, coded in Java, is capable of testing systems developed in procedural, functional and object-oriented languages. Its language-agnostic meta model enables it to test programs written in multiple languages including Java, C\#, JML and .NET. The core features of YETI include easy extensibility for future growth, capability to test programs using multiple strategies, high speed tests execution, real time logging, GUI support and auto generation of test report at the end of test session~\cite{Oriol2012, Oriol2011}.

YETI can be divided into three loosely coupled main parts: the core infrastructure, the language-specific bindings and the strategies. The core infrastructure contains representation for routines, a group of types and a pool of specific type objects. The language-specific bindings contain the code to make the calls and process the results. The strategies define the procedure of selecting the modules (classes), the routines (methods) and generation of values for instances involved in the routines. By default, YETI uses the R strategy, if no particular strategy is defined during test initialization. It also enables the user to control the probability of using null values and the percentage of newly created objects for each test session. In addition to GUI, YETI also provides extensive logs of the test session for more in-depth analysis. 

%The class hierarchy is shown in Figure \ref{fig:hierarchyofDSSR}.

%\begin{figure}[h]
%\centering
%\includegraphics[width=4cm,height=5cm]{hierarchy.pdf}
%\caption{Class Hierarchy of DSSR in YETI}
%\label{fig:hierarchyofDSSR}
%\end{figure}





%%%%%%%%%%%%%%%%%    EVALUATION   %%%%%%%%%%%%%%%%%%%%


\section{Evaluation}\label{sec:eval}

The DSSR strategy is experimentally evaluated by comparing its performance with that of R and R+ strategy ~\cite{Leitner2007}. General factors such as system software and hardware, YETI specific factors like percentage of null values, percentage of newly created objects and interesting value injection probability have been kept constant in the experiments.

\subsection{Research questions}
For evaluating the DSSR strategy, the following research questions were formulated and addressed in this study:
\begin{enumerate}
\item Is there an absolute better strategy among R, R+ and DSSR strategies?
\item Are there specific classes for which any of the three strategies provide better results?
\item Can we pick the best default strategy among R, R+ and DSSR strategies?
\end{enumerate}



\subsection{Experiments}

To evaluate the performance of DSSR we performed extensive testing of programs from the Qualitas Corpus~\cite{Tempero2010}. The Qualitas Corpus is a curated collection of open source Java projects designed with the aim of helping empirical research in  software engineering. These projects have been collected in an organised form containing both the source and binary forms. Version 20101126 containing 106 open source Java projects was used in our experiments. From 32 randomly selected projects, 60 classes were selected at random with the help of automated pseudo-random generator. The selected classes produced at least one failure and timed out within the testing session of not more than 10 minutes. Every class was tested thirty times by each strategy (R, R+, DSSR). Test details of the classes are presented in Table~\ref{table:Results}. Programs tested at random typically fail most of the times as a result of large number of calls. Therefore, it is necessary to cluster failures that likely represent the same failure. The traditional way is to compare the full stack traces and error types and use this as an equivalence class~\cite{Ciupa2007, Oriol2012} called a unique failure. The same concept of unique failure has been adapted in the present study.  

Every class is evaluated through $10^5$ calls in each test session.\footnote{The total number of tests is thus $60\times 30\times 3 \times 10^5 = 540\times 10^6~tests$.} 
Because of the absence of the contracts and assertions in the code under test, undeclared exceptions were considered as unique failures in accordance with previous studies~\cite{Oriol2012}.


%\begin{table}[h]
%\caption{Name and versions of 32 Projects randomly selected from the Qualitas Corpus for the experiments}
%\centering
%\begin{tabular}{|r|l|r|r|}
%\hline
%S. No& 	Project Name	& 	Version		&	Size (MB)\\
%\hline
%1	&	apache-ant	&	1.8.1			&	59\\
%2	&	antlr			&	3.2			&	13\\
%3	&	aoi			&	2.8.1			&	35\\
%4	&	argouml		&	0.30.2		&	112\\
%5	&	artofillusion	&	281			&	5.4\\
%6	&	aspectj		&	1.6.9			&	109.6\\
%7	&	axion		&	1.0-M2		&	13.3\\
%8	&	azureus		&	1			&	99.3\\
%9	&	castor		&	1.3.1			&	63.2\\
%10	&	cayenne		&	3.0.1			&	4.1\\
%11	&	cobertura		&	1.9.4.1		&	26.5\\
%12	&	colt			&	1.2.0			&	40\\
%13	&	emma		&	2.0.5312		&	7.4\\
%14	&	freecs		&	1.3.20100406	&	11.4\\
%15	&	hibernate		&	3.6.0			&	733\\
%16	&	hsqldb		&	2.0.0			&	53.9\\
%17	&	itext			&	5.0.3			&	16.2\\
%18	&	jasml		&	0.10			&	7.5 \\
%19	&	jmoney		&	0.4.4			&	5.3\\
%20	&	jruby			&	1.5.2			&	140.7\\
%21	&	jsXe			&	04\_beta		&	19.9\\
%22	&	quartz		&	1.8.3			&	20.4\\
%23	&	sandmark		&	3.4			&	18.8\\
%24	&	squirrel-sql	&	3.1.2			&	61.5\\
%25	&	tapestry		&	5.1.0.5		&	69.2\\
%26	&	tomcat		&	7.0.2			&	24.1\\
%27	&	trove			&	2.1.0			&	18.2\\
%28	&	velocity		&	1.6.4			&	27.1\\
%29	&	weka		&	3.7.2			&	107\\
%30	&	xalan		&	2.7.1			&	85.4\\
%31	&	xerces		&	2.10.0		&	43.4\\
%32	&	xmojo		&	5.0.0			&	15\\
%\hline
%\end{tabular}
%\bigskip
%\label{table:projects}
%4\end{table}



All tests were performed with a 64-bit Mac OS X Lion Version 10.7.4 running on 2 x 2.66 GHz 6-Core Intel Xeon processor with 6 GB (1333 MHz DDR3) of RAM. YETI runs on top of the Java\texttrademark  SE Runtime Environment [version 1.6.0\_35]. The machine took approximately 100 hours to process the experiments.


\subsection{Performance measurement criteria}
Various measures including the E-measure (expected number of failures detected), P-measure (probability of detecting at least one failure) and F-measure (number of test cases used to find the first failure) have been used by researchers to find the effectiveness of the R strategy. The P-measure has been heavily criticized and is not considered effective measuring technique ~\cite{Chen2008} while the F-measure has been often used by various researchers~\cite{Chen1996, Chen2004}. In our initial experiments the F-measure was used to evaluate the efficiency. However it was realized that this was not the right choice. In some experiments a strategy found the first failure quickly than the other but on completion of test session that very strategy found lower number of total failures than the rival strategy. The preference given to a strategy by F-measure, because it finds the first failure quickly, without giving due consideration to the total number of failures is not fair~\cite{Liu2012}.


  
The literature review revealed that the F-measure is used where testing stops after identification of the first failure and the system is given back to the developers to remove the failure. Currently automated testing tools test the whole system and print all discovered failures in one go, therefore, F-measure is not the favourable choice. In our experiments, performance of the strategy was measured by the maximum number of failures detected in the SUT by a particular number of test calls \cite{Pacheco2007a, Ciupa2007}. This measurement, similar to E-measure, is effective because it considers the performance of the strategy when all other factors are kept constant.

%%%%%%%%%%%%%%%%%    RESULTS   %%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
\centering
\includegraphics[width=17cm]{DssrImprove.png}
\caption{Improvement of DSSR strategy over R and R+ strategy.}
\label{fig:LineChart}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{table*} [htp!]
  \scriptsize
    \def\arraystretch{1.2}
     \caption{Complete results for R, R+ and DSSR strategies. Results present Serial Number (S.No), Class Name, Line of Code (LOC), mean, maximum number of unique failures, minimum number of unique failures and relative standard deviation for each Random (R), Random+ (R+) and Dirt Spot Sweeping Random (DSSR) strategies.}
	%\begin{minipage}[h]{\textwidth}\centering
	\makebox[\textwidth]{
 	\begin{tabularx}{0.91 \textwidth}{|r|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
      %\aline
      \hline
      \multirow{2}{*}{S. No}		& \multirow{2}{*}{Class Name}		& \multirow{2}{*}{LOC}	& \multicolumn{4}{c|}{R}	&	\multicolumn{4}{c|}{R+}	&	\multicolumn{4}{c|}{DSSR}	\\
      \cline{4-15} 
      						&						&			& Mean 	& Max	& Min 	& 	R-STD				& Mean 	& Max 	& Min 	&	 R-STD 			& Mean 		& Max 		& Min		& R-STD \\
\hline
1						& ActionTranslator			&709		& 96		&	96	&	96	& 	0					& 96		& 96 		& 96		& 		0			& 96			& 96			& 96			&	0\\     
2						& AjTypeImpl				&1180		& 80		&	83	&	79	& 	0.02					& 80		& 83 		& 79		& 		0.02			& 80			& 83			& 79			&	0.01\\      
\textbf{3}					& \textbf{Apriori}			&\textbf{292}	& \textbf{3}&	\textbf{4}	&\textbf{3}	& \textbf{0.10}			& \textbf{3}& \textbf{4} 		& \textbf{3}& \textbf{0.13}	& \textbf{3}	& \textbf{4}	& \textbf{3}	&\textbf{0.14}\\      
4						& BitSet					&575		& 9		&	9	&	9	& 	0					& 9		& 9 		& 9		& 		0			& 9			& 9			& 9			&	0\\       
5						& CatalogManager			&538		& 7		&	7	&	7	& 	0					& 7		& 7 		& 7		& 		0			& 7			& 7			& 7			&	0\\    
\textbf{6}					& \textbf{CheckAssociator}	&\textbf{351}	& \textbf{7}	&	\textbf{8}	&	\textbf{2}	& 	\textbf{0.16}					& \textbf{6}		& \textbf{9} 		& \textbf{2}		& 		\textbf{0.18}			& \textbf{7}			& \textbf{9}			& \textbf{6}			&	\textbf{0.73}\\    
\textbf{7}						& \textbf{Debug}					&\textbf{836}		& \textbf{4}		&	\textbf{6}	&	\textbf{4}	& 	\textbf{0.13}					& \textbf{5}		& \textbf{6} 		& \textbf{4}		& 		\textbf{0.12}			& \textbf{5}			& \textbf{8}			& \textbf{4}			&	\textbf{0.19}\\       
\textbf{8}						& \textbf{DirectoryScanner}			&\textbf{1714}		& \textbf{33}		&	\textbf{39}	&	\textbf{20}	& 	\textbf{0.10}					& \textbf{35}		& \textbf{38} 		& \textbf{31}		& 		\textbf{0.05}			& \textbf{36}			& \textbf{39}			& \textbf{32}			&	\textbf{0.04}\\      
9						& DiskIO					&220		& 4		&	4	&	4	& 	0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
10						& DOMParser				&92			& 7		&	7	&	3	& 	0.19					& 7		& 7 		& 3		& 		0.11			& 7			& 7			& 7			&	0\\      
11						& Entities					&328		& 3		&	3	&	3	& 	0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
12						& EntryDecoder			&675		& 8		&	9	&	7	& 	0.10					& 8		& 9 		& 7		& 		0.10			& 8			& 9			& 7			&	0.08\\   
13						& EntryComparator			&163		& 13		&	13	&	13	& 	0					& 13		& 13 		& 13		& 		0			& 13			& 13			& 13			&	0\\      
14						& Entry					&37			& 6		&	6	&	6	& 	0					& 6		& 6 		& 6		& 		0			& 6			& 6			& 6			&	0\\   
15						& Facade					&3301		& 3		&	3	&	3	& 	0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\   
16						& FileUtil					&83			& 1		&	1	&	1	& 	0					& 1		& 1 		& 1		& 		0			& 1			& 1			& 1			&	0\\      
17						& Font					&184		&12		&	12	&	11	& 	0.03					& 12		& 12 		& 11		& 		0.03			& 12			& 12			& 11			&	0.02\\        
18						& FPGrowth				&435		& 5		&	5	&	5	& 	0					& 5		&  5		& 5		& 		0			& 5			& 5			& 5			&	0	\\       
19						& Generator				&218		& 17		&	17	&	17	& 	0					& 17		& 17 		& 17		& 		0			& 17			& 17			& 17			&	0	\\      
\textbf{20}						& \textbf{Group}					&\textbf{88}			& \textbf{11}		&	\textbf{11}	&	\textbf{10}	& 	\textbf{0.02}					& \textbf{10}		& \textbf{4} 		& \textbf{11}		& 		\textbf{0.15}			& \textbf{11}			& \textbf{11}			& \textbf{11}			&	\textbf{0}	\\      
21						& HttpAuth				&221		& 2		&	2	&	2	& 	0					& 2		& 2 		& 2		& 		0			& 2			& 2			& 2			&	0	\\         
\textbf{22}						& \textbf{Image}					&\textbf{2146}		& \textbf{13}		&	\textbf{17}	&	\textbf{7}	& 	\textbf{0.15}					& \textbf{12}		& \textbf{14} 		& \textbf{4}	& 		\textbf{0.15}			& \textbf{14}			& \textbf{16}			& \textbf{11}			&	\textbf{0.07}\\        
23						& InstrumentTask			&71			& 2		&	2	&	1	& 	0.13					& 2		& 2 		& 1		& 		0.09			& 2			& 2			& 2			&	0	\\    
24						& IntStack					&313		& 4		&	4	&	4	& 	0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0	\\      
25						& ItemSet					&234		& 4		&	4	&	4	& 	0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0	\\       
26						& Itextpdf					&245		& 8		&	8	&	8	& 	0					& 8		&  8		& 8		& 		0			& 8			& 8			& 8			&	0\\      
\textbf{27}						& \textbf{JavaWrapper}				&\textbf{513}		&\textbf{3}		&	\textbf{2}	&	\textbf{2}	& 	\textbf{0.23}					& \textbf{4}		& \textbf{4} 		& \textbf{3}		& 		\textbf{0.06}			& \textbf{4}			& \textbf{4}			& \textbf{3}			&	\textbf{0.05}\\      
28						& JmxUtilities				&645		& 8		&	8	&	6	& 	0.07					& 8		& 8 		& 7		& 		0.04			& 8			& 8			& 7			&	0.04\\      
\textbf{29}						& \textbf{List}					&\textbf{1718}		& \textbf{5}		&	\textbf{6}	&	\textbf{4}	& 	\textbf{0.11}					& \textbf{6}		& \textbf{6} 		& \textbf{4}		& 		\textbf{0.10}			&\textbf{6}			& \textbf{6}			& \textbf{5}			&	\textbf{0.09}\\      
30						& NameEntry				&172		& 4		&	4	&	4	& 	0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0	\\  
\textbf{31}						& \textbf{NodeSequence}			&\textbf{68}			& \textbf{38}		&	\textbf{46}	&	\textbf{30}	& 	\textbf{0.10}					& \textbf{36}		& \textbf{45} 		& \textbf{30}		& 		\textbf{0.12}			& \textbf{38}			& \textbf{45}			& \textbf{30}			&	\textbf{0.08}	\\     
32						& NodeSet				&208		& 28		&	29	&	26	& 	0.03					& 28		& 29 		& 26		& 		0.04			& 28			& 29			& 26			&	0.03	\\  
33						& PersistentBag			&571		& 68		&	68	&	68	& 	0					& 68		&  68		& 68		& 		0			& 68			& 68			& 68			&	0	\\         
34						& PersistentList				&602		& 65		&	65	&	65	& 	0					& 65		&  65		& 65		& 		0			& 65			& 65			& 65			&	0	\\    
35						& PersistentSet				&162		& 36		&	36	&	36	& 	0					& 36		&  36		& 36		& 		0			& 36			& 36			& 36 			&	0	\\        
\textbf{36}						& \textbf{Project}					&\textbf{470}		& \textbf{65}		&	\textbf{71}	&	\textbf{60}	& 	\textbf{0.04}					& \textbf{66}		&  \textbf{78}		& \textbf{62}		& 		\textbf{0.04}			& \textbf{69}			& \textbf{78}			& \textbf{64}			&	\textbf{0.05}	\\        
\textbf{37}						& \textbf{Repository}				&\textbf{63}			& \textbf{31}		&	\textbf{31}	&	\textbf{31}	& 	\textbf{0}					& \textbf{40}		&  \textbf{40}		& \textbf{40}		& 		\textbf{0}			& \textbf{40}			& \textbf{40}			& \textbf{40}			&	\textbf{0}	\\         
38						& Routine					&1069		& 7		&	7	&	7	& 	0					& 7		&  7		& 7		& 		0			& 7			& 7			& 7			&	0	\\
39						& RubyBigDecimal			&1564		& 4 		&	4	&	4	& 	0					& 4		& 4 		& 4		& 		0			& 4			& 4			& 4			&	0\\      
40						& Scanner				&94			& 3		&	5	&	2	& 	0.20					& 3		& 5 		& 2		& 		0.27			& 3			& 5			& 2			&	0.25\\      
\textbf{41}						& \textbf{Scene}					&\textbf{1603}		& \textbf{26}		&	\textbf{27}	&	\textbf{26}	& 	\textbf{0.02}					& \textbf{26}		& \textbf{27} 		& \textbf{26}		& 		\textbf{0.02}			& \textbf{27}			& \textbf{27}			& \textbf{26}			&	\textbf{0.01}\\      
42						& SelectionManager			&431		& 3		&	3	&	3	& 	0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
\textbf{43}						& \textbf{Server}					&\textbf{279}		&\textbf{15}		&	\textbf{21}	&	\textbf{11}	& 	\textbf{0.20}					& \textbf{17}		& \textbf{21} 		& \textbf{12}		& 		\textbf{0.16}			& \textbf{17}			& \textbf{21}			& \textbf{12}			&	\textbf{0.14}\\      
\textbf{44}						& \textbf{Sorter}					&\textbf{47}			& \textbf{2}		&	\textbf{2}	&	\textbf{1}	& 	\textbf{0.09}					& \textbf{3}		& \textbf{3} 		& \textbf{2}		& 		\textbf{0.06}			&\textbf{3}			& \textbf{3}			& \textbf{3}			&	\textbf{0}\\      
45						& Sorting					&762		& 3		&	3	&	3	& 	0					& 3		& 3 		& 3		& 		0			& 3			& 3			& 3			&	0\\      
\textbf{46}						& \textbf{Statistics}				&\textbf{491}		& \textbf{16}		&	\textbf{17}	&	\textbf{12}	&	\textbf{0.08}					& \textbf{23}		& \textbf{25} 		& \textbf{22}		& 		\textbf{0.03}			& \textbf{24}			& \textbf{25}			& \textbf{22}			&	\textbf{0.04}\\      
47						& Status					&32			& 53		&	53	&	53	& 	0					& 53		& 53 		& 53		& 		0			& 53			& 53			& 53			&	0\\      
\textbf{48}						& \textbf{Stopwords}				&\textbf{332}		& \textbf{7}		&	\textbf{8}	&	\textbf{7}	& 	\textbf{0.03}					& \textbf{7}		&  \textbf{8}		& \textbf{6}		& 		\textbf{0.08}			& \textbf{8}			& \textbf{8}			& \textbf{7}			&	\textbf{0.06}\\      
\textbf{49}						& \textbf{StringHelper}				&\textbf{178}		& \textbf{43}	 	& 	\textbf{45}	&	\textbf{40}	& 	\textbf{0.02}					& \textbf{44}		&  \textbf{46}		& \textbf{42}		& 		\textbf{0.02}			& \textbf{44}			& \textbf{45}			& \textbf{42}			&	\textbf{0.02}\\      
50						& StringUtils				&119		& 19 		&	19	&	19	& 	0					& 19		& 19 		& 19		& 		0			& 19			& 19			& 19			&	0\\      
51						& TouchCollector			&222		& 3		&	3	&	3	& 	0					& 3		&  3		& 3		& 		0			& 3			& 3			& 3			&	0\\      
52						& Trie					&460		& 21		&	22	&	21	& 	0.02					& 21		&  22		& 21		& 		0.01			& 21			& 22			& 21			&	0.01\\      
53						& URI					&3970		& 5 		&	5	&	5	& 	0					& 5		&  5		& 5		& 		0			& 5			& 5			& 5			&	0\\      
54						& WebMacro				&311		& 5		&	5	&	5	& 	0					& 5		&  6		& 5		& 		0.14			& 5			& 7			& 5			&	0.28\\      
55						& XMLAttributesImpl			&277		& 8		&	8	&	8	& 	0					& 8		&  8		& 8		& 		0			& 8			& 8			& 8			&	0\\      
56						& XMLChar				&1031		& 13		&	13	&	13	& 	0					& 13		&  13		& 13		& 		0			& 13			& 13			& 13			&	0\\      
57						& XMLEntityManger			&763		& 17		&	18	&	17	& 	0.01					& 17		&  17		& 16		& 		0.01			& 17			& 17			& 17			&	0\\      
58						& XMLEntityScanner			&445		& 12		&	12	&	12	& 	0					& 12		&  12		& 12		& 		0			& 12			& 12			& 12			&	0\\      
59						& XObject					&318		& 19		&	19	&	19	& 	0					& 19		&  19		& 19		& 		0			& 19			& 19			& 19			&	0\\      
\textbf{60}						& \textbf{XString}					&\textbf{546}		& \textbf{23}		&	\textbf{24}	&	\textbf{21}	& 	\textbf{0.04}					& \textbf{23}		&  \textbf{24}		& \textbf{23}		& 		\textbf{0.02}			& \textbf{24}			& \textbf{24}			& \textbf{23}			&	\textbf{0.02}\\      
\hline 
    				&{\textbf{Total}}	&35,785	&1040	&	1075	&    973	&	2.42				& 1061	&1106	&1009	&		2.35		& 1075		& 1118		& 1032		& 	1.82\\
   \hline
     \end{tabularx} }
 	%\end{minipage}
    \bigskip
    \label{table:Results}
\end{table*}

\section{Results}\label{sec:res}
Results of the experiments including class name, Line of Code (LOC), mean value, maximum and minimum number of unique failures and relative standard deviation for each of the 60 classes tested by R, R+ and DSSR strategies are presented in Table~\ref{table:Results}. Each strategy found an equal number of failures in 31 classes while in the remaining 29 classes the three strategies performed differently from one another. The total of mean values of unique failures in DSSR (1075) is higher than for R (1040) or R+ (1061) strategies. 
%Results given in Table~\ref{table:ttest} can be split into three different categories as shown in Table~\ref{table:categories}. 
DSSR also finds a higher number of maximum unique failures (1118) than both R (1075), and R+ (1106). DSSR strategy finds 43 and 12 more unique failures compared to R and R+ respectively. The minimum number of unique failures found by DSSR (1032) is also higher than R (973) and R+ (1009), which attributes to higher efficiency of DSSR strategy over R and R+ strategies. 

% How to write relative standard deviation.
% Eventually, the standard deviations are all of the order of magnitude of .1\% for all strategies.

\subsection{Is there an absolute better strategy among R, R+ and DSSR strategies?}
Based on our findings DSSR strategy performs better than R and R+ strategies.  Figure~\ref{fig:LineChart} presents the average improvement of DSSR strategy over R and R+ strategies for 17 classes showing substantial difference between DSSR and R, DSSR and R+. The blue line with diamond symbol shows the performance of DSSR over R and the red line with square symbols depicts the performance of DSSR over R+ strategy. The findings show that DSSR strategy perform up to 33\% better than R and up to 17\% better than R+ strategy. In some cases DSSR perform equally well with R and R+ but in no case DSSR performed lower than R and R+. Based on the results it can be stated that DSSR strategy is a better choice than R and R+ strategy. The improvement of DSSR over R and R+ strategy is calculated by applying the formula (1) and (2) respectively.

\begin{equation} \frac{Averagefailures_{(DSSR)} - Averagefailures_{(R)}}{Averagefailures_{(R)}} * 100  \end{equation}

\begin{equation} \frac{Averagefailures_{(DSSR)} - Averagefailures_{(R+)}}{Averagefailures_{(R+)}}  * 100 \end{equation}





\begin{table*}[htp]
\small
\caption{T-test results of the classes showing different results}
\centering
\begin{tabular}{|r|l|r|r|r|l|}
\hline
 \multirow{2}{*} {S. No}	& \multirow{2}{*}{Class Name}	&  \multicolumn{3}{c|}{T-test Results} & \multirow{2}{*}{Interpretation} \\
\cline{3-5}
								& & 	DSSR, R	& DSSR, R+	&  R, R+ 	& 		\\
								\hline

1		&	AjTypeImpl		&	1 				& 1 			& 1			& 		\\	
2		&	Apriori			&	\textbf{0.03}	 	& 0.49		& 0.16		&		\\	
3		&	CheckAssociator	&	\textbf{0.04}	 	& \textbf{0.05}	& 0.44		& DSSR better		\\	
4		&	Debug			&	\textbf{0.03}	 	& 0.14		& 0.56		&		\\	
5		&	DirectoryScanner	&	\textbf{0.04}	 	& \textbf{0.01}	& 0.43		& DSSR better		\\
6		&	DomParser		&	\textbf{0.05}	 	& 0.23		& 0.13		&				\\
7		&	EntityDecoder		&	\textbf{0.04}	 	& 0.28		& 0.3			&		\\			
8		&	Font				&	0.18	 			& 0.18		& 1			&		\\
9		&	Group			&	0.33	 			& \textbf{0.03}	& \textbf{0.04}	& DSSR = R \textgreater~R+	\\
10		&	Image			&	\textbf{0.03}		& \textbf{0.01}	& 0.61		& DSSR better \\		
11		&	InstrumentTask		&	0.16				& 0.33		& 0.57		& \\
12		&	JavaWrapper		&	\textbf{0.001}		& 0.57		& 0.004		& DSSR = R+ \textgreater~R \\
13		& 	JmxUtilities		&	0.13				& 0.71		& 0.08		&	\\
14		&	List				& 	\textbf{0.01}		&0.25		&\textbf{0}		& DSSR = R+ \textgreater~R \\
15		&	NodeSequence	&	0.97				&\textbf{0.04}	&\textbf{0.06}	& DSSR = R \textgreater~R+ \\
16		&	NodeSet			&	\textbf{0.03}		&0.42		&0.26		& 	\\
17		&	Project			&	\textbf{0.001}		&0.57		&\textbf{0.004}	& DSSR better \\		
18		&	Repository		&	\textbf{0}			&1			&\textbf{0}		& DSSR = R+ \textgreater~R \\
19		&	Scanner			&	1				&\textbf{0.03}	&\textbf{0.01}	& DSSR better \\
20		&	Scene			&	\textbf{0}			&\textbf{0}		&1			& DSSR better \\
21		&	Server			&	\textbf{0.03}		& 0.88		&\textbf{0.03} 	& DSSR = R+ \textgreater~R \\
22		&	Sorter			& 	\textbf{0}			& 0.33		&\textbf{0}		& DSSR = R+ \textgreater~R \\
23		&	Statistics			&	\textbf{0}			& 0.43		&\textbf{0}		& DSSR = R+ \textgreater~R\\
24		&	Stopwords		&	\textbf{0}			& 0.23		&\textbf{0}		& DSSR = R+ \textgreater~R \\
25		&	StringHelper		&	\textbf{0.03}		& 0.44		&0.44		& DSSR = R+ \textgreater~R\\
26		& 	Trie				&	0.1				& 0.33		&0.47		& DSSR better \\
27		&	WebMacro		&	0.33				& 1			&0.16		& \\
28		&	XMLEntityManager	&	0.33				& 0.33		&0.16		& \\
29 		&	XString			&	0.14				&\textbf{0.03}	&0.86		& \\

\hline
\end{tabular}
\bigskip
\label{table:ttest}
\end{table*}





\subsection{Are there classes for which any of the three strategies provide better results?}


T-tests applied to the data given in Table~\ref{table:ttest} showed significantly better performance of DSSR compared to R strategy in 17 classes and R+ strategy in 9 classes. In the remaining classes all the three strategies performed equally well. It is interesting to note that in no single case R and R+ strategies performed better than DSSR strategy. We attribute this to DSSR possessing the qualities of R and R+ whereas containing the spot sweeping feature.


%Results of the 60 classes tested in the study are divided in to 11 different categories as presented in ~\ref{table:categories}. 
\begin{comment}
\begin{table}[h]
\caption{Results of the 60 classes are divided into 11 categories}
\centering
\begin{tabular}{|r|l|r|}
\hline
S. No	& 	Category			& 	Result\\
\hline
1		&	DSSR > R			&	12 \\	
2		&	DSSR > R+		&	10 \\	
3		&	DSSR = R			&	5 \\	
4		&	DSSR = R+		&	7 \\	
5		&	R+ > R 			&	10 \\	
6		&	R+ < R			&	5 \\	
7		&	R+ = R			&	2 \\	
8		&	R > R+			&	4 \\
9		&	DSSR < R			&	0 \\	
10		&	DSSR < R+		&	0 \\
11		&	DSSR = R = R+	&	43 \\			
\hline
\end{tabular}
\bigskip
\label{table:categories}
\end{table}

% pie chart is removed because the length of the paper is exceeding 10 pages and also it don't make much sense i believe.
%\begin{figure}[h]
%\centering
%\includegraphics[width=8cm,height=7cm]{pie5.png}
%\caption{Division of result in to categories}
%\label{fig:pie}
%\end{figure}






The first category contain 12 classes where DSSR strategy performs better than R. 
The second category contain 10 classes where DSSR strategy performs better than R+. 
The third category contain 5 classes where DSSR strategy and R performs equally well.
The fourth category contain 7 classes where DSSR and R+ performs equally well. 
The fifth category contain 10 classes where R+ performs better than R.
The sixth category contain 5 classes where R performs better than R+.
The seventh category contain 2 classes where R and R+ performs equally well.
The eighth category contain 4 classes where R performs better than R+.
Category 9 and 10 shows that neither R nor R+ performed better than DSSR strategy.
The last category shows each strategy performing equally well for 43 classes. Expressing in percentage, 72\% classes do not show different behaviours whereas in 28\% of classes, the DSSR strategy performs better than R and R+ strategy. It is interesting to note that in no single case R and R+ strategies performed better than DSSR strategy. This is attributed to the fact that DSSR strategy possess the qualities of R and R+ and has the additional advantage of spot sweeping.

\end{comment}

\subsection{Can we pick the best default strategy between R, R+ and DSSR strategies?}

Analysis of the experimental data reveals that DSSR strategy has an edge over R and R+. This is mainly because of the additional feature of DSS in DSSR strategy which can identify new faults quickly in the case of different faults or helps in debugging by providing more fault revealing input in the case of single fault. However, further analysis of DSSR strategy in terms of code coverage and overhead is required before picking it as a default strategy.

%%%%%%%%%%%%%%%%%    DISCUSSION   %%%%%%%%%%%%%%%%%%%%

\section{Discussion}\label{sec:discussion}
In the present study, we evaluated DSSR strategy against R and R+ strategies under identical conditions. This is in accordance with the common practice to evaluate performance of an extended strategy by applying it in combination with other existing strategies to the same programs and comparing the results obtained~\cite{Gutjahr1999, Hamlet1990}. We used random testing as a baseline for comparison as advocated by Arcuri et al~\cite{Arcuri2012}.

In our experiments we selected projects from the Qualitas Corpus~\cite{Tempero2010} which is a collection of open source java programs maintained for independent empirical research. These projects are carefully selected and span across the whole set of java applications~\cite{Oriol2012, Tempero2010}. The selection of programs in the current study is made through automated random process to avoid any bias and maintain representative selection of classes. 

All the three strategies have the potential of finding failures. However, DSSR strategy found more number of unique failures than both R and R+ strategies as reflected in the results. This improved performance of the strategy can be attributed to the additional feature of DSS in the DSSR strategy.
 
In DSSR strategy no pre-existing test cases are required because it utilises the border values from R+ and regenerate the data very cheaply in a dynamic fashion different for each class under test without any prior test data and with comparatively lower overhead. 
 
The DSSR strategy performs comparatively better when the number of failures is higher in the SUT. The logical explanation is that the higher the number of failures in the SUT, the more connected are their failure domains, increasing the chances of finding failures by DSS.

As pointed out earlier, DSSR strategy relies on R+ strategy to identify the first unique failure. We noticed in our experiments that discovery of first unique failure in the early stage of the test triggers the activation of Dirt Spot Sweeping, resulting in quick finding of failures in the SUT. The process is delayed when the identification of the first unique failure is not accomplished in the early stage by R+ strategy. The limitation of the new strategy may be addressed in future studies by avoiding the reliance of DSSR strategy on R+ for the discovery of first failure.

% In this section we discuss various factors such as the time taken, effect of test duration, number of tests, number of failures in the different strategies and the effect of finding first failure in the DSSR strategy.
%\textbf{Time taken to execute an equal number of test cases:}
%The DSSR strategy takes slightly more time (up to 5\%) than both pure random and random plus which may be due to maintaining sets of interesting values during the execution. We do not believe that the overhead can be reduced. 

%\textbf{Effect of test duration and number of tests on the results:}
%All three techniques have the same potential for finding failures. If testing is continued for a long duration then all three strategies will find the same number of unique failures and the results will converge. We suspect however that some of the unique failures will take an extremely long time to be found by using random or random+ only. Further experiments should confirm this point.


%\textbf{Effect of number of failures on results:} 
%We found that the DSSR strategy performs better when the number of failures is higher in the code. The reason seems to be that when there are more failures, their domains are more connected and DSSR strategy works better. Further studies might use historical data to pick the best strategy.

%\textbf{Dependence of DSSR strategy to find the first unique failure early enough:}
%During the experiments we noticed that if a unique failure is not found  quickly enough, there is no value added to the list of interesting values and then the test becomes equivalent to random+ testing. This means that better ways of populating failure-inducing values are needed for sufficient leverage to DSSR strategy. As an example, the following piece of code would be unlikely to fail under the current setting:

%\begin{lstlisting}
%public void test(float value){
% if(value == 34.4445)   10/0;
%}
%\end{lstlisting}

%In this case, we could add constant literals from the SUT to the list of interesting values in a dynamic fashion. These literals can be obtained from the constant pool in the class files of the SUT.

%In the example above the value 34.4445 and its surrounding values would  be added to the list of interesting values before the test starts and the DSSR strategy would find the unique failure right away.

%\textbf{DSSR strategy and coverage:} Random strategies are typically notorious for achieving code coverage, however, it might be interesting to compare R, R+ and DSSR with respect to the achieved coverage or even to use a DSSR variant that adds a new interesting value and its neighbours when a new branch is reached.


%\textbf{Threats to validity:} As usual with such empirical studies, the present work might suffer from a non-representative selection of classes.
%The selection in the current study is however made through random process and objective criteria, therefore, it seems likely that it would be representative.

%The parameters of the study might also have prompted incorrect results. But this is very unlikely due to previous results on random testing~\cite{Oriol2012}.



%%%%%%%%%%%%%%%%%    RW   %%%%%%%%%%%%%%%%%%%%

\section{Related Work}\label{sec:rw}

Random testing used in the current study is a popular technique with simple algorithm but a proven method to find subtle failures in complex programs and Java libraries~\cite{Pacheco2005, Csallner2004}. Its simplicity, ease of implementation and efficiency in generating test cases make it one of the best choices for test automation~\cite{Hamlet1994}. Some of the well known automated tools based on R strategy include Jartege~\cite{Oriat2004}, Eclat~\cite{Pacheco2005}, JCrasher~\cite{Csallner2004}, AutoTest \cite{Ciupa2008} and YETI~\cite{Oriol2012}.

The R+ strategy is an extension of the R strategy in which interesting values, beside pure random values, are added to the list of test inputs~\cite{Leitner2007}. These interesting values include border values which have high tendency of finding failures in the given SUT~\cite{Beizer1990}. Results obtained with R+ strategy show significant improvement over R strategy~\cite{Leitner2007}. 

In pursuit of better test results and lower overhead, many variations of R strategy have been proposed. Adaptive random testing (ART) \cite{Chen2010}, Quasi-random testing (QRT) \cite{Chen2005} and Restricted Random testing (RRT) \cite{Chan2002} achieved better results by selecting test inputs randomly but evenly spread across the input domain. Mirror Adaptive Random Testing (MART) \cite{Chen2003} and Adaptive Random Testing through dynamic partitioning \cite{Chen2003} increased the performance by reducing the overhead of ART. The main reason behind better performance is that even spread of test input increases the chance of exploring the failure patterns present in the input domain.

A more recent research study \cite{Yoo2012} stresses the effectiveness of data regeneration in close vicinity of the existing test data. Their findings showed up to two orders of magnitude more efficient test data generation than the existing techniques. Two major limitations of their study are the requirement of existing test cases to regenerate new test cases and increased overhead due to using ``meta heuristics search'' based on hill climbing algorithm to regenerate new data. 
  

%It is interesting that numerous efforts have been made to discover the failure patterns~\cite{Chen2010, Chen2005, Chan2002, Chen2003}, etc. but in our knowledge, none has been published on covering/sweeping all the failures lying in a specific pattern once it has been discovered.


%In our experiments we selected projects from the Qualitas Corpus~\cite{Tempero2010} which is a collection of open source java programs maintained for independent empirical research. The projects in Qualitas Corpus are carefully selected that spans across the whole set of java applications~\cite{Oriol2012, Tempero2008}.


%%%%%%%%%%%%%%%%%    CONCLUSIONS   %%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conc}
In the present study, we developed a new Dirt Spot Sweeping Random strategy as an upgraded version of the R+ strategy based on the Dirt Spot Sweeping feature, which strengthens the ability of finding more failures in lower number of tests. The DSSR strategy is particularly more effective when the failure domain forms block and strip patterns across the input domain. In addition, the findings of the present study indicated significantly better performance of DSSR in comparison with R and R+ strategies with respect to finding higher number of failures. 

%The main goal of the present study was to develop a new random strategy which could find more failures in lower number of test cases. We developed a new strategy named. ``DSSR strategy'' as an extension of R+, based on the assumption that in a significant number of classes, failure domains are contiguous or located closely. The DSS strategy, a strategy which adds neighbouring values of the failure finding value to a list of interesting values, was implemented in the random testing tool YETI to test 60 classes, 30 times each, from Qualitas Corpus with each of the 3 strategies R, R+ and DSSR. The newly developed DSSR strategy uncovers more unique failures than both random and random+ strategies with a 5\% overhead. We found out that for 7 (12\%) classes DSSR was significantly better than both R+ and R, for 8 (13\%) classes DSSR performed similarly to R+ and significantly better than R, while in 2 (3\%) cases DSSR performed similarly to R and significantly better than R+. In all other cases, DSSR, R+ and R do not seem to perform significantly differently. Overall, DSSR yields encouraging results and advocates to develop the technique further for settings in which it is significantly better than both R and R+ strategies.

%%%%%%%%%%%%%%%%%    ACKNOWDLEGEMENT   %%%%%%%%%%%%%%%%%%%%
%\section{Acknowledgments}
% should put it back in final version
%The authors thank the Department of Computer Science, University of York for its financial support with the Departmental Overseas Research Scholarship (DORS) award. We also thanks to Richard Page for his valuable help and generous support.







\bibliographystyle{IEEEtran}
\bibliography{bare_conf}

\end{document}



