In this chapter we give a brief introduction and motivation for the research work presented in this thesis. We commence by introducing the problems in random testing. We then describe the alternative approaches to overcome these problems, followed by our research goals and contributions. At the end of the chapter, we give an outline of the thesis.

\section{The Problems}
In software testing, one is often confronted with the problem of selecting a test data set because exhaustive testing is not always applicable. Manual test data generation is a time-consuming and laborious exercise, therefore, automated test data generation is always preferred. Test data set is a subset of domain carefully selected to test the given software. Finding an adequate test data set is a crucial process in any testing technique as it aims to represent the whole infinite domain and evaluate the given system under test (SUT) for structural or functional properties \cite{mccabe1983} \cite{howden1986}. Test data generators are classified in to Pathwise, Goal-Oriented, Intellighent and Random \cite{wiki2013}. Random test data generation generate test data set randomly from the whole domain. Unlike other approaches Random approach is simple, easy to implement, fastest in computation and free from bias \cite{Ciupa2007}. \\ 
Despite the benefits random testing offers, its simplistic and non-systematic nature expose it to high criticism. \cite{Myers2004} mentioned it as "Probably the poorest methodology of all is random-input testing...". Where this statement is based on intuit n instead of experimental results but even if it is considered correct it is worth while to upgrade the strategy to generate more fault revealing test data. Random testing is also considered notorious for providing low code coverage \cite{Offutt1996}. For example, in random testing when the statement  ``if (x == 25) then ... "  is exposed to execution then there is only one chance, of the ``then..." part of the statement, to be executed out of $2^\text{32}$. If x is an integer variable of 32 bit value \cite{Godefroid2005}. Random testing is no exception when it comes to evaluate the results. Modern testing techniques deal with it by truncating the lengthy log files and display only the fault revealing test cases in the form of unit tests. However effort is required to show the test results in more compact and user friendly way. 


\section{Our Goals}
The overall goal of this thesis is to develop new techniques for automated testing based on random strategy that addresses the above mentioned problems. Particularly,\\*

\begin{enumerate}
\item We aim to develop an automated testing technique which is able to generate more fault revealing test data out of the whole domain. In particular, it takes benefit of the presence of fault clusters found in the form of block and strip fault domains in the whole domain. Hence, we can automatically generate more fault revealing test data to evaluate the given SUT. Thus we are able to handle the problem of generating arbitrary data by random testing. 

\item We aim to develop a novel framework for finding the faults, their domains and their presentation on a graphical chart inside the specified lower and upper bound. It
considers the correlations of the fault and fault domain. It also gives a simplified and user friendly report to easily identify the faulty regions across the whole domain.

\item We aim to develop an automated strategy that utilises the literals of the SUT to particularly increase the total coverage and ultimately the overall results. On test initialisation it scan binary of the given SUT for literals. These literals are added to the prioritised list of interesting values to increase coverage and test performance. Thus we are able to handle the problem of coverage by random testing. 


\section{Contributions}
To achieve the research goals described in Section xx, we make the following specific contributions:

\subsection{Dirt Spot Sweeping Random Strategy}
Development of a new, enhanced and improved form of automated random testing: the Dirt Spot Sweeping Random (DSSR) strategy. This strategy is based on the assumption that faults and unique failures reside in contiguous blocks and stripes. The DSSR strategy starts as a regular random+ testing strategy Ñ a random testing technique with preference for boundary values. When a failure is found, it increases the chances of using neighbouring values of the failure in subsequent tests, thus slowly sweeping values around the failures found in hope of finding failures of different kind in its vicinity.The DSSR strategy is implemented in the YETI random testing tool. It is evaluated against random (R) and random+ (R+) strategies by testing 60 classes (35,785 line of code) with one million (105) calls for each session, 30 times for each strategy. The results indicate that for 31 classes, all three strategies find the same unique failures. We analysed the 29 remaining classes using t-tests and found that for 7 classes DSSR is significantly better than both R+ and R, for 8 classes it performs similarly to R+ and is significantly better than R, and for 2 classes it performs similarly to ran- dom and is better than R+. In all other cases, DSSR, R+ and R do not perform significantly differently. Numerically, the DSSR strategy finds 43 more unique failures than R and 12 more unique failures than R+.
\subsection{Automated Discovery of Failure Domain}


\subsection{Directed Random Plus Strategy}

\section{Thesis Outline}








%Today, the primary focus of software companies is to achieve high quality. These companies spend an estimated thirty to ninety percent of the total software development cost on testing \ref{Beizer1990}, \ref{Standards2002}. In spite of spending 

%Software testing is the process of executing a software with specific test data followed by evaluation of the results to check whether it is working according to its specification or not \ref{Sommerville2006}.
% check here if we can replace specification with oracle or not.
%The test passes if the output complies to its specification and fails otherwise. The success of testing correlates with the number of failures found in the Software Under Test (SUT): a test is more successful if it finds more faults.

%It is interesting that program testing is used to show the presence of bugs, rather than absence of bugs [6]. Therefore the SUT that passes all the tests without returning a single failure does not guarantee that there is no fault. The testing process increases however the reliability and confidence of both the developers and the users in the tested product [7] [8] [9].

%Random testing is a black-box testing technique in which the SUT is executed against ran- domly selected test data. Test results obtained are compared either against the oracle defined, using SUT specifications in the form of assertions or exceptions defined by the programming language. The rapid increase in software development in today?s modern world prompts the need for automated testing to ensure high quality. The generation of random test data is com- paratively cheap and does not require too much intellectual and computation efforts [10] [11]. It is for this reason that various researchers have recommended this strategy for incorporation in automatic testing tools [12]. YETI [13] [14], AutoTest [15] [16], QuickCheck [17], Randoop [18], JArtage [19] are a few of the most common tools based on random strategy.
