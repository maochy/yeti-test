\externaldocument{chapter8}
\chapter{Conclusions}
\label{chap:conclusions_8}

%\section{Introduction}\label{sec:intro7}


This research aims at understanding the nature of failures in software, discovering how to leverage failure domain for finding more bugs and developing new improved automated random test strategies to achieve the desired objectives. The existing random test strategies find individual failures and do not focus on failure domain. The knowledge of failure along with failure domain is of great benefit to debuggers for quick and effective removal of failures.

Various aspects of failure domains with respect to automated random testing were explored. The study focused on three main issues. First, to minimize the number of test cases required to discover a failure domain. Second, to identify the pass and fail input domains and generate the result in graphical form. Third, to compare the developed strategy with the standard automated tool Daikon for finding the failure domain was the third focus of the study.\\* 
It was revealed in the study that the input inducing failures residue in contagious locations forming certain geometrical shapes in the input domain. These shapes can be divided into point, block and strip domains. A set of techniques and tools have been developed for improving the effectiveness of automated random testing in finding failures and failure domains. 


%Three new techniques were developed which could effectively identify and plot failure domains.



%by using the neighbouring values surrounding the randomly generated value which lead to the discovery of first failure. Second, to identify the legitimate and failure-domains and represent them in a graphical form. Third, to upgrade the existing strategy and compare its behaviour with Daikon in finding the failure-domain.
%by collecting and evaluating the results of the tests execution. 
%Section \ref{ResearchGoals_1} and Section \ref{contributions_1} presented the goals and contributions of the thesis.

 % which are briefly recapped here.

%Failures reside in contagious locations forming point, block and strip failure-domains. The existing random test strategies tries to find individual failures but do not focus on its domain. Providing the knowledge of failure along with its domain benefits debuggers to remove the failure from its root quickly and effectively. The test reports could be further simplified when the tested values are shown on a graphical form separating pass and fail values and highlighting the failure domains.

%The thesis describe three techniques, Dirt Spot Sweeping Random (Chapter~\ref{chap:DSSR}), Automated Discovery of Failure Domain (Chapter~\ref{chap:ADFD}) and Automated Discovery of Failure Domain+ (Chapter~\ref{chap:ADFD+}) to discover, analyse and present the failure-domains in a graphical form. All the techniques are implemented in YETI which is available for download from \url{https://code.google.com/p/yeti-test/}.

The first technique, Dirt Spot Sweeping Random (DSSR) strategy starts by testing the program at random. When a failure is identified, the strategy selects the neighbouring input values for the subsequent tests. The selected values sweep around the identified failure leading to the discovery of new failures in the vicinity. This results in a quick and efficient identification of failures in the software under test. The results stated in Chapter \ref{chap:DSSR} showed that DSSR performs significantly better than random and random$^+$ strategies.

The second technique, Automated Discovery of Failure Domain (ADFD) was developed to find failure and failure domains in a given software and provides visualization of the identified pass and fail domains within a specified range in the form of a chart. The technique starts with a random$^+$ strategy to find the first failure. When a failure is identified, a new Java program is dynamically created at run-time which, then compiled and executed to search for failure domains along the projections on various axis. The output of the program shows pass and fail domains in the graphical form. The results stated in Chapter \ref{chap:ADFD} showed that ADFD technique correctly identify the failure domains. The technique is highly effective in testing and debugging by providing an easy to understand test report in the visualized form. 

The third technique, Automated Discovery of Failure Domain$^+$ (ADFD$^+$) is an upgraded version of ADFD technique with respect to the algorithm and graphical representation of failure domains. The new algorithm searches for the failure domain around the failure in a given radius as against ADFD, which limits the search between lower and upper bounds. The ADFD$^+$ graphical output was further improved by providing labelled graphs to make it easily understandable and user-friendly. To find the effectiveness of ADFD$^+$, it was compared with Daikon using error seeded programs. The ADFD$^+$ correctly pointed out all the seeded failure domains while Daikon identified individual failures but was unable to discover the failure domains. 


%The results are collected and represented on a graph in a more clarified form. 
%relying on prior existence of test cases to generate invariants, was not able to correctly identify the failure domain

%The ADFD+ output is also compared with the Daikon's output to analyse their behaviour in response to known failure domains.
%Experimental results show that ADFD+ correctly pointed out the planted failure domains 

%whereas Daikon, which rely on the existing of test cases to generate invariants, was not able to generate invariants that correctly point to the failure-domain. 

%Testers and/or developers can inspect those generated graphs for failures and failure-domains, instead of inspecting a large number of all generated tests. 

%To incorporate the feature of generating results in the graphical form the Automated Discovery of Failure Domain (ADFD) strategy was developed.  

%A set of techniques and tools have been developed for improving the effectiveness of automated random testing in finding failures and failure-domains. The first technique DSSR (Chapter \ref{chap:DSSR}), starts with random strategy to find the first failure. When a failure is identified, the DSSR strategy selects neighbouring values for the subsequent tests. The selected values sweep around the failure, leading to the discovery of new failures in the vicinity. Experimental results show that DSSR performed significantly better than random and random+ strategy. 

%The second technique ADFD (Chapter \ref{chap:ADFD}), starts with random+ strategy to find the first failure. When a failure is identified a new Java program is dynamically created at run-time. The program is compiled and executed to search for failure-domains in the specified range. The output of the tests are collected and represented in the graphical form.
%It improves the debugging efficiency as debuggers keep all the instances of a failure under consideration during debugging. Experimental results show that ADFD technique correctly identified the failure domains in the error-seeded programs. 


%The third technique ADFD+ ((Chapter \ref{chap:ADFD+}), is an improvement of ADFD strategy with a better algorithm to find failure and failure-domains. It searches for the failure-domains around the found failure in a given radius instead of the lower and upper bound. The results are collected and  represented on a graph in a more simplified manner. The ADFD+ output is also compared with the Daikon's output to analyse their behaviour in response to known failure domains. Experimental results show that ADFD+ correctly pointed out the planted failure-domains whereas Daikon, which rely on the existing of test cases to generate invariants, was not able to generate invariants that correctly point to the failure-domain. The ADFD+ graphical output is made further user friendly providing labelled graphs making it more simple to understand. Testers and/or developers can inspect those generated graphs for failures and failure-domains, instead of inspecting a large number of all generated tests. 

%\newpage
\section{Lessons Learned}
Research in the field of software testing has been proceeding for more than three decades but only a handful of free and open source fully automated testing tools are available for software testing. The current study is a continuation of the research efforts to find improved testing techniques capable of identifying failures and failure domains quickly, efficiently and effectively. In this section, the lessons learned during the study are presented in the summarized form, which may be of interest to the researchers pursuing future research.

%The research in this thesis has developed new techniques to improve the effectiveness of automated random testing. 
%Our research is motivated to investigate how to improve the effectiveness of random testing for contagious failures across the input domain and graphical representation of the failure-domains. 
%Our research has shed light on this promising direction and pointed out future work along this direction. 
%In this section, we summarize some lessons that we learned from this study and we hope these lessons may be of interest to the researchers in pursuing future research.
%Better tool support is needed to help the research community in order to devise new testing techniques to meet the demand for high software reliability.  
  
    
%\subsection{Performance measurement criteria may be carefully chosen}

%\newpage






\textbf{Selection of performance measurement criteria}

Among the three measuring techniques used for finding the effectiveness of random testing, the E-measure and P-measure have been criticised~\cite{chen2005adaptive} whereas the F-measure has been often used by researchers~\cite{chen2004statistical, chen1996expected}. In our experiments, the F-measure was initially used but its weakness was soon realised as stated in Section \ref{measurementCriteria_4}. The F-measure is effective in traditional testing and counts the number of test cases used to find the first failure. The system is then handed over to developers for fixing the identified failure. Automated testing tools test the whole system and report all discovered failures in one go, thus the F-measure is not a favourable choice. We addressed the issue by measuring the maximum number of failures detected in a particular number of test calls as the criterion for finding the effectiveness of the test strategy.\\

%In some experiments a strategy found the first fault quickly than the other but on completion of test session that very strategy found lower number of total faults than the rival strategy. The preference given to a strategy by F-measure because it finds the first fault quickly without giving due consideration to the total number of faults is not fair
%~\cite{liu2012comparison}. 
%The literature review revealed that the F-measure is used where testing stops after identification of the first failure and the system is given back to the developers to remove the fault. Currently automated testing tools test the whole system and print all discovered failures in one go and F-measure is not the favourable choice. In our experiments, performance of the strategy was measured by the maximum number of failures detected in SUT by a particular number of test calls \cite{ciupa2007experimental, pacheco2007feedback, ciupa2008predictability}. This measurement was effective because it considers the performance of the strategy when all other factors are kept constant.\\









\textbf{Test results in random testing keep on changing} 

In random testing, due to the random generation of test input, the results keep on changing even if all the test parameters and the program under test remain the same. Therefore, the efficiency of one technique in comparison with the other becomes difficult. We addressed  the issue by taking five steps. 1) Each experiment was repeated 30 times and the average was taken for comparison 2) In each experiment 10000 test cases were executed to minimize the random effect 3) Sufficiently large number of representative test samples (60) were taken for evaluation 4) Error seeded programs with known locations of faults were used to verify the results 5) The experimental results were statistically analysed to estimate the difference on statistical basis.\\


   
\textbf{Testing of neighbouring values around the failure decreases computation}

Developing new versions of random testing with higher fault finding ability usually result in increased computation, higher overhead and lower performance. We addressed the issue by developing new strategies which, uses neighbouring values around the failure finding value for the subsequent tests. This approach saves the computation involved in generating suitable test values from the whole input domain. \\
%It is difficult to achieve high quality strategy without any overhead but efforts should be made to keep it to bare minimum. For example, if we test a program with two strategies A and B. Strategy A consumes 100 tests to find a fault while strategy B consumes 500 tests to find the same fault.  It looks that strategy A supersedes strategy B however the time to generate the test cases may also be considered which can be equal for both the strategies. In addition to time other resources like hardware etc should also be kept minimum.\\
\\

\textbf{Random testing coupled with exhaustive testing}

Random testing test values at random while exhaustive testing test the whole input domain, which is very effective but not usually feasible for a larger domain. The issue was addressed by coupling the random testing with exhaustive testing. In our newly developed strategies the testing starts at random till a failure is identified and then switches to exhaustive testing to select the values around the failure finding value in the sub-domain set by the tester. This partially provides the benefit of exhaustive testing in the random testing and results in quick identification of the neighbouring failures which may be difficult to find by using random testing alone.\\


%\textbf{Starting with random testing and switching to exhaustive strategy helps}

%In our techniques ADFD and ADFD+, we started the test using random strategy and later switched to exhaustive strategy to find the failure domain of a program. In the strategies the testing starts with random strategy and when a failure is identified the test strategy changes to exhaustive strategy and search all the values around the failure to the specified range set by the tester. The benefit of the approach is that it quickly identify the neighbouring faults which may be difficult to find using random strategy. \\




\textbf{Easy to understand user-friendly test output}

Random testing is no exception when it comes to the complexity of understanding and evaluating test results. No random strategy seems to provide the graphical representation of the failures and failure domains. The issue of getting in easy to understand user-friendly format has been addressed the present study. The ADFD strategy has been developed with the feature of giving the result output in the visualized graphical form. This feature has been further improved in the ADFD$^+$ strategy which clarify and label individual failures and the failure domains in a two-dimensional graph. The identification of failures and failure domains in graphical form helps developers to follow the test reports easily while fixing the faults.  \\


%Random testing lacks the capability to 
%In our technique we represented the test output in an easy to understand graphical form. The lines and circles with blue colour represent pass values while lines and squares with red colour represents fail values. The graph shows red points when the program fails for only one value, block when the program fails for multiple values and strip when the program fails for a long range of values.  The technique help the debuggers in two ways. First, it reduces the `to' and `from' movement of the program between the testers and debuggers as it identify all the faults in one go. Second, it identifies locations of all fault domains across the input domain in a user-friendly way helping debugger to fix the fault keeping in view its all occurrences.\\
%\textbf{Graphical representation of multi-argument method and reference data type is complicated}
%Results of random testing contains No attempt has been made to represent 
%The difficulty to represent results is directly proportional to the number of arguments used in a method. Which means as the number of arguments in a method increases, the more dimensions we need and the more complicated the process get in to. Though it is not difficult to extend the approach to test more than two-dimensional programs containing other primitive types, it would however be difficult to plot them on the chart. The approach can also be extended to test object-oriented programs by implementing objects distance proposed by Ciupa et al. \cite{ciupa2006object}. The details of such an implementation with graphical representation will be complicated.\\


\textbf{Auto-generation of primitive and user-defined data types}

We noticed that auto-generation of user-defined data type is more complex as compared to the primitive data type. We addressed the issue by creating objects of the classes under test and randomly calling the methods with random inputs in accordance with the parameter's space. The inputs were divided into primitive types and user-defined data types.  For primitive data generation \verb+Math.random()+ method is used and for generation of user-defined data object of the class is created at run time as stated in Section{sec:constructionOfTestCases}. The approach adopted helps in achieving a fully automated testing system.\\

%The constructor of may possibly require another object, then the strategy recursively calls the constructor of that object. This process is continued till an object with empty constructor or a constructor with only primitive types or the set level of recursion is reached.\\


% Send this to future work.

%\textbf{Contracts are helpful in finding failures, used as documentation and oracle}

%Although our research has developed testing techniques that do not require contracts, we found that the effectiveness of automated testing could be further improved if the tools are given extra guidance in the form of pre-conditions, post-conditions and class invariants. Contracts are like assert statements and its violation i.e. when its value turn false indicate an error in the program. It also serves the purpose of documentation and you can tell on the fly what data a software take and the state before and after execution. The post-condition of a program also serves as an oracle.\\


%\subsection{Random testing may be a good way to go on with robustness testing}





