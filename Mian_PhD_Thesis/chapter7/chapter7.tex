\externaldocument{chapter7}
\chapter{Conclusions}
\label{chap:conclusions_7}

%\section{Introduction}\label{sec:intro7}


The research study aimed at understanding the nature of failures in software, discovering how to leverage failure domain for finding more bugs and developing new improved automated random test strategies to achieve the desired objectives. The existing random test strategies find individual failures and do not focus on failure domain. The knowledge of failure along with failure domain is of great benefit to debuggers for quick and effective removal of failures.

Various aspects of failure-domains with respect to automated random testing were explored. The study focused on three main issues. To minimize the number of test cases required to discover a failure-domain was the first, to identify the pass and fail input domains and generate the result in graphical form was the second, to compare the developed strategy with the standard automated tool Daikon for finding the failure domain was the third focus of the study.\\* 
It was revealed in the study that the input inducing failures residue in contagious locations forming certain geometrical shapes in the input domain. These shapes can be divided in to point, block and strip domains. A set of techniques and tools have been developed for improving the effectiveness of automated random testing in finding failures and failure-domains. 


%Three new techniques were developed which could effectively identify and plot failure domains.



%by using the neighbouring values surrounding the randomly generated value which lead to the discovery of first failure. Second, to identify the legitimate and failure-domains and represent them in a graphical form. Third, to upgrade the existing strategy and compare its behaviour with Daikon in finding the failure-domain.
%by collecting and evaluating the results of the tests execution. 
%Section \ref{ResearchGoals_1} and Section \ref{contributions_1} presented the goals and contributions of the thesis.

 % which are briefly recapped here.

%Failures reside in contagious locations forming point, block and strip failure-domains. The existing random test strategies tries to find individual failures but do not focus on its domain. Providing the knowledge of failure along with its domain benefits debuggers to remove the failure from its root quickly and effectively. The test reports could be further simplified when the tested values are shown on a graphical form separating pass and fail values and highlighting the failure domains.

%The thesis describe three techniques, Dirt Spot Sweeping Random (Chapter~\ref{chap:DSSR}), Automated Discovery of Failure Domain (Chapter~\ref{chap:ADFD}) and Automated Discovery of Failure Domain+ (Chapter~\ref{chap:ADFD+}) to discover, analyse and present the failure-domains in a graphical form. All the techniques are implemented in YETI which is available for download from \url{https://code.google.com/p/yeti-test/}.

The first technique, Dirt Spot Sweeping Random (DSSR) strategy is developed which starts by testing the program at random. When a failure is identified, the strategy selects the neighbouring input values for the subsequent tests. The selected values sweep around the identified failure leading to the discovery of new failures in the vicinity. This results in quick and efficient identification of failures in the software under test. The results stated in chapter \ref{chap:DSSR} showed that significantly better performance of DSSR strategy as compared to random and random+ strategies.

The second technique, Automated Discovery of Failure Domain (ADFD) was developed with the capability to find failure and failure-domains in a given software and provides visualization of the identified pass and fail domains within a specified range in the form of a chart. The technique starts with random+ strategy to find the first failure. When a failure is identified, a new Java program is dynamically created at run-time which is then compiled and executed to search for failure-domains. The output of the program showing pass and fail domains is represented in the graphical form. The results stated in chapter \ref{chap:ADFD} showed that ADFD technique correctly identify the failure domains. The technique is highly effective in testing and debugging by providing an easy to understand test report in the visualized form. 

The third technique, Automated Discovery of Failure Domain+ (ADFD+) is an upgraded version of ADFD technique with respect to algorithm and graphical representation of failure domains. The new algorithm searches for the failure-domain around the failure in a given radius as against ADFD which limits the search between lower and upper bounds. The ADFD+ graphical output was further improved by providing labelled graphs to make it easily understandable and user friendly. To find the effectiveness of ADFD+, it was compared with Daikon using error seeded programs. The ADFD+ correctly pointed out all the seeded failure domains while Daikon identified individual failures but was unable to discover the failure domains. 


%The results are collected and represented on a graph in a more clarified form. 
%relying on prior existence of test cases to generate invariants, was not able to correctly identify the failure domain

%The ADFD+ output is also compared with the Daikon's output to analyse their behaviour in response to known failure domains.
%Experimental results show that ADFD+ correctly pointed out the planted failure-domains 

%whereas Daikon, which rely on the existing of test cases to generate invariants, was not able to generate invariants that correctly point to the failure-domain. 

%Testers and/or developers can inspect those generated graphs for failures and failure-domains, instead of inspecting a large number of all generated tests. 

%To incorporate the feature of generating results in the graphical form the Automated Discovery of Failure Domain (ADFD) strategy was developed.  

%A set of techniques and tools have been developed for improving the effectiveness of automated random testing in finding failures and failure-domains. The first technique DSSR (Chapter \ref{chap:DSSR}), starts with random strategy to find the first failure. When a failure is identified, the DSSR strategy selects neighbouring values for the subsequent tests. The selected values sweep around the failure, leading to the discovery of new failures in the vicinity. Experimental results show that DSSR performed significantly better than random and random+ strategy. 

%The second technique ADFD (Chapter \ref{chap:ADFD}), starts with random+ strategy to find the first failure. When a failure is identified a new Java program is dynamically created at run-time. The program is compiled and executed to search for failure-domains in the specified range. The output of the tests are collected and represented in the graphical form.
%It improves the debugging efficiency as debuggers keep all the instances of a failure under consideration during debugging. Experimental results show that ADFD technique correctly identified the failure domains in the error-seeded programs. 


%The third technique ADFD+ ((Chapter \ref{chap:ADFD+}), is an improvement of ADFD strategy with a better algorithm to find failure and failure-domains. It searches for the failure-domains around the found failure in a given radius instead of the lower and upper bound. The results are collected and  represented on a graph in a more simplified manner. The ADFD+ output is also compared with the Daikon's output to analyse their behaviour in response to known failure domains. Experimental results show that ADFD+ correctly pointed out the planted failure-domains whereas Daikon, which rely on the existing of test cases to generate invariants, was not able to generate invariants that correctly point to the failure-domain. The ADFD+ graphical output is made further user friendly providing labelled graphs making it more simple to understand. Testers and/or developers can inspect those generated graphs for failures and failure-domains, instead of inspecting a large number of all generated tests. 

\section{Lessons Learned}
Research in the field of software testing has been in progress for more than three decades but only a handful of free and open source fully automated testing tools are available for software testing. The current study is in continuation of the research efforts to find improved testing techniques capable to identify failures and failure domains quickly, efficiently and effectively. In the following section, the lessons learned during the study are presented in the summarized form which may be of interest to the researchers pursuing future research.\\

%The research in this thesis has developed new techniques to improve the effectiveness of automated random testing. 
%Our research is motivated to investigate how to improve the effectiveness of random testing for contagious failures across the input domain and graphical representation of the failure-domains. 
%Our research has shed light on this promising direction and pointed out future work along this direction. 
%In this section, we summarize some lessons that we learned from this study and we hope these lessons may be of interest to the researchers in pursuing future research.
%Better tool support is needed to help the research community in order to devise new testing techniques to meet the demand for high software reliability.  
  
    
%\subsection{Performance measurement criteria may be carefully chosen}
\textbf{Careful selection of performance measurement criteria}

Various measures including the E-measure, P-measure and F-measure have been reported in the literature for finding the effectiveness of random test strategy. The E-measure and P-measure have been criticised~\cite{chen2005adaptive} and are not considered effective while the F-measure has been often used by various researchers~\cite{chen2004statistical, chen1996expected}. In our initial experiments, the F-measure was used to evaluate the efficiency of test strategy. However it was later realised that this was not the right choice. In some experiments a strategy found the first fault quickly than the other but on completion of test session that very strategy found lower number of total faults than the rival strategy. The preference given to a strategy by F-measure because it finds the first fault quickly without giving due consideration to the total number of faults is not fair
~\cite{liu2012comparison}. \\

\textbf{Test results of random testing are fluctuating} 

In random testing the results of the experiments keep on changing even if all the test parameters and the program under test remain the same. The difference in results is due to the random generation of test input. It is therefore complicated to find the efficiency of one technique in comparison to the other technique. We cope the problem in five different ways. 1) To repeat the experiment for sufficient number of times (30 times in our case) and calculate the average. 2) To execute tests in each experiment for relatively large number of times (10,000 in our case). 3) To take sufficient number of test samples (60 in our case) before reaching any conclusion. 4) To Experiment on error-seeded programs where the precise locations of faults are well know before the test. 5) To compare the obtained results with other relevant strategies and find the statistical significance. All the above suggestions minimize the effect of randomness, however, the experiments must be repeated for at least 30 times before deciding the effectiveness of one approach over the other.\\

   
\textbf{More computation decreases performance and increases overhead}

Versions of random testing \cite{} improves the fault finding ability of the technique, however, the cost of generating test data is too high and results in unwanted overhead. It is difficult to achieve high quality strategy without any overhead but efforts should be made to keep it to bare minimum. For example, if we test a program with two strategies A and B. Strategy A consumes 100 tests to find a fault while strategy B consumes 500 tests to find the same fault.  It looks that strategy A supersedes strategy B however the time to generate the test cases may also be considered which can be equal for both the strategies. In addition to time other resources like hardware etc should also be kept minimum.\\


\textbf{Starting with random testing and switching to exhaustive strategy helps}

In our techniques ADFD and ADFD+, we started the test using random strategy and later switched to exhaustive strategy to find the failure domain of a program. In the strategies the testing starts with random strategy and when a failure is identified the test strategy changes to exhaustive strategy and search all the values around the failure to the specified range set by the tester. The benefit of the approach is that it quickly identify the neighbouring faults which may be difficult to find using random strategy. \\


\textbf{Graphical form helps to easily understand the output}

In our technique we represented the test output in an easy to understand graphical form. The lines and circles with blue colour represent pass values while lines and squares with red colour represents fail values. The graph shows red points when the program fails for only one value, block when the program fails for multiple values and strip when the program fails for a long range of values.  The technique help the debuggers in two ways. First, it reduces the `to' and `from' movement of the program between the testers and debuggers as it identify all the faults in one go. Second, it identifies locations of all fault domains across the input domain in a user-friendly way helping debugger to fix the fault keeping in view its all occurrences.\\

\textbf{Auto-generation of data is simple for primitive types and complicated for reference data types}

We found that comparatively it is less challenging to auto-generate primitive data type values than reference or object or user defined data type values using random testing. To meet the challenge our strategy constructed test cases by creating objects of the classes under test and randomly calls its methods with random inputs according to the parameter's-space. We divided input values into two types i.e. primitive data types and user defined data types. For primitive data types as methods parameters, the random strategy calls \verb+Math.random()+ method to generate arithmetic values which are converted to the required type using Java cast operation. For user-defined data type as a parameter the strategy calls the constructor to generate object of the class at run time. The constructor may possibly require another object, then the strategy recursively calls the constructor of that object. This process is continued till an object with empty constructor or a constructor with only primitive types or the set level of recursion is reached.\\


\textbf{Graphical representation of multi-argument method and reference data type is complicated}

The difficulty to represent results is directly proportional to the number of arguments used in a method. Which means as the number of arguments in a method increases, the more dimensions we need and the more complicated the process get in to. Though it is not difficult to extend the approach to test more than two-dimensional programs containing other primitive types, it would however be difficult to plot them on the chart. The approach can also be extended to test object-oriented programs by implementing objects distance proposed by Ciupa et al. \cite{ciupa2006object}. The details of such an implementation with graphical representation will be complicated.\\


\textbf{Contracts are helpful in finding failures, used as documentation and oracle}

Although our research has developed testing techniques that do not require contracts, we found that the effectiveness of automated testing could be further improved if the tools are given extra guidance in the form of pre-conditions, post-conditions and class invariants. Contracts are like assert statements and its violation i.e. when its value turn false indicate an error in the program. It also serves the purpose of documentation and you can tell on the fly what data a software take and the state before and after execution. The post-condition of a program also serves as an oracle.\\


%\subsection{Random testing may be a good way to go on with robustness testing}





