\externaldocument{chapter2} 
\chapter{Literature Review}
\label{chap:softwareTesting}
The famous quote of Paul Ehrlich, ``to err is human, but to really foul things up you need a computer", is quite relevant to the software programmers. Programmers being humans are prone to errors. In spite of best efforts, some errors may remain in the software after it is finalised.  Errors cannot be tolerated in software because a single error may lead to a large upset in the system. The destruction of Mariner 1 rocket (1962) costing \$18.5 million, Hartford Coliseum Collapse (1978) costing \$70 million, Wall Street crash (1987) costing \$500 billion, failing of long division by Pentium\texttrademark (1993) costing \$475 million, Ariane 5 Rocket disaster (1996) costing \$500 million and many more were caused by minor errors in the software \cite{garfinkel2005history}. According to the National Institute of Standards and Technology, US companies alone bear \$59.5 billion loss every year due to software faults while one-third of it can be eliminated by improving the testing infrastructure~\cite{tassey2002economic}. A software has to satisfy rigorous stages of testing to achieve high quality. The more complex the software the higher the requirements for software testing because of the consequent larger damage involved if a fault remains in the software.

\section{Software Testing} \label{sec:softwareTesting}
According to the ANSI/IEEE standard glossary of software engineering~\cite{american1984}, testing is defined as, ``the process of exercising or evaluating a system or system component by manual or automated means to verify that it satisfies specified requirements or to identify differences between expected and actual results". The testing process, being an integral part of Software Development Life Cycle (SDLC), starts from requirement phase and continues throughout the life of the software according to a predefined test plan. Test plan is a document which defines the goal, scope, method, resources and time schedule of testing \cite{futrell2001quality}. In addition, it includes the testable deliverables and the associated risk assessment. The test plan explains \verb+who, when, why+ and \verb+how+ to perform a specific activity in the testing process. 

In traditional testing, when a fault is found by the testers, the software is returned to the developers for rectification and the updated version is given back to the testers for retesting. It is important to note that a successful test is the one that fails a software or identifies a fault in the software~\cite{myers2011art} while fault denotes error made by programmers during software development~\cite{american1984}. The faulty code on execution can lead to software failures. A software that passes all tests without giving any error is not guaranteed to be error free. However, the testing process increases confidence of users and reliability of the tested product~\cite{dahl1972structured}. 

\bigskip
\begin{figure}[h]
	\centering
	\centerline{\includegraphics[width=16cm, height=5cm]{chapter2/softwareTesting.png}}
	\bigskip
	\caption{A simplified version of software testing process}
	\label{fig:softwareTestingParts_2}
\end{figure}
\bigskip

The process of software testing in its simplest form is shown in Figure~\ref{fig:softwareTestingParts_2}. In the process, test input data selected from the input domain is used to form test cases. The test cases are executed against the SUT and the output obtained is declared as pass or fail on the criteria defined in the test oracle. The input domain, test case and test oracle are briefly described below.

\subsection{Input Domain} 
The input domain comprises all possible inputs for a software, including all global variables, method arguments and externally assigned variables. For a given program P with input vector $ P =\{x1, x2, . . . , xn\}$, having $\{D1, D2, . . . , Dn\}$ as the domain of each input so that $x1 \in D1, x2 \in D2$ and so on, the domain D of a function is the cross product of the domains of each input: $D = D1 \times D2 \times . . . \times Dn$.

\subsection{Test Case}
%\begin{wrapfigure}{r}{0.35\textwidth}
%  \vspace{-20pt}
%  \begin{center}
%    \includegraphics[width=0.30\textwidth]{chapter2/testCase.png}
%  \end{center}
%  \vspace{-20pt}
%  \caption{Test case}
%  \label{fig:testCase}
%  \vspace{-10pt}
%\end{wrapfigure}
Test case is an artifact which delineates the input, action and expected output corresponding to the input~\cite{ahmed2010software}. The test case is declared pass if the output obtained after executing the test case comply with the expected output meaning thereby that the functionality is working correctly. Alternatively the test case is declared fail if the output obtained after executing the test case does not comply with the expected output meaning thereby that the functionality is working incorrectly. %Interesting the test case which fails is considered successful from software testing point of view. 
A test suite comprising a series of test cases is commonly executed to establish the desired level of quality.


\subsection{Test Oracle}
Test oracle is defined as, ``a source containing expected results for comparison with the actual result of the SUT"~\cite{ahmed2010software}. For a program \verb+P+, an oracle is a function which verifies that the output from \verb+P+$ is the same as the output from a ‘correct’ version of \verb+P+$~\cite{howden1986functional}. Test oracle sets the acceptable behaviour for test execution~\cite{baresi2001test}. Software testing techniques depend on the availability of test oracle~\cite{gaudel2010software}. Designing test oracle for ordinary software may be simple and straightforward. However, for relatively complex software, designing of oracle is quite cumbersome and requires special expertise. 
Some common issues related to the design of test oracle are as follows:

\begin{enumerate}
\item It is assumed that the test results are observable and comparable with the oracle~\cite{weyuker1982testing}.
\item Ideally, test oracle would satisfy desirable properties of program specifications~\cite{baresi2001test}.
\item As pointed out by Weyuker, ``truly general test oracles are often unobtainable''~\cite{weyuker1982testing}. 
\end{enumerate}
\bigskip
Post-conditions are commonly used test oracle in automated software testing. Post-conditions are conditions which must be true after a method is successfully executed. In such oracle a fault is signalled when a post-condition is violated~\cite{meyer2009programs}. 
Some common artefacts used as oracles are as follows:

\begin{enumerate}
\item Specification and documentation of software.
\item Products similar to the SUT but different in algorithm. %to solve the similar problem.
\item Heuristic algorithms to provide exact results for a set of test cases. % Hoffman, Douglas; Heuristic Test Oracles, Software Testing & Quality Engineering Magazine, 1999
\item Statistical characteristics to generate test oracle. % \cite{mayer2004test}. 
\item Comparison of the result of one test with another for consistency. % Hoffman, Douglas; Analysis of a Taxonomy for Test Oracles, Quality Week, 1998
\item Generation of model for verification of SUT behaviour. % \cite{robinson1999finite}.
\item Manual analysis by human experts to verify the test results. %\cite{jalote1997integrated}. 
\end{enumerate}

\section{Software testing view points}
Software testing from various view points is presented in Figure~\ref{fig:softwareTestingFlow_2} and each one is described in the following sections.

\begin{figure}[H]
	\centering
	\centerline{\includegraphics[width=16cm, height=19cm]{chapter2/softwareTestingFlow.png}}
	%\centerline{\includegraphics[width=\textwidth]{chapter2/softwareTestingFlow.png}}
	\caption{Software testing from various view points}
	\label{fig:softwareTestingFlow_2}
\end{figure}

%\begin{table}[ht]
%%\scriptsize
%\caption{Parts of Software Testing} % title of Table
%\bigskip
%\centering % used for centering table
%{\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
%\begin{tabular}{| l | l | l | l | } % centered columns (4 columns)
%\hline
%
%Level 					&Purpose		 		& Perspective					& Form 					\\
%\hline
%Unit					& Functional			& White Box							& Static 					\\
%Integration				& Structural			& ~~~~Data Flow Analysis				& ~~~~Reviews			\\
%System					& Robustness		& ~~~~Control Flow Analysis			& ~~~~Walkthroughs		\\
%						& Stress				& ~~~~Code-based fault injection 		& ~~~~Inspection		\\
%						& Compatibility		& Black Box							& Dynamic				\\
%						& Performance		& ~~~~Use-case testing				& ~~~~Execution			\\
%						&					& ~~~~Partition testing				& ~~~~Stubs				\\
%						&					& ~~~~Boundary Value testing			& ~~~~Drivers			\\
%						&					& ~~~~Formal Specification testing		&						\\
%
%
%
%\hline %inserts single line
%\end{tabular}
%}
%\bigskip
%\label{table:softwareTestingParts} % is used to refer this table in the text
%\end{table}



\section{Software Testing Levels}
The three main levels of software testing reported in the literature includes Unit testing, Integration testing and System testing~\cite{chilenski1994applicability}. Unit testing deals with evaluation of code piece-by-piece and each piece is considered as independent unit. Integration testing is performed to make sure that the integration in the components formed by the combination of units are working properly. System testing ensures that the system formed by the combination of components proceeds properly to give the required output.

\section{Software Testing Purpose}
The purpose of software testing is to achieve high quality by identifying and eliminating faults in the given SUT. Maximum number of faults can be identified if software is tested exhaustively. However, exhaustive testing is not always possible because of limited resources and infinite number of input values that a software can take. Therefore, the purpose of testing is generally directed to achieve confidence in the system involved from a specific point of view. For example, functionality testing is performed to check that functional aspects of software are working correctly. Structural testing involves analyses of code structure for generating test cases in order to evaluate paths of execution and identification of unreachable or dead code. Robustness testing includes observation of the software behaviour when it receives input outside the expected range. Stress and performance testing aims at testing the response of software under high load and checking its ability to process different nature of tasks~\cite{cohen2005robustness}. Compatibility testing is performed to check and find any fault in the interaction of software with the underlying application or system software.
 %As proper planning is the key to success for many projects this is often also true with software testing. A software test plan is a well defined document that defines the goal, scope, method, resources and time schedule of the testing.
%A software testing technique in which a software is tested with all possible combination of inputs. This technique can prove conclusively that the software meet its specification however exhaustive testing is seldom feasible because of the large input domain or too many paths in a software code. 

\section{Software Testing Perspective}
Based on the perspective taken, software testing is divided into white-box and black-box testing as described below.

\subsection{White-box Testing}
%\begin{wrapfigure}{r}{0.36\textwidth}
%  \vspace{-35pt}
%  \begin{center}
%   \includegraphics[width=0.30\textwidth]{chapter2/whiteBox.png}
%  \end{center}
%  \vspace{-20pt}
%  \bigskip
%  \caption{White-box testing}
% \vspace{-18pt}
%\end{wrapfigure}
White-box testing also known as structural testing is a technique that takes into consideration the structure of the software. The testers should know about the complete structure of the software in order to make necessary modifications, if so required. Test cases are derived from the code structure and test passes if the results are correct  and the proper code is followed during test execution~\cite{ostrand2002white}. A simplified version of white-box testing is shown in Figure~\ref{fig:whiteBox}. Some commonly used white-box testing techniques are as follows:

\bigskip
\begin{figure}[H]
\begin{center}
	\includegraphics[width=4cm, height=4.5cm ]{chapter2/whiteBox.png}
	\bigskip
	\caption{White-box testing}
	\label{fig:whiteBox}
\end{center}  
\end{figure}

\subsubsection{Data Flow Analysis}
Data Flow Analysis (DFA) is a technique which focuses on the input values by observing the behaviour of respective variables during execution of the SUT~\cite{clarke1989formal}. In this technique a Control Flow Graph (CFG) representing all possible states of a program is drawn to determine the paths that are traversed by the program during test execution. Test cases are generated and executed to verify conformance with CFG on the basis of data flow. 

The data flow analysis observes the program execution as data flow from input to output. The data may transform into several intermediary steps before reaching the final state. The transformation process is prone to several errors e.g. references made to non existing variables, values assigned to undeclared variables and change of variables in undesired manner. Ordered use of data is crucial to ensure that the aforementioned errors do not occur~\cite{fosdick1976data}.

\subsubsection{Control Flow Analysis}
Control Flow Analysis (CFA) is a technique which takes into consideration the control structure of a given SUT. Control structure is the order in which the statements, instructions and function-calls are executed. Like DFA, this technique also involves drawing of CFG to determine the traversable paths by a program during the execution. Test cases are generated and executed to verify conformance with CFG on the basis of control flow. For example, the CFA ensures that the selected test cases execute all possible control choices at least once when two or more control choices are available to reach a particular state in the given SUT. %Two of the most common measurement criteria defined by Vilkomir et al. are Branch coverage and Condition coverage~\cite{vilkomir2003tolerance}. 

\subsubsection{Fault Injection Testing}
Fault injection testing is a validation technique to find error handling behaviour of software, examine the capability of test procedure and measure the code coverage achieved by the testing process~\cite{arlat1990validation}. The fault injection is usually software-based or hardware-based as stated below:

\textbf{Software-based fault injection:} In this technique faulty code is injected in to the SUT at one or more locations to analyse the software behaviour in response to the faults~\cite{voas1997software}. For example, changing value of a variable or return type of a method. The process of code addition (instrumentation) is performed before compilation and execution of software. 

\textbf{Hardware-based fault injection:} In this technique faults are injected by disturbing physical environment of the system to analyse the system behaviour in response to the change condition. For example injecting different voltage sags, introducing electromagnetic interference and change in temperature.

\subsection{Black-box Testing}
%\begin{wrapfigure}{r}{0.36\textwidth}
%  \vspace{-35pt}
%  \begin{center}
%    \includegraphics[width=0.30\textwidth]{chapter2/blackBox.png}
%  \end{center}
%  \vspace{-20pt}
%  \bigskip
%  \caption{Black-box testing}
%  \label{fig:blackBox}
%  \vspace{-18pt}
% \end{wrapfigure}
Black-box testing also known as functional testing is a technique that takes into consideration the function of the software. The testers may not know about the structure of the software. Test cases are derived from the software specifications and test passes if the result is according to expected output irrespective of the internal code followed during test execution~\cite{beizer1995black}. A simplified version of black-box testing is shown in Figure~\ref{fig:blackBox}. Some commonly used black-box testing techniques are as follows.

\bigskip
\begin{figure}[h]
\begin{center}
	\includegraphics[width=4cm, height=4.5cm ]{chapter2/blackBox.png}
	\bigskip
	\caption{Black-box testing}
 	\label{fig:blackBox}
\end{center}  
\end{figure}
\bigskip



\subsubsection{Use-case Based Testing}
% check if it is use case-based testing or use case testing.
It is a testing technique which utilizes use-cases of the system to generate test cases. Use-case defines functional requirement at a particular point in the system from actor's perspective. It consists of a sequence of actions to represent a particular behaviour of the system. A use-case format includes brief description, flow of events, pre-conditions, post-conditions, extension points, context and activity diagrams. The use-case contains all the relevant information required for test case, therefore, it can be easily transformed into a test case~\cite{armour2000advanced}. Use-case testing is effective in terms of cheap generation of test cases, avoidance of test duplication, increased test coverage, easier regression testing and early identification of missing requirements.  

% steps taken from presentation of Raional User Conference 2003. Check it for viva.
\subsubsection{Partition Testing}
It is a testing technique in which the input domain of a given SUT is divided into equal sub-domains for testing each sub-domain individually. The division is based on software specifications, code structure and the process involved in software development~\cite{hamlet1990partition}. The performance of partition testing is directly proportional to the quality of sub-domain~\cite{weyuker1991analyzing}. Division of input domain into equal partitions is often difficult. To overcome the problem, a new version of partition testing called proportional partition testing is devised~\cite{chan1996proportional}. In this version, the sub-domains vary in size and the number of test cases selected from each partition is directly proportional to the size of the partition. Ntafos~\cite{ntafos1998random} have provided experimental evidence in support of better performance of proportional partition testing than partition testing.


\subsubsection{Boundary Value Analysis}
Boundary Value Analysis (BVA) is a testing technique based on the assumption that errors often reside along the boundaries of the input variables. Thus border values are taken as the test data set in BVA. According to IEEE standards~\cite{radatz1990ieee}, boundary values contain minimum, maximum, internal and external values specified for a system.
The following code illustrates the ability of BVA to find a failure. 
\bigskip
\begin{lstlisting}
 public void test (int arg) {
	arg = arg $^+$ 1;
	int [] intArray = new intArray[arg];
	...
 }
\end{lstlisting}
\bigskip
On passing interesting value \verb+MAX_INT+ as argument to the \verb+test+ method, the code in the method increment it by 1 making it a negative value and thus an failure is generated when the SUT tries to set the array size to a negative value.

BVA and partition testing may be used in combination by choosing test values from the whole input domain and also from the borders of each sub-domain. Reid et al. \cite{reid1997empirical} have provided evidence in support of better performance of BVA in combination with partition testing as compared to each individually. They have attributed better performance to accurate identification of partition and selection of boundary values. 


\subsubsection{Formal Specification Testing}
It is a testing technique based on mathematical model which provides the opportunity to handle the specifications mathematically. The mathematical notations express the formal specifications with precisely defined vocabulary, syntax and semantics. This feature facilitates isolation, transformation, assembly and repackaging of the information available in the specifications for use as test cases~\cite{donat1997automating}. The two approaches of formal specification testing are Algebraic and Model-based.

The formal specification testing is more productive because of the creation of test cases independent from the code of the SUT~\cite{gaudel2010software}. The effort of generating test oracle is avoided by using available specification model for verifying the test results~\cite{bertolino2007software}. The technique is highly effective in identifying errors, incompleteness and inconsistencies in software requirements~\cite{sommerville2013software}.
  

%\section{Common Techniques of Software Testing}
%This section briefly define some of the most common techniques of software testing currently being used in the testing industry. These include techniques from both white-box and black-box testing techniques.

% Check wikipedia for them.


%\subsubsection{Grey-Box Testing}
%Grey-Box testing is the combination of both black-box/functionality and white-box/structural testing. The tester knows about both the functionality and the internal structure of the SUT. Some of the test cases are based on the functionality and some of the test cases are based on the structure. Emphasis of grey-box testing is both on code coverage as well as functionality~\cite{Savenkov2008}.

%\subsection{Software Testing Workflow}
%There are many software techniques like unit testing, integration testing, random testing, regression testing, system testing, acceptance testing, performance testing, load testing, stress testing, alpha testing, beta test etc. All testing techniques belong to black-box, white-box or grey-box approach. Each testing technique has its own strength and weaknesses but the technique in focus here is Random Testing.


%\begin{figure}[h]
%\begin{center}
%	\includegraphics[width=16cm, height=12cm ]{Literature/Drawing34.jpg}
%	\caption{Software Testing Workflow}
%\end{center}  
%\end{figure}


%We have explained software testing graphically with the help of plotting venn diagram on two dimensional axis. The positive x axis represent black-box while negative x axis represent white-box testing. Grey-box testing in the middle is represented by the overlapping of black-box and white-box testing. Similarly on positive y axis we have dynamic testing and on negative y axis we have static testing.
%Now if a test is black box and dynamic then the test will fall in 0 to 90 degree on the diagram and if the test is black-box and static then it will fall in 270 to 360 degree. On the other hand if the test is white-box and dynamic then it will fall in 90 to 180 degree and if the test is white-box and static then it will fall in 180 to 270 degrees.

%\subsection{Automated Test Generation}
%\subsection{Generation Strategies}


\section{Software Testing Types}
There are two types of software testing: static and dynamic.

\textbf{Static testing:} It involves analysis of test cases statically for checking errors without executing the test cases. Static analysis is applicable to evaluate the quality of software code and supporting documents including requirements, design, user manual, technical notes and marketing information. Reviews, walkthroughs and inspections are most commonly used techniques for static testing.

\textbf{Dynamic testing:} It involves execution of test cases against SUT. The results obtained are analysed against expected output to find any error in the software. Unit testing, integration testing, system testing and acceptance testing are most commonly used methods for dynamic testing~\cite{fairley1978tutorial}.

%Dynamic testing can be manual or automated. In manual testing the programmer develops the test cases which are executed by the developed software to find any error in processing or output. Similarly in automated testing the software or components of the software is given as input to testing software that automatically generates test cases and executes the SUT against them to find any errors. Manual testing typically consumes more time and resources than automated testing.

\subsection{Manual Software Testing}
Manual testing is the technique in which the tester writes the code by hand to create test cases and test oracles for finding faults in software~\cite{ciupa2008finding}. Its advantage is that the test cases are usually generated to target those parts of the SUT which are assumed to be more error-prone. It is highly useful to capture special cases for which automated testing might not guess. Manual unit testing is particularly popular because of the availability of xUnit family of tools, e.g. jUnit for Java, sUnit for Smalltalk and pyUnit for Python. According to a recent survey, 79\% of the Microsoft developers write unit tests for software testing~\cite{leitner2009effectiveness}. The tools automate the execution process of the hand written test cases providing significant practical benefits.  

The limitations of manual testing are that the testers should have appropriate skills, experience and knowledge of the SUT for its evaluation from different perspectives.Manual testing may be effective at smaller scale of software testing but at a larger scale it is generally laborious, time consuming and error-prone~\cite{tretmans2000automatic}. Manual testing usually produce low coverage because sheer numbers of test cases are required for high coverage.


 
\subsection{Automated Software Testing}
Automated software testing refers to the technique in which an automated tool is used to perform the testing process automatically~\cite{leitner2007reconciling}. There are some tools available for automating a part of testing process like generation of test cases or execution of test cases or evaluation of results while other tools are available for automating the whole testing process. 

A fully automated testing system is capable to test software without any user intervention. This is usually achieved by utilizing the contracts (preconditions, postconditions and invariants) embedded in the program code. Preconditions are used to filter out invalid inputs and postconditions are used as oracle~\cite{leitner2007reconciling}. Eiffel~\cite{ecma2005367} and Spec\#~{barnett2005spec} languages have built-in contracts whereas Java can use add-ons like JML~\cite{leavensjml}, iContract~\cite{kramer1998icontract} or OCL~\cite{richters1998formalizing} to enable it. 
%Increase in functionality, higher productivity and lower cost of production without compromising quality are the desirable features in favour of automating the process of software testing. 
Automated software testing may involve higher initial cost but brings the key benefits of lower cost of production, higher productivity, maximum availability, greater reliability, better performance and ultimately proves highly beneficial for any organisation. Automated testing is particularly effective when the nature of job is repetitive and it is performed on routine basis like unit testing and regression testing where the tests are re-executed after each modification~\cite{huang2003automated}. The use of automated software testing makes it possible to test large volumes of code, which may be impossible otherwise~\cite{ramamoorthy1975testing}.

\section{Test Data Generation}
Test data generation in software testing is the process of identifying and selecting input data which satisfies the given criterion. A test data generator is used to assist testers in the generation of data while the selection criterion defines the properties of test cases to be generated based on the test plan and perspective taken~\cite{korel1990automated}. Various artefacts of the SUT can be considered to generate test data like requirements, model, code etc. The choice of artefacts selected limits the kind of test selection criteria that can be applied in guiding the test case generation. 

A typical test data generator consists of three parts: Program analyser, Strategy handler and Generator~\cite{edvardsson1999survey}. Program analyser performs initial assessment of software prior to testing and may alter the code if so required. For example, it performs code instrumentation or construction of CFG to measure the code coverage during testing. Strategy handler defines the test case selection criteria. This may include the formalisation of test coverage criterion, the selection of paths and normalisation of constraints. It may also get input from program analyser or user before or during test execution.

\bigskip
\begin{figure}[h]
	\centering
	\centerline{\includegraphics[width=16.5cm, height=7.2cm ]{chapter2/Generators.png}}
	\bigskip
	\caption{Types of test data generator}
	\label{testDataGenerators_2}
\end{figure}
\bigskip

Generator generates test cases according to the selection criteria identified by the strategy handler. Test data generators are classified into path-wise, goal-oriented, intelligent, search-based and random on the basis of approach followed as shown in Figure~\ref{testDataGenerators_2}. Each type is briefly described in the following section.

\subsection{Path-wise Test Data Generator} 
\label{sec:pathwise_2}
Path-wise test data generator selects a set of test data from the input domain in order to target path, branch and statement coverage in a given SUT. The process is typically accomplished in three stages: CFG construction, path selection and test data generation. In the process a particular path is selected either manually or by automatic means. The generator identifies and generates relevant test data required for execution of the intermediary statements along the selected path. The data generated in path testing expresses boolean true/false behaviour for each node in the path. A complete path contains multiple sub-domains, each consisting of test inputs required to traverse the path. The boundary of sub-domains are obtained by the predicates in the path condition. %The test data generated to traverse a certain path in the SUT is selected from an input space split into a set of sub-sections. 

% the pathwise test data generation is taken from a book knowldge mining using intelligent systems. you can reference it too.


\subsection{Goal-oriented Test Data Generator}
\label{sec:goaloriented_2}
Goal-oriented test data generator generates data to target a specific program point~\cite{chungautomated}. The tester can select any path among a set of existing paths as long as it reaches the specified program point. This technique utilizes runtime information for computing accurate test data~\cite{ferguson1996chaining}. Among various methods used in goal-oriented test data generation the following two commonly adopted approaches are briefly described.

\subsubsection{Chaining Approach}
The chaining approach uses data dependent analysis to guide the test data generation. In the process all the related statements affected by execution of the statement under test are selected automatically. The dependent statements are executed before the selected statement to generate the required necessary data for the execution of the statement under test~\cite{ferguson1996chaining}. The chaining approach analyses the program according to the edges and nodes. For each test coverage criterion different initial event sequence and goal nodes are determined. For example, consider  the branch $(p, q)$, where $p$ is  the starting node of the branch and $q$ is the last node in the branch. The initial event sequence $E$ for the branch $(p, q)$ is defined as $E =< (s,\phi), (p,\phi),(q,\phi) >$, provided that $s$ is the starting node of the program and $\phi$ is the set of variables referred to as constraint. The Branch classification process identifies critical, semi-critical and non-critical nodes for each branch. During execution, the classification guides the search process to select specific branches to reach the goal node.  

\subsubsection{Assertion-oriented Approach}
The assertion-oriented approach adds assertions to the program code with the goal to identify program input on which an assertion is violated indicating a fault in the SUT. An assertion is a constraint applicable to a state of computation which can be either true or false. For example, consider a given assertion A, now find program input x on which assertion A is false, i.e. when the program is executed on input x and the execution reaches assertion A, it is evaluated as false indicating a fault in the SUT. It is not always possible to generate test cases that violate assertions. However, experiments have shown that assertion-oriented test data generation may frequently detect errors in the program related to assertion violation. The major advantage of this approach is that each generated test data uncovers an error in the program with violation of an assertion. An assertion is violated due to fault in program code, in pre or post-condition or a fault in the assertion itself.

% check the korel1996assertion for the above whole text and assertion oriented approach $^+$ the model hard copy thesis.


\subsection{Intelligent Test Data Generator}
\label{sec:intelligent_2}
Intelligent test data generator is used to overcome the problems like generation of meaningless data, duplicated data and complex data associated with traditional data generators. The approach increases user confidence in the generated test data and the testing process~\cite{ramamoorthy1975testing}. It helps in finding the appropriate test data by performing sophisticated analysis to anticipate different situations that may arise at any point in the SUT such as genetic algorithm. The approach produces test data which satisfy the SUT requirements but consumes more time and resources.

\subsubsection{Genetic Algorithm}
The genetic algorithm is a heuristic that mimics the evolution of natural species for searching optimal solution of a problem. It is guided by control dependencies in the program to search for data which satisfy test requirements. The algorithm evaluates the existing test data, and guides the direction of search by using the program control-dependence graph~\cite{pargas1999test}. The approach emphasises on the execution of a given statement, branch, path and condition in the given SUT. The benefit of the genetic approach is quick generation of test cases with proper focus and direction. The new test cases are generated by applying simple operations on the appropriate existing test cases having good potential of satisfying the test requirements. The success of the approach depends heavily on the way in which the existing test data are measured~\cite{pargas1999test}.

% Please paraphrase the above section genetic algorithm.

\subsection{Search-based Test Data Generator} \label{sec:search_based_2}
Search-based test data generation is mentioned in the literature for the first time by Webb Miller and David Spooner in 1976~\cite{miller1976automatic}. It uses meta-heuristic algorithms to generate test data. The technique rely on the problem-specific fitness function for optimization process. Fitness function guides the search to good solutions from a large or infinite search space within a specific time~\cite{mcminn2011search}. Hill-climbing, simulated annealing and genetic algorithms are the most common optimization algorithms used in search-based test data generation. Search-based test data generation technique can reduce time and effort by automatically generating relevant test cases. It has been successfully applied, especially in structural testing~\cite{mcminn2011search, jones1996automatic}.

In search-based test data generation technique each input vector~\verb+x+ is associated with a measure~\verb+cost(x)+ which represents the difference between the input value~\verb+x+ and the set goal. Input value closer to the set goal has low cost value as against the input value away from the set goal. 
Let us consider the following program.
\bigskip
\bigskip

\begin{lstlisting}
void test(int x,int y) {
	if (x >= 20) {
		y = z; 
	}
	else {
		y = 2 * z;
	}
}
\end{lstlisting}
\bigskip

Suppose we want the true branch of the if/else statement to be executed. An input value of~\verb+x == 25+ clearly satisfies the predicate, and a value of~\verb+x == 15+ is closer to satisfy the predicate than a value of~\verb+x == 5+. We evaluate a cost function probe of the form~\verb+cost(x) = max {0, 20 - x}+. Thus \verb+x == 25+ has cost~\verb+0+,~\verb+x == 15+ has cost~\verb+5+ and~\verb+x = 5+ has cost~\verb+15+. It is apparent that finding data to satisfy the branch predicate is essentially a search over the input domain of ~\verb+x+ to find a value such that~\verb+cost(x) == 0+. The data which satisfy each of the predicates at different points may be selected to follow a particular path in the code. This leads to a cost function which combines costs at each of the relevant branching point. The cost function plays the role of oracle for each targeted test requirement. Consequently, the cost function must change as per requirement. Frequent re-instrumentation of program is required to find test data that fully satisfy common coverage criteria. 



%the cost function plays the role of oracle for each targeted test requirement.
%At its heart search based software testing requires the use of search or optimisation algorithms. 

% The details need not concern us here, and the reader is referred to McMinn [50] for details. Overall the search based 





%\subsection{Using A Model Checker}
%\subsection{Test Case Generation with Gatel by Using Lustre}
%\subsection{Using Models in Z}
%\subsectioin{Using UML Diagrams}
%\subsection{Using Misuse / Abuse Cases for Robustness Testing}
%\subsection{Randomly-generated test suites} % Dynamically discovering likely program invariants thesis page 77
%\subsection{Grammar-generated test suites Randomly}% Dynamically discovering likely program invariants thesis page 78
%\subsection{Test-case generation} %Combining over-and under-approximating program analysis for automatic software testing (section 4.2)
%\subsection{Mutation generation} % Automatic testing of software with structurally complex inputs. page 79 section 7.2.1
%\subsection{Test Data Generation} % page 70, 4.6.1, Coverage analysis for GUI Testing
% Check Generating Structurally complex tests from declarative constraints thesis 




% add diagram for generators similar to john paper in automated program flaw finding using simulated annealing.


\subsection{Random Test Data Generator}
\label{sec:randomgenerator_2}
Random test data generator is the simplest approach for generation of test data. Its advantage is the adaptability to generate input data for any type of program. However, random test data generation is based solely on probability and cannot accomplish high coverage as its chances of finding semantically small faults are quite low~\cite{godefroid2005dart}. If a fault is only revealed by a small percentage of the program input it is said to be a semantically small fault. As an example of a semantically small fault, consider the following code:
\bigskip
\begin{lstlisting}
void test(int x, int y) {
    if (x==y){
        System.out.println("Equal");
    }
    else {
        System.out.println("Not Equal");
    }
}
\end{lstlisting}
\bigskip

It is clear that the probability of execution of the first statement is significantly lower than that of the second statement. As the structure gets more and more complex, the probability of execution decreases accordingly. Thus semantically small faults are hardly detectable by using random test data generator. 



\section{Random Testing} \label{sec:randomTesting}
Random testing is mentioned in the literature for the first time by Hanford in 1970 who reported syntax machine, a tool that randomly generated data for testing PL/I compilers~\cite{hanford1970automatic}. Later in 1983, Bird and Munoz described a technique to produce randomly generated self checking test cases~\cite{bird1983automatic}. 
%Work on random test generation dates back to a 1970 paper by Hanford. In it he reports on the ``syntax machine", a program that generated random but syntactically-correct programs for testing PL/I compilers \cite{hanford1970automatic}. The next published work on random testing was a 1983 paper by Bird and Munoz \cite{bird1983automatic}. They describe a technique to generate randomly-generated ``self-checking" test cases: test cases that in addition to generating test inputs generated checks for expected outputs using knowledge of the semantics of the software under test. For example, when generating random programs for testing a compiler, the generator kept track of the expected values of arithmetic computations and inserted the appropriate checks into the test case. They also applied their technique to checking sorting and merging procedures and graphic display software~\cite{pacheco2009directed}.
%One of the best-known works in the field is Miller et al.’s “fuzz testing” paper, where they generate random ASCII character streams and use them as input to test Unix utilities for abnormal termination or non-terminating behaviour \cite{miller1990empirical}. In subsequent work, they extended fuzz testing to generate sequences of keyboard and mouse events, and found errors in applications running in X Windows, Windows NT and Mac OS X \cite{forrester2000empirical, miller2006empirical}. Today, fuzz testing is used routinely in industry. It is frequently used as a tool for finding security vulnerabilities and is applied to test formats and protocols that attackers might use to gain unauthorized access to computers over a network. Other work that applies random test generation to operating systems code includes Kropp’s use of random test generation to test low-level system calls \cite{kropp1998automated}, and Groce et al.’s use of random test generation at NASA to test a file system used in space missions \cite{groce2007randomized}. In all these studies, a random test generator invariable found many errors in the software under test, and many errors were critical in nature. For example, Groce et al. found dozens of errors in the file system under test, many of which could have jeopardized the success of a mission.
Random testing is a dynamic black-box testing technique in which the software is tested with non-correlated unpredictable test data from the specified input domain~\cite{chan2003normalized}. As stated by Richard~\cite{hamlet1994random}, the test data are randomly selected from the identified input domain by means of random generator. The program under test is executed on the test data and the results obtained are compared with the program specifications. The test case fails if the results are not according to the specifications reflecting a fault in the given SUT and vice versa. Generating test data by random generator is economical and requires less intellectual and computational efforts~\cite{ciupa2008artoo}. Moreover, no human intervention is involved in data generation which ensures an unbiased testing process. The working mechanism of random testing is shown in Figure~\ref{fig:workingmechanism}.

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=11cm, height=2.5cm ]{chapter2/randomTesting.jpg}
%	\caption{Random Testing}
%\end{figure}
\bigskip
\begin{figure}[ht]
	\centering
	\centerline{\includegraphics[width=16cm, height=10.5cm ]{chapter2/randomTestingMechanism.png}}
	\bigskip
	\caption{Working mechanism of random testing}
	\label{fig:workingmechanism}
\end{figure}
\bigskip
 The generation of test data with out using any background information makes random testing susceptible to criticism. Random testing is criticized for generating many of the test cases that falls at the same state of software. It is also stated that random testing generates test inputs that violates requirements of the given SUT, which makes it less effective~\cite{pacheco2009directed, sen2007effective}. Myers mentioned random testing as one of the least effective testing technique~\cite{myers2011art}. However, Ciupa et al.~\cite{ciupa2007experimental} stated that Myers statement is not based on any experimental evidence. Later experiments performed by several researchers~\cite{ciupa2008finding, hamlet1994random,  leitner2007efficient} confirmed that random testing is as effective as any other testing technique. It is reported~\cite{duran1981report} that random testing can also discover subtle faults in a given SUT when subjected to large number of test cases. It is pointed out that the simplicity and cost effectiveness of random testing makes it more feasible to run large number of test cases as opposed to systematic testing techniques which require considerable time and resources for test case generation and execution. The empirical comparison shows that random testing and partition testing are equally effective~\cite{hamlet1990partition}. Ntafos~\cite{ntafos1998random} conducted a comparative study and concluded that random testing is more effective as compared to proportional partition testing. Miller et al.~\cite{miller1990empirical} generated random ASCII character streams and used the Unix utilities for abnormal terminating and non-terminating behaviours. Subsequently, the technique was extended to discover errors in software running on X Windows, Windows NT and Mac OS X~\cite{forrester2000empirical, miller2006empirical}. Other famous studies using random testing includes low-level system calls~\cite{kropp1998automated} and file systems used in missions at NASA~\cite{groce2007randomized}. The use of random testing at academic and commercial level reported in the literature cited above provides evidence to the fact that it is a well accepted and widely adapted technique for software testing.



\section{Pass and Failure-domain} \label{sec:failuredomains_2}
\label{sec:genuineandfailuredomain}
The sequence of test data across the input domain for which the software behaves correctly is called pass-domain and for which the software behave incorrectly is called failure-domain. Chan et al.~\cite{chan1996proportional} observed that input inducing failures are contiguous and form certain geometrical shapes in the whole input domain. They divided these into point, block and strip failure-domains as described below.

\begin{enumerate}
\item Point domain: In the point failure-domain, input inducing failures are scattered across the input domain in the form of stand-alone points. Example of point failure-domain is the failure caused by the statement: $total = num1/num2;$ where $num1$, $num2$ and $total$ are variables of type integer.
\item Block domain: In the block failure-domain, input inducing failures lie in close vicinity to form a block in the input domain. Example of block failure-domain is failure caused by the statement: $if ( (num \textgreater 10) \&\& (num \textless 20) )$. Here 11 to 19 are a block of failures.
\item Strip domain: In the strip failure-domain, input inducing failures form a strip across the input domain. Example of strip failure-domain is failure caused by the statement: $num1 + num2 = 20$. Here multiple values of $num1$ and $num2$ can lead to the fault value 20. 
\end{enumerate}
\smallskip
\begin{figure} [H]
\centering
\subfigure[Point domain]{
\includegraphics[width=3cm,height=3cm]{chapter2/point.png}
\label{fig:point}
}
\subfigure[Block domain]{
\includegraphics[width=3cm,height=3cm]{chapter2/block.png}
\label{fig:block}
}
\subfigure[Strip domain]{
\includegraphics[width=3cm,height=3cm]{chapter2/strip.png}
\label{fig:strip}
}
\bigskip

\caption{Failure domains across input domain~\cite{chan1996proportional}}
\label{fig:patterns2}
\end{figure}

%\begin{figure}[h]
%	\centering
%	\centerline{\includegraphics[width=13cm, height=5cm ]{chapter2/pointblockstrip.jpg}}
%	\caption{Domains of failure causing inputs~\cite{chan1996proportional}}
%	\label{fig:patterns2}
%\end{figure}

The Figure~\ref{fig:patterns2} shows failure domains across the input domains. The squares in the figure indicate the whole input domain. The white space in each square shows legitimate and faultless values while the black colour in the form of points, block and strip indicate failures in the form of point, block and strip failure-domains.

\newpage
\section{Versions of Random testing}
Researchers have tried various approaches to develop new versions of random testing for better performance. The prominent improved versions of random testing are shown in Figure \ref{fig:randomTestingVersions_2}.

\begin{figure}[H]
	\centering
	\centerline{\includegraphics[width=16.5cm, height=18cm]{chapter2/randomTestingVariations.png}}
	%\centerline{\includegraphics[width=\textwidth]{chapter2/randomTestingVariations.png}}
	\caption{Various versions of random testing}
	\label{fig:randomTestingVersions_2}
\end{figure}

\subsection{Random$^+$ Testing} \label{subsec:randomPlusTesting}
The random$^+$ testing~\cite{leitner2007reconciling} is an extension of the random testing. It uses some special pre-defined values which can be simple boundary values or values that have high tendency of finding faults in the SUT. Boundary values~\cite{beizer2003software} are the values at the start and end of a particular data type. For instance, such values for \verb+int+ could be -3, -2, -1, 0, 1, 2, 3, \verb+Integer.MAX_VALUE+, \verb+Integer.MAX_VALUE-1+, \verb+Integer.MAX_VALUE-2+, \verb+Integer.MIN_VALUE+, \verb-Integer.MIN_VALUE+1-, \verb-Integer.MIN_VALUE+2-. These special values can add a significant improvement to a testing method. \\*
Let us consider the following piece of code:
\bigskip
\bigskip
\begin{lstlisting}
public void test (int arg) {
	arg = arg + 1;
	int [] intArray = new intArray[arg];
	...
}
\end{lstlisting}
\bigskip

In the given code, on passing interesting value \verb+MAX_INT+ as argument, the code increment it by 1 making it a negative value and thus an error is generated when the system tries to build an array of negative size. Similarly, the tester may add some other special values that are considered effective for finding faults in the SUT. For example, if a program under test has a loop from -50 to 50 then the tester can add -55 to -45, -5 to 5 and 45 to 55 to the pre-defined list of special values. This static list of interesting values is manually updated before the start of the test. Interesting values included in the list are given higher priority than random values because of their relevance and better chances of finding faults in the given SUT. As reported in the literature, interesting values have high impact on the results, particularly for detecting problems in specifications~\cite{ciupa2008finding}.


\subsection{Adaptive Random Testing}
\label{sec:artpatterns_2}
Adaptive random testing (ART) proposed by Chen et al.~\cite{chen2005adaptive} is based on the previous work of Chan et al.~\cite{chan1996proportional} regarding the existence of failure domains across the input domain (Section~\ref{sec:genuineandfailuredomain}). Chen et al.~\cite{chen2005adaptive} argued that ordinary random testing might generate test inputs lurking too close or too far from the input inducing failure and thus fails to discover the fault. To generate more fault-targeted test inputs, they proposed ART as a modified version of random testing where test values are selected at random as usual but are evenly spread across the input domain.  The technique uses the candidate set as well as the executed set both of which are initially and as soon as the testing begins ART fills the candidate set with randomly selected test cases from the input domain. The first test case selected at random from the candidate set is executed and stored in the executed set. The second test case is the one selected from the candidate set which is located far away from the previously executed test case. The process continues till test completion and provide greater chances of finding failures from failure domains.

Chen et al.~\cite{chen2005adaptive} used ART in their experiments with the number of test cases required to detect first fault (F-measure) as a performance matrix instead of the traditional matrices (P-measure) and (E-measure). The results showed up to 50\% increase in performance compared to random testing. However, the authors pointed out indicated their concern regarding the issues of spreading test cases across the input domain for complex objects, efficient ways of selecting candidate test cases and higher overhead. %Chen et al. continued their work on ART for addressing the issues and proposed the following upgraded versions. %~\cite{chen2009enhanced} 

\subsection{Mirror Adaptive Random Testing}
%As discussed in the above section ART provide better results, however the increase in overhead due to extra computation to achieve even spread of test inputs makes it less cost effective. 
Mirror Adaptive Random Testing (MART) is an improvement on ART by using mirror-partitioning technique to reduce the overhead and decrease the extra computation involved in ART~\cite{chen2004mirror}. 
\bigskip
\bigskip
\begin{figure}[h]
\begin{center}
	\includegraphics[width=13.5cm, height=6.5cm ]{chapter2/mart2.pdf}
	\bigskip
	\caption{Mirror functions for mapping of test cases}
\label{fig:mirrorART}
\end{center}  
\end{figure}
\bigskip

In this technique, the input domain of the program under test is divided into n number of disjoint sub-domains of equal size and shape. One of the sub-domains is called source sub-domain while all others are termed as mirror sub-domains. ART is then applied only to the source sub-domain while test cases are selected from all sub-domains by using mirror function. In MART $(0, 0), (u, v)$ are used to represent the whole input domain where $(0, 0)$ is the leftmost and $(u, v)$ is the rightmost top corner of the two-dimensional rectangle. On splitting it into two sub-domains we get $(0, 0), (u/2, v)$ as source sub-domain and $(u/2, 0), (u, v)$ as mirror sub-domain. Suppose we get $x$ and $y$ test cases by applying ART to source sub-domain, so we can linearly translate these test cases to achieve the mirror effect, i.e. $(x + (u/2), y)$ as shown in Figure~\ref{fig:mirrorART}. The experimental results by comparing MART with ART provides evidence of equally good performance of the two techniques with the added advantage of lower overhead in MART by using only one quarter of the calculation as compared with ART~\cite{chen2004mirror}.


\subsection{Restricted Random Testing}
Restricted Random Testing (RRT) is another approach~\cite{chan2003normalized} to overcome the problem of extra overhead in ART. The RRT achieves this by creating a circular exclusion zone around the executed test case. A candidate is randomly selected from the input domain as a next test case. Before execution the candidate is checked and discarded if it lies inside the exclusion zone. This process repeats until a candidate present outside the exclusion zone is selected. It ensures that the test case to be executed is well apart from the last executed test case. The radius of exclusion zone is constant in each test case and the area of input domain decreases progressively with successive execution of test cases.
\bigskip
\bigskip
\begin{figure}[h]
	\centering
	\includegraphics[width= 7.5cm, height = 5.5cm]{chapter2/RRT.pdf}
	\bigskip
	\caption{Input domain showing exclusion zones around selected test cases}
\end{figure}
\bigskip

The authors compared MART with ART to find the comparative performance and reported that the performance of both techniques is similar with the added advantage of lower overhead in case of MART which uses only one quarter of the calculations as compared with ART. %Normalized Restricted Random Testing~\cite{chan2003normalized} is an improvement over RRT by allowing the testers to have better information about the target exclusion rate (R) of RRT. 
They further found that RRT is up to 55\% more effective than RT in terms of F-measure.



\subsection{Directed Automated Random Testing}
Godefroid et al.~\cite{godefroid2005dart} proposed Directed Automated Random Testing (DART). %Its main purpose was to overcome the cost and difficulty of manual testing while keeping its quality intact. It automate the whole testing process including generation of unit tests, test drivers/harness and assertions for functional correctness.
In DART process, the given SUT is instrumented to track the dynamic behaviour of the SUT at run time. It also identifies external interfaces of a given SUT. These interfaces include external variables, external methods and the user-specified main method responsible for program execution. After that it automatically generates test drivers for running the randomly generated test cases. Finally the results obtained are analysed in real time to systematically direct the test case execution along alternative path for maximum code coverage. 

The DART algorithm is implemented in the tool which is completely automatic and accepts the test program as input. After the external interfaces are extracted it uses the pre-conditions and post-conditions of the program under test to validate the test inputs. For languages that do not support contracts inside the code (like C), public methods or interfaces are used to mimic the scenario. DART attempts to cover different paths of the program code to trigger errors. Its oracle consists of checking for crashes, failed assertions and non-termination.



\subsection{Quasi Random Testing}
Quasi-random testing (QRT) is a testing technique~\cite{chen2007quasi} which takes advantage of failure-region contiguity for distributing test cases evenly and thus decreases computation. %Chan et al after the analysis of faults in various experiments found that the fault patterns across the input domain are continuous. 
To achieve even spreading of test cases, QRT uses a class with a formula that forms an s-dimensional cube in s-dimensional input domain and generates a set of numbers with small discrepancy and low dispersion. The set of numbers is then used to generate random test cases that are per-mutated to make them less clustered and more evenly distributed. An empirical study was conducted to compare the effectiveness of QRT with ART and RT. The results showed that in 9 out of 12 programs QRT found a fault quicker than ART and RT while there was no significant improvement in the remaining three programs.
%\subsection{Monti Carlo Random Testing}

%\subsection{Good Random Testing}

\subsection{Feedback-directed Random Testing}
Feedback-directed Random Testing (FDRT) is a technique that generates unit test suite at random for object-oriented programs~\cite{pacheco2007randoop}. As the name implies, FDRT uses the feedback received from the execution of first batch of randomly selected unit test suite to generate next batch of directed unit test suite. In this way redundant  and wrong unit tests are eliminated incrementally from the test suite with the help of filtration and application of contracts. For example a unit test that produces $IllegalArgumentException$ on execution is discarded because the arguments used in the unit test are not according to the required type. Pacheco et al. performed a case study in which a team of testers applied FDRT to a critical component of .NET architecture. Results showed that the faults discovered by FDRT in 15 hours of manual processing and 150 hours of CPU processing are more than a test engineer finds in one year by manual and other automated techniques. As a result FDRT has been added to the tool list used at Microsoft for the betterment of software~\cite{pacheco2008finding}. 



%\subsection{Adaptive Random Testing for Object-Oriented}
\subsection{The Artoo Testing}
The Adaptive random testing for object oriented (Artoo) testing is based on object distance. Ciupa et al.~\cite{ciupa2006object} defined the parameters that can be used to calculate distance between the objects. Two objects have more distance between them if they have more dissimilar properties. The parameters to specify the distance between the objects are: dynamic types, values of primitive fields and values of reference fields. Strings are treated in terms of directly usable values and Levenshtein formula~\cite{levenshtein1966binary} is used as a distance criterion between the two strings.

In the Artoo testing, two sets are taken, the candidate-set containing the objects ready to be run by the system and the used-set which is initially empty. The first object is selected randomly from the candidate-set which is moved to used-set after execution. The second object selected from the candidate-set for execution is the one with the largest distance from the last executed object in the used-set. The process continues till the fault is found or the objects in the candidate-set are finished~\cite{ciupa2006object}.

The Artoo testing, implemented in AutoTest~\cite{ciupa2008artoo}, was evaluated in comparison with Directed Random (D-RAN) testing \cite{godefroid2005dart} by selecting classes from EiffelBase library \cite{meyer1987eiffel}. The experimental results indicated that some faults found by the Artoo were not identified by the D-RAN strategy. Moreover the Artoo found first fault with small number of test cases than the D-RAN testing. However, more computation was required to select a test case in the Artoo strategy and the process required more time and cost to generate test cases as compared to D-RAN testing.

% the same team implemented that model and performed several experiments to evaluate the proposed model. Adaptive Random Testing for Object Oriented (ARTOO) is a testing strategy, based on object distance, implemented in AutoTest \cite{16 search it Mendeley}.
%ARTOO was implemented as a plug-in strategy in AutoTest. It only deals with creating and selecting inputs and all other functionality of the AutoTest was the same. Since ARTOO is based on object distance therefore the method for test input selection is to pick that object from the candidate set (A pool of objects that is a potential candidate to be executed by the system) that has the highest average distance in comparison to the objects already executed. In the experiments classes from EiffelBase library \cite{17 search it mendeley} were used. To evaluate ARTOO the same tests were also applied to directed random strategy (RAND). The outcome of the experiments showed that ARTOO finds the first bug with fewer test cases than RAND. The computation to select test case in ARTOO is more than RAND and therefore ARTOO takes more time to generate a test input. The experiments also found few of the bug found by ARTOO were not pointed out by RAND furthermore ARTOO is less sensitive to the variation of seed value than RAND

%\subsection{Object Distance and its application}
%To improve the performance of random testing the emphasis of ART was on the distance be- tween the test cases. But this distance was defined only for primitive data types like integers and other elementary input. Ciupa et al defined the parameters that can be used to calculate distance between the composite programmer-defined types so that ART can be applicable to testing of today’s object-oriented programs~\cite{Ciupa2006}. Two objects have more distance between them if they have more dissimilar properties. The parameters to specify the distance between the objects are dynamic types, values of its primitive and reference fields. Strings are treated as a directly usable values and Levenshtein distance~\cite{Levenshtein1966} that is also known as edit distance is used as a distance criteria between the two strings. To implement object distance first all the distances of the objects are measured. Then two sets candidate- objects containing the all the objects ready to be run by the system and the used-objects set, which is initially empty. First object is selected randomly from the candidate-object set and is moved to used-object set when executed by the system. Now the second object selected from the candidate set for execution is the one with the biggest distance from the last executed object present in the used-object set. This process is continuing until the bug is found or the objects in the candidate-object set are finished.

%\subsubsection{Experimental Assessment of RT for Object-Oriented Software}
%In this research the effect of various parameters involved in random testing and its effect on efficiency is evaluated by performing various experiments on Industrial-grade code base. Large-scale clusters of computers were used for 1500 hours of CPU time which resulted in 1875 test sessions for 8 classes under test.~\cite{Ciupa2007} The finding of the experiments are 1. Version of random testing algorithm that is efficient for smaller testing timeout is equally efficient for higher testing timeouts. 2. The value of seed for random testing algorithm plays a vital role in finding the number of bugs in specific time. 3. Most of the bugs are found in the first few minutes of the testing sessions.



%\subsection{Design by Contract}
% section taken from binding yeti with .net, check it out correct it.
%\textcolor{blue}{Modern software development as it is well known has adopted the paradigm of Object Oriented (OO) Programming. The primary and most important reason for this evolution is the desire for better quality software and a more efficient way to bridge the gap between requirements and code.
%Industrial use of OO confirmed the superiority of this approach over procedural
%programming languages. However, so far no final approach has been generally accepted over a methodology on how to construct OO software in order to achieve one of the most important factors of quality, which is reliability or else robustness. As the work of Dr. Bertrand Meyer has shown \cite{meyer1992applying, meyer1988object} the decision on how to make software more reliable is crucial if the developers want to use the benefits of OO. These benefits include:
%1 Reuse of software components. This implies using a software component at multiple environments apart from the environment in which its developers originally deployed it.
%2 The goal of having reliability plays an important role in characterizing the quality of a software module.
%3 The abstract types that OO introduces. A reliable way to construct abstract types emerges. To achieve the previous goals the software development world has used two approaches, Defensive programming and Design by Contract. As the next paragraphs show the latter one has been showing more advantages than the first one. That is why most researches on Automated Testing use design by contract as a requirement that the SUT must conform to, in order for their tool to find as many bugs as possible \cite{Leitner2007}.
%The idea of Defensive Programming, which is inherited from the software development world prior to OO, is in general advising to ―… include as many checks as possible even if they are redundant \cite{meyer1992applying}. This concept is intuitively stating that having extra checking can never do harm especially if someone wants to protect the software from inexperienced users. This last concept unfortunately is not correct. This kind of approach puts more code inside the software and this contributes to greater complexity of the software, and complexity as Dr. Meyer has very correctly stated ―…is the single worst obstacle to software quality in general, and to reliability in particular \cite{meyer1992applying}. The reason is that this extra code is a source of things that might go wrong and so we must also check it, and so on to infinitum. The need for a more systematic approach is evident based on the concept that software elements are implementations of well-understood specifications by the developers and that is exactly what Design by Contract does. Design by Contract concept adopts the idea that any operation a routine of a software performs, should bind the caller of that routine to the routine itself. This binding provides specific obligations that each party has, along with benefits. Essentially the obligations of one part describe the benefits of the other part. These ideas apply to software routines (i.e. methods, functions of classes) via assertions that implement the binding, which is like a contract between two persons. These assertions are classified into pre-conditions, post-conditions and class invariants. Figure 2.1 which is a figure of the ―Applying Design by Contract article \cite{meyer1992applying}, illustrates the use of these assertions.
%As anyone can understand from Figure ?? pre and post-conditions denote the idea that if the caller promises to invoke the routine with the pre-conditions holding, the routine guarantees the caller that it will return the system in a final state in which the post-conditions hold. Thus, the invoker knows nothing about how the routine reaches the final state but can depend on the results. This implies another thing, which is that the routine is responsible only for the cases where the pre- conditions hold. This means rejecting the whole concept of defensive programming because if something is an assertion in the pre-condition the routine does not have to handle it in its body/code and vice versa. Overall, it is forbidden for the same assertion to exist in both parts. How strong or weak a pre-condition should be is a decision of the developer. So far the notions of pre and post-conditions are assertions that developers can use even with procedural programming languages at each of their routines. The notion that enhances more the efficiency of the contracts is the notion of the class invariant. Through class invariants a developer can describe a general condition that any instantiation of that class must hold at any time \cite{meyer1992applying, meyer1988object}. By this the developers classify the requirements that they have in the same abstract way in which they develop the software; making the bridging of requirements to code more traceable, thus more possible to achieve all the requirements, the basic goal of software development.
%In addition, to the previous benefits design by contract can facilitate documentation. By providing contract information documentation describes a module of code certainly better than just presenting the interface and the result it yields. Furthermore, monitoring these assertions can be very helpful while debugging.
%It is up to the developers to decide what the software must do when one of the parties breaks a contract. It can stop the execution, prompt the event, ask the user to decide etc. It is a decision based on the specifications. Basically as the article of Meyer B. \cite{meyer1992applying} describes, three things can mainly occur
%1. An alternative algorithm starts due to this exceptional behaviour 2. System stops its executions and returns to a prior consistent state 3. A rare but possible false alarm has happened due to operating system or hardware signals and after correcting actions execution continues
%The general approach is to consider and to monitor any violation because most of the times this violation describes a bug. Either in the logic of the requirements (pre-conditions, class invariants) or in the algorithm of the implementation (post-conditions).
%Sometimes research papers mention the contracts of the code as partial specifications \cite{daniel2007automated} because they provide more information than just requirements documents but at the same time they are not a full specification model which describes the whole algorithm of a software module as formal methods do.
%Design by Contract principle helps developers use appropriately polymorphism and dynamic binding. With contracts, designers can have inheritance and reassure that the class that inherits another class will respect the original contract and if the designer wants he/she can add assertions based on the functionality of the child class. The rules that the designers must follow, to maximally use the contracts, is to allow the child classes, when it is desired, to: i) have weaker pre-conditions or ii) stronger post-conditions. In this way contracts provide an immediate guideline to the designer.}


%\subsection{Daikon} % Generating high confidence contracts without user input using... page 6
\newpage
%\section{Automated Random Testing Tools}
\section{Automatic Random Testing Tools}
A number of automatic random testing tools used in research and reported in the literature are briefly described in the following section.
% it is written in Randoop: feedback directed random testing that jcrasher, eclat and jartege are academic tools.


\subsection{JCrasher}
Java Crasher (JCrasher) is an automatic robustness testing tool developed by Csallner and Smaragadakis~\cite{csallner2004jcrasher}. JCrasher tests the Java program with random input. The exceptions thrown during the testing process are recorded and compared with the list of acceptable standards defined as heuristics. The undefined runtime exceptions are considered as failures. JCrasher randomly tests only the public methods of SUT based on the fact that users interact with programs through public methods.

The working mechanism of JCrasher is illustrated by testing a $.java$ program as shown in Figure~\ref{fig:JCrasher}. The source file is first compiled using $javac$ to get the byte code. The byte code obtained is passed as input to JCrasher, which uses Java reflection library~\cite{chan1999java} to analyse all the methods declared by class $T$. The JCrasher uses methods transitive parameter types $P$ to generate the most appropriate test data set which is written to a file $TTest.java$. The file is compiled and executed by JUnit. All exceptions produced during test case executions are collected and compared with robustness heuristic and resulted violations are reported as errors.
\bigskip
\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=6.7cm]{chapter2/JCrasher.png}
	\bigskip
	\caption{How a class T can be checked for robustness with JCrasher. First, the JCrasher application generates a range of test cases for T and writes them to TTest.java. Second, the test cases can be executed with JUnit, and third, the JCrasher runtime filters exceptions according to the robustness heuristic~\cite{csallner2004jcrasher}}
	\label{fig:JCrasher}
\end{figure}
\bigskip

JCrasher is a pioneering tool with the capability to perform fully automatic testing, including test case generation, execution, filtration and report generation. Its novel feature is the generation of test cases as JUnit files which can be easily read and used for regression testing. Another important feature of JCrasher is independent execution of each new test on clean-slate. This ensures that the changes made by the previous tests do not affect the new test.  

% check parameter space or parameter graph in the figure???


\subsection{Jartege} \label{jartege_2}
Java random test generator (Jartege) is an automated testing tool~\cite{oriat2005jartege} that randomly generates unit tests for Java classes with contracts specified in JML. The contracts include class invariants and the pre and post-conditions of methods. Initially Jartege uses the contracts to eliminate irrelevant test cases and later on the same contracts serve as test oracle to differentiate between faults and false positives. Jartege uses simple random testing to test classes and generate test cases. Additionally, testing of a specific part of the class can be prioritized by changing the parameters to get interesting sequence of calls if so desired by the tester. The parameters include the following: 

\begin{itemize}
\item Operational profile of the classes i.e. the likely use of the class under test by other classes.  
\item Weight of the class and method under test. Higher weight prioritizes the class or method over lower weight during test process. 
\item Probability of creating new objects during test process. Low probability means creation of fewer objects and more re-usability for different operations while high probability means numerous new objects with less re-usability.
\end{itemize}

The Jartege technique evaluates a class by entry pre-conditions and internal pre-conditions. Entry pre-conditions are the contracts to be met by the generated test data for testing the method while internal pre-conditions are the contracts which are inside the methods and their violations are considered as faults either in the methods or in the specifications. The Jartege checks for faults in program code as well as in specifications and the JUnit tests produced by Jartege can be used later for regression testing. Its limitation is the prior requirement of JML specifications of the program.

\subsection{Eclat}
Eclat~\cite{pacheco2005eclat} is an automated testing tool which generates and classifies unit tests for Java classes. The tool takes a software and a set of test cases for which the software runs properly. Based on the correct software operations an operational model is created to test the selected data. If the operational pattern of the test data differs from the model, the following three outcomes may be possible: (a) a fault in the given SUT (b) model violation despite normal operation (c) illegal input which the program is unable to handle.

The testing process is accomplished by Eclat in three stages as shown in Figure~\ref{fig:eclat} . In the first stage, a small subset of test inputs is selected, which may likely reveal faults in the given SUT. In the second stage, reducer function is used to discard any redundant input, leaving only a single input per operational pattern. In the third stage, the acquired test inputs are converted into test cases and oracles are created to determine the success or failure of the test.
\bigskip
\begin{figure}[h]
	\centering
	\includegraphics[width=14.5cm, height=10cm]{chapter2/eclat_working.png}
	\bigskip
	\caption{The input selection technique. Implicit in the diagram is the program under test. Rectangles with rounded corners represent steps in the technique, and rectangles with square corners represent artifacts~\cite{pacheco2005eclat}}
	\label{fig:eclat}
\end{figure}
\bigskip

Csallner and Smaragdakis~\cite{csallner2004jcrasher} compared Eclat with JCrasher by executing nine programs on  both tools. They reported that Eclat performed better than JCrasher. On the average, Eclat selected 5.0 inputs per run out of which 30\% revealed faults while JCrasher selected 1.13 inputs per run out of which 0.92\% revealed faults. The limitation of Eclat is dependence on initial pool of correct test cases. Any error in the pool may lead to the creation of wrong operational model which will adversely affects the testing process.   

%\subsection{JTest}
%Parasoft Jtest is a commercial tool that automatically generates and execute unit tests. It can be easily integrated to Java IDEs like Eclipse where it provide two main functionalities, i.e. Static Analysis, Unit testing and code coverage. [25]
%In static analysis Jtest takes a complete project or set of classes as input and compares it with a list of built-in rules. The statement violating any of these rules is an error. It also suggests probable fixes for the detected fault.
%For unit testing it takes a class as an input and processes a number of scenarios against it to generate and execute unit tests. Once unit tests are executed they become the part of regression test for future reference.
%Jtest also shows the code coverage of the program by colour coding the statements that are not executed by the unit tests.


\subsection{Randoop}
Random tester for object oriented programs (Randoop) is the tool used for implementing FDRT technique~\cite{pacheco2007randoop}. Randoop is a fully automatic tool, capable of testing Java classes and .Net binaries. It takes a set of classes, contracts, filters and time limit as input and gives a suite of JUnit for Java and NUnit for .Net program as output. Each unit test in a test suite is a sequence of method calls (hereafter referred as sequence). Randoop builds the sequence incrementally by randomly selecting a public method from the class under test.  Arguments for these methods are selected from the predefined pool in case of primitive type and as sequence of null values in case of reference type. Randoop maintains two sets called $ErrorSeqs$ and $NonErrorSeqs$ to record the feedback. It extends $ErrorSeqs$ set in case of contract or filter violation and $NonErrorSeqs$ set when no violation is recorded in the feedback. The use of this dynamic feedback evaluation at runtime brings an object to an interesting state. On test completion, $ErrorSeqs$ and $NonErrorSeqs$ are produced as JUnit or NUnit test suite. The following command runs Randoop to test OneDimPointFailDomain for 100 seconds in CLI mode. Values is a text file containing interesting values which is maintained manually by the tester.
\bigskip

\begin{lstlisting}[language=bash]
$ java randoop.main.Main gentests   \
 --testclass=OneDimPointFailDomain   \
 --testclass=Values --timelimit=100
\end{lstlisting}
\bigskip

In terms of coverage and number of faults discovered, Randoop using FDRT technique was compared with JCrasher and JavaPathFinder and 14 libraries of both Java and .Net were evaluated~\cite{visser2004test}. The results showed that Randoop achieved more branch coverage and better fault detection than JCrasher. 

\subsection{QuickCheck}
QuickCheck~\cite{claessen2011quickcheck} is a lightweight random testing tool used for testing of Haskell programs~\cite{hudak2007history}. Haskell is a functional programming language where programs are evaluated by using expressions rather than statements. Most of the functions in Haskell are pure except the IO functions, therefore QuickCheck mainly focuses on testing pure functions. QuickCheck is designed to have a simple domain-specific language of testable specifications embedded in Haskell. This language is used to define expected properties of the functions under test. %- for example, reversing a list with single element must result in the same list.\\ %(author check the definition of pure functions)\\

QuickCheck takes as inputs the function to be tested and properties of the program (Haskell functions). The tool uses built-in random generator to generate test data, but it is also capable to use custom built data generator. The tester-defined-properties must hold while executing the function on the generated test data. Any violation of the defined properties will indicate error in the function.



% The function is executed against the generated test data. The QuickCheck evaluates and declares a fault in the function where a test case violates the set properties.   



%\subsection{AgitarOne}
%AgitarOne is a commercial tool that automatically generates unit tests. It has a Junit Generator engine that can create 25,000 lines or more of Junit per hour [29]. It can be easily integrated into famous IDE like Eclipse. It takes as input, classes under test, time and optionally any knowledge or test cases that has a positive influence on the performance of the testing process. The generated Junit tests can be run from the same IDE and can also be used for later regression testing. The GUI interface is called a dashboard which provides in depth knowledge of the tests conducted, failures detected, alerts and the archieves of the tests conducted earlier. It also shows the coverage obtained after executing the Junits against the code under test.

\subsection{AutoTest} \label{autotest_2}
The AutoTest is used to test Eiffel language programs~\cite{ciupa2007automatic}. The Eiffel language uses the concept of contracts which is effectively utilized by AutoTest. For example, the auto generated inputs are filtered using pre-conditions and non-complying test inputs are discarded. The post-conditions are used as test oracle to determine whether the test passes or fails. Besides automated testing the AutoTest also allows the tester to manually write the test cases to target specific section of the code. The AutoTest takes one or more methods or classes as inputs and automatically generates test input data according to the requirements of the methods or classes.
\bigskip
\bigskip
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm, height=8cm]{chapter2/autotest.png}
	\bigskip
	\caption{AutoTest architecture~\cite{leitner2007reconciling}}
	\label{fig:autotest}
\end{figure}
\bigskip
\noindent As shown in Figure~\ref{fig:autotest}, the architecture of AutoTest can be split into the following four parts:
\begin{enumerate}
\item \textbf{Strategy:} It is a pluggable component where testers can fit any strategy according to the testing requirement. The strategy contains the directions for testing.%- for example what instructions should be executed on the SUT. Using the information the strategy synthesize test cases and forward it to the proxy. 
The default strategy creates random data to evaluate the methods/classes under test.
\item \textbf{Proxy:} It handles inter-process communication. The proxy receives execution requests from the strategy and forward these to the interpreter. It also sends the execution results to the oracle part.
\item \textbf{Interpreter:} It executes operations on the SUT. The most common operations include: create object, invoke routine and assign result. The interpreter is kept separate to increase robustness.
\item \textbf{Oracle:} It is based on contract-based testing. It evaluates the results to see if the contracts are satisfied. The outcome of the tests are formatted in HTML and stored on disk.
\end{enumerate}

\subsection{TestEra}
TestEra~\cite{marinov2001testera} is a novel framework of auto generation and evaluation of test inputs for a Java program. Input of the tool includes specifications, numerical value and method to be tested. It uses pre-conditions of the method to generate all non isomorphic valid test inputs in the specified limit. The test inputs are executed and the results obtained are compared against post-conditions of the method serving as oracle. A test case that fails to satisfy post-conditions is considered as fault. 
\bigskip
\begin{figure}[h]
	\centering
	\centerline{\includegraphics[width=15cm, height=10.9cm]{chapter2/TestEra.png}}
	\bigskip
	\caption{TestEra framework~\cite{marinov2001testera}}
	\label{fig:testera}
\end{figure}
\bigskip
TestEra uses the Alloy modelling language~\cite{jackson2001micromodularity} to express constraints of test inputs from program specifications. The constraints are solved by the Alloy Analyser~\cite{jackson2000alcoa} which performs the following three functions: (a) it translates Alloy predicates into propositional formulas%, i.e. constraints where all variables are boolean 
 (b) it evaluates the propositional formulas to find the outcome (c) it translates each outcome from propositional domain into the relational domain.




TestEra uses program specifications to guide the auto generation of test inputs as against Jartege (Section \ref{jartege_2}) and AutoTest (Section \ref{autotest_2}) which use program specifications for filtrating the irrelevant random generated test data. However, all the three tools use program specifications in a similar way for test oracle. 


%Testera use specifications to guide the automatic generation of test inputs. It uses Alloy language for specification and Alloy Analyser to generate all non-isomorphic instances for a given size according to the specification automatically. Then, TestEra translate the instances to Java input as test cases for the program under test. After executing the test, TestEra then translate the outputs back to Alloy and Alloy Analyzer check the input and output against the correctness criteria given in Alloy. When it detects a violation, TestEra generates report in the form of concrete counterexamples. Figure 2.6 [50] illustrates the basic framework of TestEra.

\subsection{Korat} % please read thesis of khurshid in Mendeley in phd thesis section for more information.
Korat~\cite{boyapati2002korat} is a framework for automated testing of Java programs based on formal specifications~\cite{chang1999structural}. %Korat and TestEra~\cite{marinov2001testera} are developed by the same team. 
Korat uses Java Modelling Language (JML) for specifications. It uses bounded-exhaustive testing in which the code is tested against all possible inputs within the specified bounds~\cite{khurshid2001checking}. %Korat generates structurally complex inputs by solving imperative predicates. An imperative predicate is a piece of code that takes a structure as input and evaluates it to a boolean value. 
Input to the tool includes imperative predicates and finitization value. Korat systematically explores the input space of the predicates and generates all non-isomorphic inputs for which the predicates return true. The core part of Korat monitors execution of the predicates on candidate inputs to filter out the fields accessed during executions. These inputs are taken as test cases. 

Korat uses \verb+repOK()+ and \verb+checkRep()+ methods. The \verb+repOK()+ is used to check the class invariants for validating test inputs while \verb+checkRep()+ is used to verify the post-conditions for validating the test case. Korat does not require existing set of operations to create input values. Therefore it has the advantage of generating input values that may be difficult or impossible with a given set of operations. The disadvantage of the approach is the requirement of significant manual efforts \cite{pacheco2009directed}.    

%As the test start, it uses methods pre-condition to generate all non-isomorphic test cases up to a given size. It then executes each of the test case and compare the obtained results to the methods post-condition, which serves as an oracle to evaluate the correctness of each test case. 

\subsection{YETI}
York Extensible Testing Infrastructure (YETI) is an open-source automated random testing tool. YETI, coded in Java, is capable of testing systems developed in procedural, functional and object-oriented languages. Its language agnostic meta-model enables it to test programs written in multiple languages including Java, C\#, JML and .NET. The core features of YETI include easy extensibility for future growth, capability to test programs using multiple strategies, high speed tests execution, real time logging, GUI support and auto generation of test report at the end of test session. Detailed information about YETI is presented in Chapter \ref{chap:yeti_3}.




\begin{sidewaysfigure}[htp]
\centering
\centerline{\includegraphics[width=26cm, height=15cm]{chapter2/RandomTools01.png}}
	\caption{Main features of automatic testing tools using random testing}
	\label{fig:Tools}
\end{sidewaysfigure}


%\section{Automated Random Testing}
%\subsection{Test Data Generation}
%\subsection{Test Execution}



%\subsection{Test Report}

\section{Summary}
The software testing is summarized graphically with the help of two dimensional venn diagram as shown in Figure~\ref{fig:testDataGenerators_2}. 

\bigskip
\begin{figure}[h]
	\centering
	\centerline{\includegraphics[width=16cm, height=12cm ]{chapter2/DrawingTesting.png}}
	\bigskip
	\caption{Types of software testing}
	\label{fig:testDataGenerators_2}
\end{figure}
\bigskip

The positive x-axis represent black-box while negative x-axis represent white-box testing. Similarly on positive y axis we have dynamic testing and on negative y axis we have static testing. If a testing technique is black-box and dynamic then it will fall in 0 to 90 degree and if it is black-box and static it will fall in 270 to 360 degree. On the other hand if the test is white-box and dynamic then it will fall in 90 to 180 degree and if the test is white-box and static then it will fall in 180 to 270 degrees.

The chapter gives an overview of software testing, including definition, common types, need, purpose and uses. It differentiates manual and automated software testing and describes various ways of software test data generation, being the crucial part of any testing system. The later part describes random testing and the various ways of improving the performance of random testing. Finally, information is presented on how the automated testing tools implement random technique for software testing. Main features of automatic testing tools used in random testing are summarized in Figure~\ref{fig:Tools}.

%\begin{figure}[H]
%	\centering
%	%\centerline{\includegraphics[width=16cm, height=16cm]{chapter2/softwareTestingFlow.png}}
%	%\centerline{\includegraphics[width=15.5cm, height=15cm]{chapter2/Tools.png}}
%	\centerline{\includegraphics[width=17.5cm, height=15cm]{chapter2/RandomTools01.png}}
%	\caption{Main features of automatic testing tools using random testing}
%	\label{fig:Tools}
%\end{figure}




%\begin{figure}[H]
%	\centering
%	%\centerline{\includegraphics[width=16cm, height=16cm]{chapter2/softwareTestingFlow.png}}
%	\centerline{\includegraphics[width=15.5cm, height=15cm]{chapter2/Tools.png}}
%	\caption{Main features of automatic testing tools used in random testing}
%	\label{fig:Tools}
%\end{figure}





%\begin{sidewaystable}
%    \centering
%    \caption{Main features of automatic testing tools used in random testing}
%    \bigskip
%   \begin{tabular}{|l|l|l|l|l|l|}
%\hline

%Tool 				& Language																								& Input  																																			& Strategy 																																											 				& Output		  																								& Benefits																															\\ \hline
%JCrasher	  & Java, JML																								& Program																																			& \vtop{\hbox{\strut Method type to predict input,}\hbox{\strut Randomly find values of crash}}  				& TC																													& \vtop{\hbox{\strut Automated TC, Use} \hbox{of Heuristic Rules}} 	 \\ \hline
%Jartege			& Java																										& Classes																																			& \vtop{\hbox{\strut Random strategy with controls}\hbox{\strut like weight etc.}} 							 				& TC, RT 																											& Quick, automated																									 \\ \hline
%Eclat				& Java																										& Classes, pass TC 																														& \vtop{\hbox{\strut Create model from TC, execute}\hbox{\strut each candidate on the model}} 					& Faulty TC 																									& \vtop{\hbox{\strut produce output text,} \hbox{JML}}									\\ \hline
%Quickcheck	& Haskell																									&	\vtop{\hbox{\strut Specifications}  \hbox{\strut and Functions}}	  			  & \vtop{\hbox{\strut Specification} \hbox{\strut hold to random TC?}} 											 						& Pass/Fail																										& \vtop{\hbox{\strut Easy to use, program} \hbox{documentation}}				\\ \hline
%Randoop 		& Java, .NET																							& \vtop{\hbox{\strut Specifications,} \hbox{\strut code and time}}					  & \vtop{\hbox{\strut Generate and execute methods} \hbox{\strut \& give feedback for next generation}} 	& Fault TC, RT 																								& 																																\\ \hline
%AgitarOne		& Java																										& \vtop{\hbox{\strut Package, time}   \hbox{\strut and manual TC}}						& \vtop{\hbox{\strut Analyse SUT with auto and} \hbox{\strut provided data in specified time}} 					& TC, RT																											& \vtop{\hbox{\strut Eclipse plug-in} \hbox{\& easy to use}}  			 \\ \hline
%AutoTest		& Java																										& \vtop{\hbox{\strut Classes, time}   \hbox{\strut and manual TC}} 						& \vtop{\hbox{\strut Heuristic rules} \hbox{\strut to evaluate contracts}} 															& violations, RT 																							& \vtop{\hbox{\strut GUI in HTML,} \hbox{easy to use}} 								\\ \hline
%TestEra			& Java																										& \vtop{\hbox{\strut Specifications,} \hbox{\strut integer \& manual TC}}			& \vtop{\hbox{\strut Check contracts} \hbox{\strut with specifications}} 																& Contracts violations 																				& \vtop{\hbox{\strut short report with} \hbox{faulty TC only}} 					\\ \hline
%Korat 			& Java																										& \vtop{\hbox{\strut Specifications}  \hbox{\strut and manual tests}}					& \vtop{\hbox{\strut Check contracts} \hbox{\strut with specifications}}																& Contracts violations 																				& \vtop{\hbox{\strut GUI, short report} \hbox{with faulty TC only}} 	 \\ \hline
%YETI 				& \vtop{\hbox{\strut Java, .NET,}  \hbox{\strut JML}} 		& Code, Time 																																  & RandomPlus, Random 																																										& \vtop{\hbox{\strut Traces of found } \hbox{\strut faults}}	& \vtop{\hbox{\strut GUI, give faulty} \hbox{examples, Quick}} 			 \\ \hline %inserts single line
%\end{tabular}
%\label{table:Tools}
%\end{sidewaystable}

% ------------------------------------------------------------------------


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"

%%% End:
