\chapter{Literature Review: Software Testing}
\label{chap:softwareTesting}
The very famous quote of Paul, ``to err is human, but to really foul things up you need a computer", is quite relevant to the software programmers. Programmers being humans are prone to errors. Therefore, in spite of best efforts, some errors may remain in the software after it is finalised.  Errors cannot be tolerated in software because a single error may cause a large upset in the system. The destruction of Mariner 1 rocket (1962) costing \$18.5 million was due to a small error in formula coded incorrectly by programmer. The Hartford Coliseum Collapse (1978) costing \$70 million, Wall Street crash (1987) costing \$500 billion, failing of long division by Pentium (1993) costing \$475 million, Ariane 5 Rocket disaster costing \$500 million were caused by minor errors in the software \cite{toweysoftware}. To achieve high quality, a software has to satisfy rigorous stages of testing. The more complex the software, the higher the requirements for software testing and the larger the damage caused when a bug remains in the software.

\begin{figure}[h]
	\centering
	\includegraphics[width=15.5cm, height=4cm]{chapter2/softwareTesting.png}
	\caption{The process of software testing}
	\label{fig:softwareTesting}
\end{figure}

In the IEEE standard glossary of software engineering terminology~\cite{american1984}, testing is defined as ``the process of exercising or evaluating a system or system component by manual or automated means to verify that it satisfies the specified requirements and results". The process of software testing in its simplest form is shown in Figure~\ref{fig:softwareTesting}. 

The testing process, being an integral part of Software Development Life Cycle (SDLC), is started from requirement phase and continues throughout the life of the software. In traditional testing, when testers find fault in the given software, it is returned to the developers for rectification and consequently given back to the testers for retesting. It is important to note that a successful test is the one that fails a software or identifies fault in the software~\cite{Myers1979}. Fault denotes error made by programmers during software development~\cite{american1984}. A software that passes all the tests without giving a single error is not guaranteed to contain no error. The testing process, however, increases reliability and confidence of users in the tested product. \\


\begin{table}[ht]
%\scriptsize
\caption{Parts of Software Testing} % title of Table
\bigskip
\centering % used for centering table
{\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
\begin{tabular}{| l | l | l | l | } % centered columns (4 columns)
\hline

Level 					&Purpose		 		& Perspective							& Form 	\\
\hline
Unit						& Functional			& White Box							& Static 		\\
Integration				& Structural			& ~~~~~a. Data Flow Analysis				& Dynamic	\\
System					& Robustness			& ~~~~~b. Control Flow Analysis			&			\\
						& Stress				& ~~~~~c. Code-based fault injection &			\\
						& Compatibility			& Black Box							&			\\
						& Performance			& ~~~~~a. Use-case testing				&			\\
						&					& ~~~~~b. Partition testing				&			\\
						&					& ~~~~~c. Boundary Value testing			&			\\
						&					& ~~~~~d. Formal Specification testing		&			\\



\hline %inserts single line
\end{tabular}
}
\bigskip
\label{table:softwareTestingParts} % is used to refer this table in the text
\end{table}

\section{Definitions}
This section presents important definitions:

\subsection{Test Plan}
Test plan is a document which defines the goal, scope, method, resources and time schedule of testing \cite{futrell2001quality}. In addition, it includes the testable deliverables and the associated risk assessment. The test plan explains {\it {who, when, why}} and {\it {how}} to perform a specific activity in the testing process. 

\subsection{Input Domain} 
The input domain comprises of all possible inputs for a software, including all the global variables, method arguments and the externally assigned variables. For a given program P with input vector $ P =\{x1, x2, . . . , xn\}$, having $\{D1, D2, . . . , Dn\}$ as the domain of each input so that $x1 \in D1, x2 \in D2$ and so on, the domain D of a function is the cross product of the domains of each input: $D = D1 \times D2 \times . . . \times Dn$.

\subsection{Test Case}
%\begin{wrapfigure}{r}{0.35\textwidth}
%  \vspace{-20pt}
%  \begin{center}
%    \includegraphics[width=0.30\textwidth]{chapter2/testCase.png}
%  \end{center}
%  \vspace{-20pt}
%  \caption{Test case}
%  \label{fig:testCase}
%  \vspace{-10pt}
%\end{wrapfigure}
A test case is an artifact which delineates the input, action and expected output corresponding to that input \cite{ahmed2010software}. After executing the test case, if the output obtained comply with the expected output, the test case is declared pass which means that the functionality is working correctly, otherwise the test case is declared fail, which represents identification of fault. A series of test cases, also known as test suite, are usually required to be executed for establishing the desired level of quality.

\section{Software Testing Levels}
Unit testing, Integration testing and System testing are the three main levels of software testing reported in the literature~\cite{chilenski1994applicability}. Unit testing deals with evaluation of code piece-by-piece and each piece is considered as independent unit. Units are combined together to form components. Integration testing is performed to make sure that integration of units in a component is working properly. Finally, system testing ensures that the system formed by the combination of components proceeds properly to give the required output.

\section{Software Testing Purpose}
The primary purpose of software testing is identification of faults in the given SUT for necessary correction in order to achieve high quality. Maximum number of faults can be identified if software is tested exhaustively. In exhausting testing SUT is checked against all possible combinations of input data and for assessment the results obtained are compared with the results expected. Exhaustive testing is not always possible in most scenarios because of limited resources and infinite number of input values that software can take. Therefore, the purpose of testing is generally directed to achieve confidence in the system involved from a specific point of view. For example, functionality testing is performed to check that functional aspect are working correctly. Structural testing analyses the code structure for generating test cases in order to evaluate paths of execution and identification of unreachable or dead code. In robustness testing the software behaviour is observed in the case when software receives input outside the expected input range. Stress and performance testing aims at testing the response of software under high load and checking its ability to process different nature of tasks~\cite{cohen2005robustness}. Finally, compatibility testing is performed to see the interaction of software with the underlying operating system.
 %As proper planning is the key to success for many projects this is often also true with software testing. A software test plan is a well defined document that defines the goal, scope, method, resources and time schedule of the testing.
%A software testing technique in which a software is tested with all possible combination of inputs. This technique can prove conclusively that the software meet its specification however exhaustive testing is seldom feasible because of the large input domain or too many paths in a software code. 

\section{Software Testing Perspective}
Software testing is divided into white-box and black-box testing based on the perspective taken.

\subsection{White-box testing}
In white-box or structural testing, the testers must know about the complete structure of the software so that they may make necessary modifications, if so required. Test cases are derived from the code structure and test passes only if the results are correct and the proper code is followed during test execution~\cite{ostrand2002white}. Some commonly used white-box testing techniques are as follows:
\begin{figure}[h]
\begin{center}
	\includegraphics[width=5cm, height=6cm ]{chapter2/whiteBox.png}
	\caption{White-box testing}
	\label{fig:blackBox}
\end{center}  
\end{figure}
%\begin{wrapfigure}{r}{0.36\textwidth}
%  \vspace{-35pt}
%  \begin{center}
%   \includegraphics[width=0.30\textwidth]{chapter2/whiteBox.png}
%  \end{center}
%  \vspace{-20pt}
%  \bigskip
%  \caption{White-box testing}
%  
% \vspace{-18pt}
%\end{wrapfigure}

\subsubsection{Data Flow Analysis}
Data Flow Analysis (DFA) is a testing technique which focuses on the input values by observing the behaviour of respective variables during execution of the SUT~\cite{clarke1989formal}. In this technique a Control Flow Graph (CFG), graphically representing all possible states of a program, is drawn to determine the paths that are traversed by the program during test execution. Test cases are generated and executed to verify conformance with CFG. 

The process of program execution can be looked into as data-flow from input to output where data may transform into several intermediary steps before reaching the final state. The process is prone to several errors e.g. references made to non existing variables, values assigned to undeclared variables or change of variables in undesired manner. Ordered use of data is crucial to ensure that the aforementioned errors do not occur~\cite{fosdick1976data}.

\subsubsection{Control Flow Analysis}
Control Flow Analysis (CFA) is a testing technique which takes into consideration the control structure of a given SUT. Control structure is the order in which the statements, instructions or function calls are executed. In this technique a CFG, similar to the one required in DFA, is drawn to determine the traversable paths by a program during the execution. Test cases are generated and executed to verify conformance with CFG on the basis of control. Taking the example of following a specific path between two or more available choices at a particular state: efforts are made to ensure that, at least once, the set of selected test cases execute all the possible control choices. The effectiveness of the testing technique depends on measurement of control. Two of the most common measurement criteria defined by Vilkomir et al. are Branch coverage and Condition coverage~\cite{vilkomir2003tolerance}. 

\subsubsection{Code-based fault injection testing}
It is a testing technique in which new instructions are added to the code of the SUT at one or more locations to analyse the software behaviour in response to the instructions. \cite{voas1997software}. The process of code addition (instrumentation) is performed before compilation and execution of software. Code is added to: find error handling behaviour of software, examine the capability of test procedure and measure the code coverage achieved by the testing process.    

\subsection{Black-box testing}
In black-box or functional testing, the testers do not need to know about internal code structure of the SUT. Test cases are derived from the software specifications and test passes if the result is according to expected output~\cite{beizer1995black}. Some commonly used black-box testing techniques are stated below:
\begin{figure}[h]
\begin{center}
	\includegraphics[width=5cm, height=6cm ]{chapter2/blackBox.png}
	\caption{Black-box testing}
 	\label{fig:blackBox}
\end{center}  
\end{figure}
%\begin{wrapfigure}{r}{0.35\textwidth}
%  \vspace{-35pt}
%  \begin{center}
%    \includegraphics[width=0.30\textwidth]{chapter2/blackBox.png}
%  \end{center}
%  \vspace{-20pt}
%  \bigskip
%  \caption{Black-box testing}
%  \label{fig:blackBox}
%  \vspace{-18pt}
% \end{wrapfigure}
\subsubsection{Use-case based testing}
% check if it is use case-based testing or use case testing.
It is a testing technique which utilizes use-cases of the system to generate test cases. Use-case defines functional requirement at a particular point in the system from actor's perspective. It consists of a sequence of actions to represent a particular behaviour of the system. A use-case format includes brief description and flow of events, pre-conditions, post-conditions, extension points, context and activity diagrams. The use-case contains all the information required for test case, therefore, it can be easily transformed into a test case. Use-case testing is beneficial in terms of cheap generation of test cases, avoidance of test duplication, increased test coverage, easier regression testing and early identification of missing requirements.  

% steps taken from presentation of Raional User Conference 2003. Check it for viva.
\subsubsection{Partition Testing}
It is a testing technique in which the input domain of a given SUT is divided into sub-domains for testing each sub-domain individually. The division is based on software specifications, structure of the code and the process involved in software development~\cite{hamlet1990}. The performance of partition testing is directly dependant on the quality of sub-domain~\cite{weyuker1991analyzing}. However, division of input domain into equal partitions is often difficult. To overcome the problem, a new version of partition testing, called Proportional sampling strategy~\cite{Chan1996} is devised. In this version, the sub-domains vary in size and the number of test cases selected from each partition is directly proportional to the size of the partition. Experiments performed by Ntafos~\cite{ntafos1998random} has provided evidence for better performance of Proportional partition testing.


\subsubsection{Boundary Value Analysis}
Boundary Value Analysis (BVA) is a testing technique based on the assumption that errors often reside along the boundaries of the input variables. Thus border values are taken as the test data set in BVA. According to IEEE standards~\cite{radatz1990ieee}, boundary values contain minimum, maximum, internal and external values specified for a system. 

BVA  and partition testing may be used in combination by choosing test values from the whole input domain and also from the borders of each sub-domain. Reid et al. \cite{reid1997empirical} have provided evidence in support of better performance of BVA compared to partition testing. However, they have indicated that better performance of BVA is based on accurate identification of partition and selection of boundary values.

 The following code illustrates the ability of BVA to find a bug.  On passing interesting value \verb+MAX_INT+ as argument to the $test$ method, the code in the method increment it by 1 making it a negative value and thus an error is generated when the system try to build an array of negative size.

\begin{lstlisting}
~public void test (int arg) {
~~~~~arg = arg + 1;
~~~~~int [] intArray = new intArray[arg];
~~~~~...
~}
\end{lstlisting}



\subsubsection{Formal Specification Testing}
It is a testing technique based on mathematical model which provides the opportunity to handle the specifications mechanically. This feature facilitates the isolation, transformation, assembly and repackaging of the information available in the specifications for use as test cases~\cite{donat1997automating}.

The formal specification testing is more productive because of the creation of test cases independent from the code of the SUT~\cite{gaudel2010software}. The extra effort of generating test oracle is avoided because of using the available specification model for verifying the test results~\cite{bertolino2007software}.
  

%\section{Common Techniques of Software Testing}
%This section briefly define some of the most common techniques of software testing currently being used in the testing industry. These include techniques from both white-box and black-box testing techniques.

% Check wikipedia for them.


%\subsubsection{Grey-Box Testing}
%Grey-Box testing is the combination of both black-box/functionality and white-box/structural testing. The tester knows about both the functionality and the internal structure of the SUT. Some of the test cases are based on the functionality and some of the test cases are based on the structure. Emphasis of grey-box testing is both on code coverage as well as functionality~\cite{Savenkov2008}.

%\subsection{Software Testing Workflow}
%There are many software techniques like unit testing, integration testing, random testing, regression testing, system testing, acceptance testing, performance testing, load testing, stress testing, alpha testing, beta test etc. All testing techniques belong to black-box, white-box or grey-box approach. Each testing technique has its own strength and weaknesses but the technique in focus here is Random Testing.


%\begin{figure}[h]
%\begin{center}
%	\includegraphics[width=16cm, height=12cm ]{Literature/Drawing34.jpg}
%	\caption{Software Testing Workflow}
%\end{center}  
%\end{figure}


%We have explained software testing graphically with the help of plotting venn diagram on two dimensional axis. The positive x axis represent black-box while negative x axis represent white-box testing. Grey-box testing in the middle is represented by the overlapping of black-box and white-box testing. Similarly on positive y axis we have dynamic testing and on negative y axis we have static testing.
%Now if a test is black box and dynamic then the test will fall in 0 to 90 degree on the diagram and if the test is black-box and static then it will fall in 270 to 360 degree. On the other hand if the test is white-box and dynamic then it will fall in 90 to 180 degree and if the test is white-box and static then it will fall in 180 to 270 degrees.

%\subsection{Automated Test Generation}
%\subsection{Generation Strategies}

\subsection{Test Oracle}
Test oracle is defined as, ``a source containing expected results for comparison with the actual result of the SUT" \cite{ahmed2010software}. For a program P, an oracle is a function which verifies that the output from P is the same as the output from a ‘correct’ version of P~\cite{howden1986}. Test oracle set the acceptable behaviour for test execution~\cite{baresi2001test}. 

All software testing techniques depend on the availability of test oracle~\cite{gaudel2010software}. Designing test oracle for ordinary software may be simple and straightforward. However, for relatively complex software, designing of oracle is quite cumbersome and requires special ways to overcome the oracle problem. Some of the common oracle problems are as follows:
\begin{enumerate}
\item It is assumed that the test results are observable and comparable with the oracle.
\item Ideally, test oracle would satisfy desirable properties of program specifications~\cite{baresi2001test}.
\item A test oracle to satisfy all conditions is seldom available as rightly pointed out by Weyuker, ``truly general test oracles are often unobtainable''~\cite{weyuker1982testing}. 
\end{enumerate}

Some common artefacts used as oracles are as follows.
\begin{enumerate}
\item Specification and documentation to generate test oracle. 
\item Products similar to the SUT but different in algorithm. %to solve the similar problem.
\item Heuristic algorithms to provide exact results for a set of test cases. % Hoffman, Douglas; Heuristic Test Oracles, Software Testing & Quality Engineering Magazine, 1999
\item Statistical characteristics to generate test oracle. % \cite{mayer2004test}. 
\item Comparison of the result of one test to another for consistency. % Hoffman, Douglas; Analysis of a Taxonomy for Test Oracles, Quality Week, 1998
\item Models to generate test oracle for verification of SUT behaviour. % \cite{robinson1999finite}.
\item Manual analysis by human experts to verify the test results. %\cite{jalote1997integrated}. 
\end{enumerate}

\section{Forms of Software Testing}
There are two forms of software testing: static and dynamic. In static testing, test cases are analysed statically for checking errors without test execution. In addition to software code, high quality software is supplied with documentation including requirements, design, user manual, technical notes and marketing information. Reviews, walkthroughs and inspections are most commonly used techniques for static testing. In dynamic testing the software code is executed and input is converted into output. Results are analysed against expected outputs to find any error in the software. Unit testing, integration testing, system testing, and acceptance testing are most commonly used dynamic testing methods~\cite{fairley1978tutorial}.

%Dynamic testing can be manual or automated. In manual testing the programmer develops the test cases which are executed by the developed software to find any error in processing or output. Similarly in automated testing the software or components of the software is given as input to testing software that automatically generates test cases and executes the SUT against them to find any errors. Manual testing typically consumes more time and resources than automated testing.



\subsection{Manual Software Testing}
Manual testing is the technique of finding faults in software in which the tester writes the code manually to create test cases and test oracles~\cite{Ciupa2008}. Manual testing may be effective at smaller scale but it is generally laborious, time consuming and error-prone~\cite{tretmans1999}. Additionally, testers must have appropriate skills, experience and knowledge of the SUT for evaluation from different perspectives.
 
\subsection{Automated Software Testing}
Automated testing is the technique of finding faults in a software in which a testing tool is used to perform the testing process automatically~\cite{Leitner2007}. There are tools for automating part of a testing process e.g. for generation of test cases or execution of test cases or evaluation of results. Other tools are available for automating the whole testing process. Increase in functionality, higher productivity and lower cost of production without compromising quality are the desirable features in favour of automating the process of software testing. Automated software testing may involve higher initial cost but brings the key benefits of lower cost of production, higher productivity, maximum availability, greater reliability, better performance and ultimately proves highly beneficial for any organisation. Automated testing is particularly effective when the nature of job is repetitive and performed on routine basis like unit testing and regression testing where the tests are re-executed after each modification~\cite{huang2003automated}. The use of automated software testing makes it possible to test large volumes of code, which may be impossible otherwise~\cite{ramamoorthy1975testing}.

\section{Test Data Generation}
Test data generation in software testing is the process of identifying test input data which satisfies the given test selection criterion. A test data generator is used to assist testers in the generation of test data while the test selection criterion defines the properties of test cases to be generated based on the test plan and perspective taken \cite{korel1990}. Various artefacts of the SUT can be considered to generate test data like requirements, model, code etc. The choice of artefacts selected limits the kind of test selection criteria that can be applied in guiding the test case generation. 

A typical test data generator consists of three parts: Program analyser, Strategy handler and Generator~\cite{edvardsson1999survey}. Program analyser performs initial assessment of software prior to testing and may alter the code if so required. For example it performs code instrumentation or construction of CFG to measure the code coverage during testing. A Strategy handler defines the test case selection criteria. This may include the formalisation of test coverage criterion, the selection of paths, normalisation of constraints, etc. It may also get input from program analyser or user before or during execution. The generator taking inputs from the program analyser and strategy handler generates test cases according to the set selection criteria. Test data generators, based on their approaches, are classified into path-wise, goal-oriented, intelligent and random. Each type is briefly described in the following section.

\subsection{Path-wise Test Data Generator}
It is a technique in which the test data is generated to target path, statement and branch coverage in a given SUT. The approach generally consists of three main parts: CFG construction, path selection and test data generation. 

In path-wise test data generation, the program path to the selected statement is identified and the input data are generated for evaluating the path which can be either generated automatically or provided by the user. The data generated in path testing expresses boolean behaviour i.e. true or false for a particular node in a path. 

A complete path contains multiple sub-domains, each sub-domain consists of test inputs required to traverse the path. The boundary of the sub-domains are obtained by the predicates in the path condition. The test data traversing a certain path in the software are selected from an input space split into a set of sub-sections. 

% the pathwise test data generation is taken from a book knowldge mining using intelligent systems. you can reference it too.


\subsection{Goal-oriented Test Data Generators}
It is a technique in which the test data is generated to target a specific program point rather than a program path \cite{chungautomated}. The tester can select any path among a set of existing paths as long as it reaches to the specified program point. This technique utilizes runtime information for computing accurate test data~\cite{ferguson1996chaining}. Among various methods used in goal-oriented test data generation the following two commonly adopted approaches are briefly described.

\subsubsection{Chaining Approach}
The chaining approach uses data dependent analysis to guide the test data generation. In the process all the related statement are selected automatically by the technique that are affected by the execution of the selected statement under test. The dependant statements are executed before the selected statement to generate the required necessary data for the execution of the statement under test~\cite{ferguson1996chaining}.

The chaining approach analyses the program according to the edges and nodes. For each test coverage criterion different initial event sequence and goal nodes are determined. For example, consider  the branch (p, q), where p is  the starting node of the branch and q is the last node in the branch. The initial event sequence E for the branch (p, q) is defined as $E =< (s,\phi), (p,\phi),(q,\phi) >$, provided that s is the starting node of the program and $\phi$ is the set of variables referred to as a constraint. The the Branch Classification process identifies critical, semi-critical and non-critical nodes for each branch. During the execution of the program, this classification leads the search to decide which branch to take to reach the goal node or to cover the specified branch.  


\subsubsection{Assertion-oriented Approach}
In this approach assertions are added to the program code with the goal to identify program input on which an assertion is violated, indicating a fault in the SUT. An assertion specifies a constraint that applies to some state of a computation which evaluates to either true or false. For example, consider a given assertion A, now find program input x on which assertion A is false, i.e. when the program is executed on input x and the execution reaches assertion A. It is evaluated as false indicating a fault in the SUT.

It is not always possible to generate test cases that violate assertions. However, experiments have shown that assertion-oriented test data generation may frequently detect errors in the program related to assertion violation. The major advantage of this approach is that each generated test data uncovers an error in the program with violation of an assertion. An assertion is violated because of three reasons: a faulty, a faulty assertion and a faulty precondition.

% check the korel1996assertion for the above whole text and assertion oriented approach + the model hard copy thesis.


\subsection{Intelligent Test Data Generators}
 Intelligent test data generation is a technique used to overcome the problems associated with traditional data generation techniques like generation of meaningless data, duplicated data and failing to generate complex test data. The approach increases users confidence in the generated test data and the testing process~\cite{ramamoorthy1975testing}. It performs sophisticated analysis, such as fuzzy logic, neural networks and genetic algorithms on the SUT to assist in finding the appropriate test data. It involves complex analysis to anticipate different situations that may arise at any point. The approach produces test data which satisfy the SUT requirements, however, it consumes more time and resources.

\subsubsection{Genetic Algorithm}
Genetic algorithm is a heuristic that mimics the evolution of natural species in searching for the optimal solution of a problem. The solution sought by the genetic algorithm is the test data that causes execution of a given statement, branch, path and condition in the SUT. The genetic algorithm is guided by control dependencies in the program to search for test data which satisfy test requirements. The genetic algorithm constructs new test data from previously generated test data. The algorithm evaluates the existing test data, and guide the direction of search by using the programs control-dependence graph \cite{pargas1999test}.

The benefit of the genetic approach is quick generation of test data with focus and direction. New test cases are generated by applying simple operations on existing test cases that are judged to have good potential of satisfying the test requirements. The success of this approach, however, depends heavily on the way in which the existing test data is measured \cite{pargas1999test}.

% Please paraphrase the above section genetic algorithm.

\subsection{Random Test Data Generators}

Random test data generator is the simplest technique for generation of test data. It has the advantage of being used to generate input data for any type of program. However, random test data generation is based solely on probability and cannot accomplish high coverage as its chances of finding semantically small faults are quite low [3find that reference].

If a fault is only revealed by a small percentage of the program input it is said to be a semantically small fault. For example of a semantically small fault consider the following code:
\begin{lstlisting}
void test(char x,char y) {
    if (x==y)
        System.out.println("Equal");
    else
        System.out.println("Not Equal");
}
\end{lstlisting}

It is easy to see that the probability of execution of the first statement is significantly lower than that of the second statement. As the structure gets complex so does the probability of its execution. Thus, such semantically small faults are hard to find by using random test data generation. 

\subsection{Search-based Test Data Generation}
It is a technique that uses meta-heuristic algorithms to guide generation of test data. In Search-based test data generation technique each input vector x can be associated with a measure $cost(x)$ that represents how far away the input vector x is from satisfying the set goal. Input test values closer to the set goal have low cost values and the other have high cost values. 

Consider a program with an initial branch statement: ${\it{ if (x >= 20) y = z; else y = 2 * z;}}$ and suppose we want the true branch to be executed. An input value of $x == 25$ clearly satisfies the predicate, and a value of $x == 15$ can be seen to come closer to satisfying the predicate than a value of $x ==5$. We might evaluate a cost function probe ( immediately before the indicated statement) of the form $cost(x) = max {0, 20 - x}$. Thus $x == 25$ has cost $0$, $x == 15$ has cost $5$ and $x = 5$ has cost $15$. We can see how finding data to satisfy the branch predicate is essentially a search over the input domain of x to find a value such that $cost(x) == 0$. 

Similarly, finding data to follow a particular path in the code can be considered as the one which satisfy each of the number of predicate at different points. This leads to a cost function which combines the costs at each of the relevant branching points. The approach requires the measurement of state at appropriate points in a programs execution. Moreover, the cost function plays the role of oracle for each targeted test requirement. Consequently, the cost function must change as per requirement. Frequent re-instrumentation of program is required to find test data that fully satisfy common coverage criteria. 

%At its heart search based software testing requires the use of search or optimisation algorithms. Most standard heuristic search techniques have been used, e.g. hill-climbing, simulated annealing, tabu search and genetic algorithms. 

% The details need not concern us here, and the reader is referred to McMinn [50] for details. Overall the search based 





%\subsection{Using A Model Checker}
%\subsection{Test Case Generation with Gatel by Using Lustre}
%\subsection{Using Models in Z}
%\subsectioin{Using UML Diagrams}
%\subsection{Using Misuse / Abuse Cases for Robustness Testing}
%\subsection{Randomly-generated test suites} % Dynamically discovering likely program invariants thesis page 77
%\subsection{Grammar-generated test suites Randomly}% Dynamically discovering likely program invariants thesis page 78
%\subsection{Test-case generation} %Combining over-and under-approximating program analysis for automatic software testing (section 4.2)
%\subsection{Mutation generation} % Automatic testing of software with structurally complex inputs. page 79 section 7.2.1
%\subsection{Test Data Generation} % page 70, 4.6.1, Coverage analysis for GUI Testing
% Check Generating Structurally complex tests from declarative constraints thesis 




% add diagram for generators similar to john paper in automated program flaw finding using simulated annealing.



Random testing was first mentioned in the literature by Hanford in 1970. He reported syntax machine, a tool that randomly generated data for testing PL/I compilers~\cite{hanford1970automatic}. Later in 1983, Bird and Munoz described a technique to produce randomly generated and self checking test cases~\cite{bird1983automatic}. 

%Work on random test generation dates back to a 1970 paper by Hanford. In it he reports on the ``syntax machine", a program that generated random but syntactically-correct programs for testing PL/I compilers \cite{hanford1970automatic}. The next published work on random testing was a 1983 paper by Bird and Munoz \cite{bird1983automatic}. They describe a technique to generate randomly-generated ``self-checking" test cases: test cases that in addition to generating test inputs generated checks for expected outputs using knowledge of the semantics of the software under test. For example, when generating random programs for testing a compiler, the generator kept track of the expected values of arithmetic computations and inserted the appropriate checks into the test case. They also applied their technique to checking sorting and merging procedures and graphic display software~\cite{pacheco2009directed}.

%One of the best-known works in the field is Miller et al.’s “fuzz testing” paper, where they generate random ASCII character streams and use them as input to test Unix utilities for abnormal termination or non-terminating behaviour \cite{miller1990empirical}. In subsequent work, they extended fuzz testing to generate sequences of keyboard and mouse events, and found errors in applications running in X Windows, Windows NT and Mac OS X \cite{forrester2000empirical, miller2006empirical}. Today, fuzz testing is used routinely in industry. It is frequently used as a tool for finding security vulnerabilities and is applied to test formats and protocols that attackers might use to gain unauthorized access to computers over a network. Other work that applies random test generation to operating systems code includes Kropp’s use of random test generation to test low-level system calls \cite{kropp1998automated}, and Groce et al.’s use of random test generation at NASA to test a file system used in space missions \cite{groce2007randomized}. In all these studies, a random test generator invariable found many errors in the software under test, and many errors were critical in nature. For example, Groce et al. found dozens of errors in the file system under test, many of which could have jeopardized the success of a mission.

Random testing is a dynamic black-box testing technique in which the software is tested with non-correlated unpredictable test data from the specified input domain~\cite{Chan2002}. As stated by Richard~\cite{hamlet1994}, in random testing, input domain is first identified, then test data are randomly taken from it by means of random generator. The program under test is executed on the test data and the results obtained are compared with the program specifications. The test fails if the results are not according to the specifications and vice versa. Fail results of the test cases reflects failure in the SUT.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=2.5cm ]{chapter3/randomTesting.jpg}
	\caption{Random Testing}
\end{figure}

Generating test data by random generator is quite economical and requires less intellectual and computational efforts~\cite{Ciupa2008a}. Moreover, no human intervention is involved in data generation which ensures an unbiased testing process. However, generating test cases with out using any background information makes random testing susceptible to criticism. Random testing is criticized for generating many of the test cases that falls at the same state of software. It is also stated that, random testing generates test inputs that violates requirements of the given SUT making it less effective~\cite{pacheco2009directed, sen2007effective}. Myers mentioned random testing as one of the least effective testing technique~\cite{Myers1979}. However, Ciupa et al. stated~\cite{Ciupa2007}, that Myers statement was not based on any experimental evidence. Later experiments performed by several researchers~\cite{Ciupa2008, hamlet1994,  leitner2007efficient, Duran1981} confirmed that random testing is as effective as any other testing technique. It is reported~\cite{Duran1981} that random testing can also discover subtle faults in a given SUT when subjected to large number of test cases. It is pointed out that the simplicity and cost effectiveness of random testing makes it more feasible to run large number of test cases as opposed to systematic testing techniques which require considerable time and resources for test case generation and execution. The empirical comparison proves that random testing and partition testing are equally effective~\cite{hamlet1990}. A comparative study conducted by Ntafos~\cite{ntafos1998random} concluded that random testing is more effective as compared to proportional partition testing. A prominent work to mention is that of Miller et al.~\cite{miller1990empirical}, who generated and used random ASCII character streams to test Unix utilities for abnormal termination or non-terminating behaviour. Subsequently the same technique was extended to discover errors in softwares running on X Windows, Windows NT and Mac OS X \cite{forrester2000empirical, miller2006empirical}. Other famous studies using random testing includes low-level system calls \cite{kropp1998automated}, and file systems used in missions at NASA \cite{groce2007randomized}.


\section{Various versions of random testing}
Researchers have tried various approaches to bring about improved versions of random testing with better performance. The prominent versions of random testing are as follows:

\subsection{Adaptive Random Testing}
Adaptive random testing (ART), proposed by Chen et al.~\cite{Chen2008} is based on the previous work of Chan et al.~\cite{Chan1996} regarding the existence of failure patterns across the input domain. Chan et al. observed that failure inducing inputs formed certain geometrical patterns in the whole input domain which were divided into point, block and strip patterns described below.

\begin{enumerate}
\item {\bf Point pattern:} In the point pattern, inputs inducing failures are scattered across the input domain in the form of stand-alone points. Example of point pattern is the division by zero in the statement: $total = num1/num2;$ where $num1$, $num2$ and $total$ are variables of type integer.
\item {\bf Block pattern:} In the block pattern, inputs inducing failures lie in close vicinity to form a block in the input domain. Example of block pattern is failure caused by the statement: $if ( (num \textgreater 10) \&\& (num \textless 20) )$. Here 11 to 19 are a block of faults.
\item {\bf Strip pattern:} In the strip pattern, inputs inducing failures form a strip across the input domain. Example of strip pattern is failure caused by the statement: $num1 + num2 = 20$. Here multiple values of $num1$ and $num2$ can lead to the fault value 20. 
\end{enumerate}

\begin{figure}[h]
	\centering
	\includegraphics[width=13cm, height=5.7cm ]{chapter3/pointblockstrip.jpg}
	\caption{Patterns of failure causing inputs~\cite{Chan1996}}
	\label{fig:patterns2}
\end{figure}

In Figure~\ref{fig:patterns2} the three square boxes indicate the whole input domains. The white space in each box shows legitimate and faultless values while the black colour in the form of points, block and strip, inside the respective boxes indicates the faults in the form of point, block and strip patterns.

Chen et al.~\cite{Chen2008} argued that ordinary random testing might generate test inputs lurking too close or too far from the input inducing failure and thus fails to discover the fault. To generate more fault-targeted test inputs, they proposed Adaptive Random Testing (ART) as a modified version of random testing where test values are selected at random as usual but are evenly spread across the input domain by using two sets. The executed set comprises the test cases and the candidate set includes the test cases to be executed by the system. Initially both the sets are empty. The first test case is selected at random from the candidate set and stored in executed set after execution. The second test case is then selected from the candidate set which is far away from the last executed test case. In this way the whole input domain is tested with greater chances of generating test input from the existing fault patterns.

In the experiments conducted by Chen et al.~\cite{Chen2008}, the number of test cases required to detect first fault (F-measure) was used as a performance matrix instead of the traditional matrices (P-measure) and (E-measure). Experimental results using ART showed up to 50\% increase in performance compared to random testing. The authors pointed out that the issues of increase overhead, spreading test cases across the input domain for complex objects and efficient ways of selecting candidate test cases still exist. Chen et al. continued their work on ART to address some of these issues and proposed its upgraded versions~\cite{Chen2005, chen2009enhanced}. 

\subsection{Mirror Adaptive Random Testing}
%As discussed in the above section ART provide better results, however the increase in overhead due to extra computation to achieve even spread of test inputs makes it less cost effective. 
Mirror Adaptive Random Testing (MART)~\cite{Chen2003} is an improvement on ART by using mirror-partitioning technique to reduce the overhead by decreasing the extra computation involved in ART. 

\begin{figure}[h]
\begin{center}
	\includegraphics[width=13.5cm, height=6.5cm ]{chapter3/mart2.pdf}
	\caption{Mirror Adaptive Random Testing~\cite{Chen2003}}
\label{fig:mirrorART}
\end{center}  
\end{figure}

In this technique, the input domain of the program under test is divided into n disjoint sub-domains of equal size and shape. One of the sub-domains is called source sub-domain while all others are termed as mirror sub-domains. ART is then applied only to the source sub-domain while test cases are selected from all other sub-domains by using mirror function. In MART $(0, 0), (u, v)$ are used to represent the whole input domain where $(0, 0)$ is the leftmost and $(u, v)$ is the rightmost top corner of the two dimensional rectangle. On splitting it into two sub-domains we get {(0, 0), (u/2, v)} as source sub-domain and $(u/2, 0), (u, v)$ as mirror sub-domain. Suppose we get $x$ and $y$ test cases by applying ART to source sub-domain, now we can linearly translate these test cases to achieve the mirrored effect, i.e. $(x + (u/2), y)$ as shown in the Figure~\ref{fig:mirrorART}. 

Comparative study of MART with ART provide evidence of equally good results of the two strategies with MART having the added advantage of lower overhead by using only one quarter of the calculation as compared with ART.


\subsection{Restricted Random Testing}
Restricted Random Testing~\cite{chan2003normalized} is another approach to overcome the problem of extra overhead in ART. Restricted Random Testing (RRT) achieves this by creating a circular exclusion zone around the executed test case. A candidate is randomly selected from the input domain for the next test case. Before execution the candidate is checked and discarded if it lies inside the exclusion zone. This process repeats until a candidate laying outside the exclusion zone is selected. This ensures that the test case to be executed is well apart from the last executed test case. The radius of exclusion zone is constant in each test case and the area of input domain decreases progressively with successive execution of test cases.

\begin{figure}[h]
	\centering
	\includegraphics[width= 8cm, height = 6.5cm]{chapter3/RRT.pdf}
	\caption{Input domain with exclusion zone around the selected test case}
\end{figure}

The above authors compared RRT with ART and RT to find the comparative performance and reported that the performance of RRT increases with the increase in the size of the exclusion zone and reaches the maximum level when the exclusion zone is raised to largest possible size. %Normalized Restricted Random Testing~\cite{chan2003normalized} is an improvement over RRT by allowing the testers to have better information about the target exclusion rate (R) of RRT. 
They further found that RRT is up to 55\% more effective than random testing in terms of F-measure.



\subsection{Directed Automated Random Testing}
Godefroid et al.~\cite{Godefroid2005} proposed Directed Automated Random Testing (DART). %Its main purpose was to overcome the cost and difficulty of manual testing while keeping its quality intact. It automate the whole testing process including generation of unit tests, test drivers/harness and assertions for functional correctness. 
The following main features of DART are reported in the literature:
\begin{enumerate}
\item {\bf Automated Interface Extraction:} DART automatically identifies external interfaces of a given SUT. These interfaces include external variables and methods and the user-specified main method responsible for program execution.
\item {\bf Automatic Test Driver:} DART automatically generate test drivers for running the test cases. All the test cases are randomly generated according to the underlying environment.
\item {\bf Dynamic Analysis of execution:} DART instrument the given SUT at the start of the process in order to track its behaviour dynamically at run time. The results obtained are analysed in real time to systematically direct the test case execution along alternative path for maximum code coverage.
\end{enumerate}

The DART algorithm is implemented in the tool which is completely automatic and accepts the test program as input. After the external interfaces are extracted it then use the pre-conditions and post-conditions of the program under test to validate the test inputs. For languages that do not support contracts inside the code (like C), they used public methods or interfaces to mimic the scenario -------- to be continued

\subsection{Quasi Random Testing}
Quasi-random testing (QRT)~\cite{Chen2005} is a testing technique which takes advantage of failure region contiguity for distributing test cases evenly and thus decreases computation. %Chan et al after the analysis of faults in various experiments found that the fault patterns across the input domain are continuous. 
To achieve even spreading of test cases, QRT uses a class with a formula that forms an s-dimensional cube in s-dimensional input domain and generates a set of numbers with small discrepancy and low dispersion. The set of numbers is then used to generate random test cases that are permuted to make them less clustered and more evenly distributed. An empirical study was conducted to compare the effectiveness of QRT with ART and RT. The results showed that in 9 out of 12 programs QRT found a fault quicker than ART and RT while there was no significant improvement in the remaining three programs.
%\subsection{Monti Carlo Random Testing}

%\subsection{Good Random Testing}

\subsection{Feedback-directed Random Testing}
Feedback-directed Random Testing (FDRT) is a technique that generates unit test suite at random for object-oriented programs~\cite{Pacheco2007}. As the name implies FDRT uses the feedback received from the execution of first batch of randomly selected unit test suite to generate next batch of directed unit test suite. In this way redundant  and wrong unit tests are eliminated incrementally from the test suite with the help of filtration and application of contracts. For example unit test that produce IllegalArgumentException on execution is discarded, because, selected argument used in this test is not according to the type of argument the method required. 

%\subsection{Adaptive Random Testing for Object-Oriented}
\subsection{The ARTOO Testing}
The Adaptive Random Testing for Object Oriented (ARTOO) strategy is based on object distance. Ciupa et al.~\cite{Ciupa2006} defined the parameters that can be used to calculate distance between the objects. Two objects have more distance between them if they have more dissimilar properties. The parameters to specify the distance between the objects are dynamic types and values are assigned to the primitive and reference fields. Strings are treated in terms of directly usable values and Levenshtein formula~\cite{Levenshtein1966} is used as a distance criterion between the two strings.

In the ARTOO strategy, two sets are taken i.e. candidate-set containing the objects ready to be run by the system and the used-set, which is empty. First object is selected randomly from the candidate-set which is moved to used-set after execution. The second object selected from the candidate-set for execution is the one with the largest distance from the last executed object present in the used-set. The process continues till the bug is found or the objects in the candidate-set are finished~\cite{Ciupa2006}.

The ARTOO strategy, implemented in AutoTest~\cite{Ciupa2008a}, was evaluated in comparison with Directed Random (D-RAN) strategy by selecting classes from EiffelBase library \cite{meyer1987eiffel}. The experimental results indicated that some bugs found by the ARTOO were not identified by the D-RAN strategy. Moreover the ARTOO found first bug with small number of test cases than the D-RAN strategy. However, computation required to select test case in the ARTOO strategy was more than the D-RAN strategy and took more time and cost to generate a test case.

% the same team implemented that model and performed several experiments to evaluate the proposed model. Adaptive Random Testing for Object Oriented (ARTOO) is a testing strategy, based on object distance, implemented in AutoTest \cite{16 search it Mendeley}.
%ARTOO was implemented as a plug-in strategy in AutoTest. It only deals with creating and selecting inputs and all other functionality of the AutoTest was the same. Since ARTOO is based on object distance therefore the method for test input selection is to pick that object from the candidate set (A pool of objects that is a potential candidate to be executed by the system) that has the highest average distance in comparison to the objects already executed. In the experiments classes from EiffelBase library \cite{17 search it mendeley} were used. To evaluate ARTOO the same tests were also applied to directed random strategy (RAND). The outcome of the experiments showed that ARTOO finds the first bug with fewer test cases than RAND. The computation to select test case in ARTOO is more than RAND and therefore ARTOO takes more time to generate a test input. The experiments also found few of the bug found by ARTOO were not pointed out by RAND furthermore ARTOO is less sensitive to the variation of seed value than RAND
. 
%\subsection{Object Distance and its application}
%To improve the performance of random testing the emphasis of ART was on the distance be- tween the test cases. But this distance was defined only for primitive data types like integers and other elementary input. Ciupa et al defined the parameters that can be used to calculate distance between the composite programmer-defined types so that ART can be applicable to testing of today’s object-oriented programs~\cite{Ciupa2006}. Two objects have more distance between them if they have more dissimilar properties. The parameters to specify the distance between the objects are dynamic types, values of its primitive and reference fields. Strings are treated as a directly usable values and Levenshtein distance~\cite{Levenshtein1966} that is also known as edit distance is used as a distance criteria between the two strings. To implement object distance first all the distances of the objects are measured. Then two sets candidate- objects containing the all the objects ready to be run by the system and the used-objects set, which is initially empty. First object is selected randomly from the candidate-object set and is moved to used-object set when executed by the system. Now the second object selected from the candidate set for execution is the one with the biggest distance from the last executed object present in the used-object set. This process is continuing until the bug is found or the objects in the candidate-object set are finished.

%\subsubsection{Experimental Assessment of RT for Object-Oriented Software}
%In this research the effect of various parameters involved in random testing and its effect on efficiency is evaluated by performing various experiments on Industrial-grade code base. Large-scale clusters of computers were used for 1500 hours of CPU time which resulted in 1875 test sessions for 8 classes under test.~\cite{Ciupa2007} The finding of the experiments are 1. Version of random testing algorithm that is efficient for smaller testing timeout is equally efficient for higher testing timeouts. 2. The value of seed for random testing algorithm plays a vital role in finding the number of bugs in specific time. 3. Most of the bugs are found in the first few minutes of the testing sessions.



%\subsection{Design by Contract}
% section taken from binding yeti with .net, check it out correct it.
%\textcolor{blue}{Modern software development as it is well known has adopted the paradigm of Object Oriented (OO) Programming. The primary and most important reason for this evolution is the desire for better quality software and a more efficient way to bridge the gap between requirements and code.
%Industrial use of OO confirmed the superiority of this approach over procedural
%programming languages. However, so far no final approach has been generally accepted over a methodology on how to construct OO software in order to achieve one of the most important factors of quality, which is reliability or else robustness. As the work of Dr. Bertrand Meyer has shown \cite{meyer1992applying, meyer1988object} the decision on how to make software more reliable is crucial if the developers want to use the benefits of OO. These benefits include:
%1 Reuse of software components. This implies using a software component at multiple environments apart from the environment in which its developers originally deployed it.
%2 The goal of having reliability plays an important role in characterizing the quality of a software module.
%3 The abstract types that OO introduces. A reliable way to construct abstract types emerges. To achieve the previous goals the software development world has used two approaches, Defensive programming and Design by Contract. As the next paragraphs show the latter one has been showing more advantages than the first one. That is why most researches on Automated Testing use design by contract as a requirement that the SUT must conform to, in order for their tool to find as many bugs as possible \cite{Leitner2007}.
%The idea of Defensive Programming, which is inherited from the software development world prior to OO, is in general advising to ―… include as many checks as possible even if they are redundant \cite{meyer1992applying}. This concept is intuitively stating that having extra checking can never do harm especially if someone wants to protect the software from inexperienced users. This last concept unfortunately is not correct. This kind of approach puts more code inside the software and this contributes to greater complexity of the software, and complexity as Dr. Meyer has very correctly stated ―…is the single worst obstacle to software quality in general, and to reliability in particular \cite{meyer1992applying}. The reason is that this extra code is a source of things that might go wrong and so we must also check it, and so on to infinitum. The need for a more systematic approach is evident based on the concept that software elements are implementations of well-understood specifications by the developers and that is exactly what Design by Contract does. Design by Contract concept adopts the idea that any operation a routine of a software performs, should bind the caller of that routine to the routine itself. This binding provides specific obligations that each party has, along with benefits. Essentially the obligations of one part describe the benefits of the other part. These ideas apply to software routines (i.e. methods, functions of classes) via assertions that implement the binding, which is like a contract between two persons. These assertions are classified into pre-conditions, post-conditions and class invariants. Figure 2.1 which is a figure of the ―Applying Design by Contract article \cite{meyer1992applying}, illustrates the use of these assertions.
%As anyone can understand from Figure ?? pre and post-conditions denote the idea that if the caller promises to invoke the routine with the pre-conditions holding, the routine guarantees the caller that it will return the system in a final state in which the post-conditions hold. Thus, the invoker knows nothing about how the routine reaches the final state but can depend on the results. This implies another thing, which is that the routine is responsible only for the cases where the pre- conditions hold. This means rejecting the whole concept of defensive programming because if something is an assertion in the pre-condition the routine does not have to handle it in its body/code and vice versa. Overall, it is forbidden for the same assertion to exist in both parts. How strong or weak a pre-condition should be is a decision of the developer. So far the notions of pre and post-conditions are assertions that developers can use even with procedural programming languages at each of their routines. The notion that enhances more the efficiency of the contracts is the notion of the class invariant. Through class invariants a developer can describe a general condition that any instantiation of that class must hold at any time \cite{meyer1992applying, meyer1988object}. By this the developers classify the requirements that they have in the same abstract way in which they develop the software; making the bridging of requirements to code more traceable, thus more possible to achieve all the requirements, the basic goal of software development.
%In addition, to the previous benefits design by contract can facilitate documentation. By providing contract information documentation describes a module of code certainly better than just presenting the interface and the result it yields. Furthermore, monitoring these assertions can be very helpful while debugging.
%It is up to the developers to decide what the software must do when one of the parties breaks a contract. It can stop the execution, prompt the event, ask the user to decide etc. It is a decision based on the specifications. Basically as the article of Meyer B. \cite{meyer1992applying} describes, three things can mainly occur
%1. An alternative algorithm starts due to this exceptional behaviour 2. System stops its executions and returns to a prior consistent state 3. A rare but possible false alarm has happened due to operating system or hardware signals and after correcting actions execution continues
%The general approach is to consider and to monitor any violation because most of the times this violation describes a bug. Either in the logic of the requirements (pre-conditions, class invariants) or in the algorithm of the implementation (post-conditions).
%Sometimes research papers mention the contracts of the code as partial specifications \cite{daniel2007automated} because they provide more information than just requirements documents but at the same time they are not a full specification model which describes the whole algorithm of a software module as formal methods do.
%Design by Contract principle helps developers use appropriately polymorphism and dynamic binding. With contracts, designers can have inheritance and reassure that the class that inherits another class will respect the original contract and if the designer wants he/she can add assertions based on the functionality of the child class. The rules that the designers must follow, to maximally use the contracts, is to allow the child classes, when it is desired, to: i) have weaker pre-conditions or ii) stronger post-conditions. In this way contracts provide an immediate guideline to the designer.}


%\subsection{Daikon} % Generating high confidence contracts without user input using... page 6

%\section{Automated Random Testing Tools}
\section{Tools for Automated Random Testing}
A number of open-source and commercial automatic random testing tools reported in the literature are briefly described in the following section.


\subsection{JCrasher}
JCrasher is an automatic robustness testing tool developed by Csallner and Smaragadakis \cite{Pacheco2007b}. JCrasher tries to crash the Java program with random input and exceptions thrown during the process are recorded. The exceptions are then compared with the list of acceptable standards, defined in advance as heuristics. The undefined runtime exceptions are considered as errors. Since users interact with programs through its public methods with different kinds of inputs, therefore, JCrasher is designed to test only the public methods of the SUT with random inputs.

\begin{figure}[h]
	\centering
	\includegraphics[width=15cm, height=8cm]{chapter3/JCrasher.png}
	\caption{Illustration of robustness testing of Java program with JCrasher~\cite{Pacheco2007b}}
	\label{fig:JCrasher}
\end{figure}

The working of JCrasher is illustrated by testing a $T.java$ program as shown in the Figure~\ref{fig:JCrasher}. The source file is first compiled using $javac$ and the byte code obtained is passed as input to JCrasher which uses Java reflection library~\cite{chan1999java} to analyse all the methods declared by class $T$. The JCrasher uses methods transitive parameter types $P$ to generate the most appropriate test data set which is written to a file {\it TTest.java}. The file is compiled and executed by JUnit. All the exceptions produced during test case executions are collected and compared with robustness heuristic for any violation and reported as errors.\\

JCrasher is a pioneering tool with the capability to perform fully automatic testing, including test case generation, execution, filtration and report generation. JCrasher has the novelty to generate test cases as JUnit files which can also be easily read and used for regression testing. Another important feature of JCrasher is to execute each new test on a ``clean slate" ensuring that the changes made by the previous tests do not affect the new test.  

% check parameter space or parameter graph in the figure???


\subsection{Jartege}
Jartege (\uline{Ja}va~\uline{r}andom~\uline{te}st~\uline{ge}nerator) is an automated testing tool~\cite{Oriat2004} that randomly generates unit tests for Java classes with contracts specified in Java Modelling Language (JML). The contracts include, methods pre and post-conditions and class invariants. Initially Jartege uses the contracts to eliminate irrelevant test cases and later on the same contracts serve as test oracle to differentiate between errors and false positives. Jartege uses simple random testing to test classes and generate test cases. In addition, it parametrise its random aspect in order to prioritise testing a specific part of the class or to get interesting sequences of calls. The parameters include the following: 
\begin{itemize}
\item Operational profile of the classes i.e. the likely use of the class under test by other classes.  
\item Weight of the class and method under test. Higher weight prioritizes the class or method over lower weight during test process. 
\item Probability of creating new objects during test process. Low probability means creation of fewer objects and more re-usability for different operations while high probability means numerous new objects with less re-usability.
\end{itemize}

The Jartege technique evaluates a class by entry pre-conditions and internal pre-conditions. Entry pre-conditions are the contracts to be met by the generated test data for testing the method while internal pre-conditions are the contracts which are inside the methods and their violations are considered as errors either in the methods or in the specifications. The Jartege checks for errors in program code as well as in specifications and the Junit tests produced by Jartege can be used later as regression tests. Its limitation is the requirement of prior existence of the program JML specifications.

\subsection{Eclat}
Eclat~\cite{Pacheco2005} is an automated testing tool which generates and classifies unit tests for Java classes. The process is accomplished in three stages. In the first stage, it selects a small subset of test inputs that are likely to reveal faults in the given SUT.

\begin{figure}[h]
	\centering
	\includegraphics[width=15cm, height=10.5cm]{chapter3/eclat_working.png}
	\caption{Main component of Eclat contributing to generate test input~\cite{Pacheco2005}}
	\label{fig:eclat}
\end{figure}


The tool takes a software and a set of test cases for which the software runs properly. It creates an operational model, based on the correct software operations, and apply the test data. If the operational pattern of execution of the test data differs from the model, the following three outcomes may be possible: (a) a fault in the given SUT (b) model violation despite normal operation (c) illegal input which the program is unable to handle. In the second stage, reducer function is used to discard any redundant input, leaving only a single input per operational pattern. In the third stage, the acquired test inputs are converted into test cases and oracles are created to determine the success or failure of the test. \\

----- compared Eclat with JCrasher by executing nine programs on  both tools. ------ reported that Eclat performed better than JCrasher. On the average, Eclat selected 5.0 inputs per run out of which 30\% revealed faults while JCrasher selected 1.13 inputs per run out of which 0.92\% revealed faults. The limitation of Eclat is dependence on initial pool of correct test cases. Existence of errors in the pool leads to the creation of wrong operational model which adversely affects the testing process~\cite{Pacheco2007b}.   

%\subsection{JTest}
%Parasoft Jtest is a commercial tool that automatically generates and execute unit tests. It can be easily integrated to Java IDEs like Eclipse where it provide two main functionalities, i.e. Static Analysis, Unit testing and code coverage. [25]
%In static analysis Jtest takes a complete project or set of classes as input and compares it with a list of built-in rules. The statement violating any of these rules is an error. It also suggests probable fixes for the detected fault.
%For unit testing it takes a class as an input and processes a number of scenarios against it to generate and execute unit tests. Once unit tests are executed they become the part of regression test for future reference.
%Jtest also shows the code coverage of the program by colour coding the statements that are not executed by the unit tests.


\subsection{Randoop}
Random tester for Object Oriented Programs (RANDOOP) is the tool used for implementing FDRT technique~\cite{Pacheco2007b}. RANDOOP is a fully automatic tool, capable of testing Java classes and .Net binaries. It takes as input a set of classes, contracts, filters and time limit and gives output either as a suite of JUnit or NUnit for Java and .Net program respectively. Each unit test in a test suite is a sequence of method calls (hereafter referred as sequence). RANDOOP builds the sequence incrementally by randomly selecting a public method from the class under test and arguments for these methods are selected from the predefined pool in case of primitive types and a sequence or null value in case of reference type. RANDOOP maintains two sets called ErrorSeqs and NonErrorSeqs to record the feedback. It extends ErrorSeqs set in case of contract or filter violation and NonErrorSeqs set when no violation is recorded in the feedback. The use of this dynamic feedback evaluation at runtime brings an object to an interesting state. On test completion, ErrorSeqs and NonErrorSeqs are produced as JUnit/NUnit test suite. In terms of coverage and number of faults discovered, RANDOOP implementing FDRT was compared with JCrasher and JavaPathFinder and 14 libraries of both Java and .Net were evaluated~\cite{visser2004test}. The results showed that RANDOOP achieved more coverage than JCrasher in branch coverage and faults detection. It can achieve on par coverage with systematic approaches like JavaPathFinder. 

\subsection{QuickCheck}
QuickCheck~\cite{Claessen2000} is a lightweight random testing tool used for testing of Haskell programs~\cite{Hudak2007}. Haskell is a functional programming language where programs are evaluated by using expressions rather than statements as in imperative programming. In Haskell most of the functions are pure except the IO functions, thus main focus of the tool is on testing pure functions. QuickCheck is designed to have a simple domain-specific language of testable specifications embedded in Haskell. This language is used to define expected properties of the functions under test %- for example, reversing a list with single element must result in the same list.\\ %(author check the definition of pure functions)\\
\indent The QuickCheck takes function to be tested and properties of the program defined by tester (Haskell functions) as input. The tool uses built-in random generator to generate effective test data, however, to get adequate coverage in the case of custom data types, the testers can also develop their own generator. On executing the function with test data, the tester-defined-properties must hold for the function to be correct. Any violation of the defined properties suggest error in the function.



% The function is executed against the generated test data. The QuickCheck evaluates and declares a fault in the function where a test case violates the set properties.   



%\subsection{AgitarOne}
%AgitarOne is a commercial tool that automatically generates unit tests. It has a Junit Generator engine that can create 25,000 lines or more of Junit per hour [29]. It can be easily integrated into famous IDE like Eclipse. It takes as input, classes under test, time and optionally any knowledge or test cases that has a positive influence on the performance of the testing process. The generated Junit tests can be run from the same IDE and can also be used for later regression testing. The GUI interface is called a dashboard which provides in depth knowledge of the tests conducted, failures detected, alerts and the archieves of the tests conducted earlier. It also shows the coverage obtained after executing the Junits against the code under test.

\subsection{Autotest}
The Autotest, based on formal automated testing is used to test Eiffel language programs~\cite{Ciupa2007}. The Eiffel language uses the concept of contracts which is effectively utilized by Autotest. For example, the auto generated input is filtered using pre-conditions and unwanted test input is discarded. The contracts are also used as test oracle to determine if the test is pass or fail. Beside automated testing the Autotest also allows the tester to manually write the test cases to target specific section of the code. The Autotest may have a single method/class or suite of methods/classes as inputs, it then automatically generate test input data according to the requirement of the methods or classes.

\begin{figure}[h]
	\centering
	\includegraphics[width=13cm, height=7cm]{chapter3/autotest.png}
	\caption{Architecture of Autotest~\cite{Leitner2007}}
	\label{fig:autotest}
\end{figure}

\noindent According to Figure~\ref{fig:autotest}, the architecture of Autotest can be split into the following main parts:
\begin{enumerate}
\item \textbf{Testing Strategy:} It is a pluggable component where testers can fit any strategy according to the testing requirement. The strategy contains the directions for testing.%- for example what instructions should be executed on the SUT. Using the information the strategy synthesize test cases and forward it to the proxy. 
The default strategy creates test cases that uses random input to exercise the methods/classes under test.
\item \textbf{Proxy:} It handles inter-process communication. It receives execution requests from the strategy and forward these to the interpreter. The execution results are sent to the oracle.
\item \textbf{Interpreter:} It execute instructions on the SUT. The most common instructions include: create object, invoke routine and assign result. The interpreter is kept separate to increase robustness.
\item \textbf{Oracle:} It is based on contract-based testing. It evaluate the results to see if the contracts are satisfied. The outcome of the tests are formatted in HTML and stored on disk.
\end{enumerate}

\subsection{TestEra}
TestEra~\cite{Khurshid2004} is a novel framework for auto generation and evaluation of test inputs for a Java program. It takes methods specifications, integer value and the method under test as input. It uses pre-conditions of a method to generate all non isomorphic valid test inputs to the specified limit. The test inputs are executed on the method and the results are compared against the postconditions of the method serving as oracle. Any test case that fails to satisfy postcondition is considered as a fault. 

TestEra uses the Alloy modelling language~\cite{jackson2001micromodularity} to express constraints on test inputs and Alloy Analyser~\cite{jackson2000alcoa} to solve these constraints and generate test inputs. Alloy Analyzer performs the following three functions: (a) it translates Alloy predicates into propositional formulas, i.e. constraints where all variables are boolean (b) it evaluates the propositional formulas to find the outcome (c) it translates each outcome from propositional domain into the relational domain.

\begin{figure}[h]
	\centering
	\includegraphics[width=14cm, height=12cm]{chapter3/testera.png}
	\caption{Architecture of TestEra ~\cite{Khurshid2004}}
	\label{fig:testera}
\end{figure}


---TestEra and Korat are similar tools because they both uses program specifications to guide the auto generation of test inputs. However they are different from Jartege and AutoTest which uses specifications to filter and truncate the unnecessary random generated inputs. While the tools use program specifications differently for test input generation, they all uses it in a similar way for oracle. 


%Testera use specifications to guide the automatic generation of test inputs. It uses Alloy language for specification and Alloy Analyser to generate all non-isomorphic instances for a given size according to the specification automatically. Then, TestEra translate the instances to Java input as test cases for the program under test. After executing the test, TestEra then translate the outputs back to Alloy and Alloy Analyzer check the input and output against the correctness criteria given in Alloy. When it detects a violation, TestEra generates report in the form of concrete counterexamples. Figure 2.6 [50] illustrates the basic framework of TestEra.

\subsection{Korat} % please read thesis of khurshid in Mendeley in phd thesis section for more information.
Korat~\cite{Boyapati2002} is a novel framework for automated testing of Java programs based on the formal specifications~\cite{chang1999structural}. Korat and TestEra~\cite{Khurshid2004} were developed by the same team and perform specification based testing. The difference however is that Korat uses Java Modelling Language (JML) while TestEra uses Alloy Modelling Language for specifications. Moreover, Korat uses bounded-exhaustive testing in which the code is tested against all possible inputs within the given small bound~\cite{khurshid2001checking}.

Korat generate structurally complex inputs by solving imperative predicates. An imperative predicate is a piece of code that takes a structure as input and evaluates it to a boolean value. Korat takes imperative predicates and finitization value as inputs. It systematically explores the predicates input space and generates all non-isomorphic inputs for which the predicates return true. The core part of Korat monitors execution of the predicates on candidate inputs to filter out these fields accessed the particular fields during executions. These inputs are taken as test cases. Korat depends or developers written {\it repOK()} and {\it checkRep()} methods, where {\it repOK()} is used to check the class invariants and {\it checkRep()} is used to verify the post-conditions to validate the correctness of the test case. 

The key benefit of Korat and TestEra, representation level approaches, is that no existing set of operations are required to create input values and therefore they can achieve to create input values that may be difficult or impossible using a given set of operations. However, The only disadvantage to this approach is the requirement of significant amount of manual efforts \cite{pacheco2009directed}.    

%As the test start, it uses methods pre-condition to generate all non-isomorphic test cases up to a given size. It then executes each of the test case and compare the obtained results to the methods post-condition, which serves as an oracle to evaluate the correctness of each test case. 








%\section{Automated Random Testing}
%\subsection{Test Data Generation}
%\subsection{Test Execution}



%\subsection{Test Report}

\section{Summary}
The chapter gives an overview of software testing process, starting from defining what software testing is, why it is necessary, its common types and the purpose for which they are used. It then differentiate between manual and automated software testing and finally various ways of software test data generation, being the most critical and crucial part of any testing system are studied.


% ------------------------------------------------------------------------


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
